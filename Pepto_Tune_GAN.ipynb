{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pepto-Tune-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIqW39RgNDmLqGLHic12Ys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjatkin/Pepto-GAN/blob/tune-wgan/Pepto_Tune_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xZrtUeoBGrT",
        "colab_type": "text"
      },
      "source": [
        "# All Imports / Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB2VCzxKBLHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EfSHhTyEQj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Options():\n",
        "  def __init__(self,\n",
        "               n_epochs=40,\n",
        "               amino_acids='CDFGHILNRSVY',\n",
        "               peptide_length=8,\n",
        "               latent_dim=100,\n",
        "               glr=0.001,\n",
        "               dlr=0.001,\n",
        "               batch_size=64,\n",
        "               clip_value=0.01,\n",
        "               n_critic=5):\n",
        "\n",
        "    self.n_epochs=n_epochs\n",
        "    self.amino_acids=amino_acids\n",
        "    self.peptide_length=peptide_length\n",
        "    self.latent_dim=latent_dim\n",
        "    self.glr=glr\n",
        "    self.dlr=dlr\n",
        "    self.batch_size=batch_size\n",
        "    self.clip_value=clip_value\n",
        "    self.n_critic=n_critic\n",
        "\n",
        "wgan_opt = Options()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3zAzwXcZUs",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr3cb1xNcb3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(peptide):\n",
        "  encodings = []\n",
        "  for aa in peptide:\n",
        "    encoding = torch.zeros(len(wgan_opt.amino_acids))\n",
        "    index = wgan_opt.amino_acids.index(aa)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def one_hot_s(peptides):\n",
        "  all_encodings = []\n",
        "  for peptied in peptides:\n",
        "    all_encodings.append(one_hot(peptied))\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def decode_peptide(peptide):\n",
        "  pep = \"\"\n",
        "  for p in peptide:\n",
        "    i = p.argmax()\n",
        "    pep = pep + wgan_opt.amino_acids[i]\n",
        "  \n",
        "  return pep\n",
        "\n",
        "def decode_peptide_s(peptides):\n",
        "  peps = []\n",
        "  for pep in peptides:\n",
        "    peps.append(decode_peptide(pep))\n",
        "  \n",
        "  return peps\n",
        "\n",
        "def load_model(model, file_name):\n",
        "  model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fWc0kHjA_1E",
        "colab_type": "text"
      },
      "source": [
        "# Data Genration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVA16s-OBbQG",
        "colab_type": "code",
        "outputId": "8f695beb-2d08-418b-c863-bbd5fd8aaafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# -------------------\n",
        "# Generate Base Data\n",
        "# -------------------\n",
        "\n",
        "all_sequences = []\n",
        "all_scores = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "      line_count += 1\n",
        "      all_sequences.append(row[0])\n",
        "      all_scores.append(float(row[1]))\n",
        "    \n",
        "#This is the base dataset that everything is based on\n",
        "print(\"Saving {} Sequences...\".format(line_count))\n",
        "np.save(\"All_Peptide_Sequences\", all_sequences)\n",
        "np.save(\"All_Tox_Scores\", all_scores)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving 105416 Sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MICnZydEBjcy",
        "colab_type": "code",
        "outputId": "e16644d3-eaab-498e-e23e-be2a470aee7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# --------------------------------\n",
        "# Punish Canonical Toxic Peptides\n",
        "# --------------------------------\n",
        "def is_canonical_peptide(sequence):\n",
        "  hydrophobicity = 0\n",
        "  cysteines = 0\n",
        "  for aa in sequence:\n",
        "      if aa in [\"F\",\"I\",\"V\",\"L\",\"W\"]:\n",
        "          hydrophobicity = hydrophobicity + 1\n",
        "      if aa == \"C\":\n",
        "          cysteines = cysteines + 1\n",
        "\n",
        "  return (hydrophobicity > 3 or cysteines > 2)\n",
        "\n",
        "def punish_cannon_peptides():\n",
        "    all_sequences = []\n",
        "    all_scores = []\n",
        "    with open('all_data_filtered.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "          line_count += 1\n",
        "          all_sequences.append(row[0])\n",
        "          shift = 0\n",
        "          if is_canonical_peptide(row[0]):\n",
        "            shift = 3\n",
        "          all_scores.append(float(row[1]) + shift)\n",
        "    \n",
        "    return all_sequences, all_scores\n",
        "\n",
        "sequences, scores = punish_cannon_peptides()\n",
        "print(\"Saving {} Sequences...\".format(line_count))\n",
        "np.save(\"All_Peptide_Sequences\", sequences)\n",
        "np.save(\"All_Peptide_Scores_Punished\", scores)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving 105416 Sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9USNuab1A9Yu",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOnGQIlwBlso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GenericDataloader(Dataset):\n",
        "    def __init__(self, data, labels, train=True):\n",
        "      split = len(data)//10\n",
        "      self.data = data[split:]\n",
        "      self.labels = labels[split:]\n",
        "\n",
        "      if not train:\n",
        "        self.data = data[:split]\n",
        "        self.labels = labels[:split]\n",
        "      \n",
        "      if train == \"all\":\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "      return (self.data[index], self.labels[index])\n",
        "    \n",
        "    def __len__(self):\n",
        "      return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nimPpsXBDCb",
        "colab_type": "text"
      },
      "source": [
        "# WGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xntkv3T3CapZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------\n",
        "# Define the Washerstein Generator and Descriminator\n",
        "# ---------------------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(wgan_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(wgan_opt.peptide_length * len(wgan_opt.amino_acids))),\n",
        "            # nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], wgan_opt.peptide_length, len(wgan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(wgan_opt.amino_acids) * wgan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "generator.cuda()\n",
        "\n",
        "discriminator = Discriminator()\n",
        "discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "dataset = GenericDataloader(\n",
        "    one_hot_s(np.load(\"All_Peptide_Sequences.npy\")),\n",
        "    np.load(\"All_Peptide_Scores_Punished.npy\"),\n",
        "    train=\"all\")\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=wgan_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=wgan_opt.glr)\n",
        "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=wgan_opt.dlr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk5naIfNy2N1",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Condition Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Bx7A7xy9Em",
        "colab_type": "code",
        "outputId": "465963e8-88bd-4c37-ba43-4c1a6c6f7f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Preconditioned DataSet\n",
        "sequences = np.load(\"All_Peptide_Sequences.npy\")\n",
        "\n",
        "pre_dataset = GenericDataloader(\n",
        "    np.random.normal(0, 1, (len(sequences), wgan_opt.latent_dim)),\n",
        "    one_hot_s(sequences),\n",
        "    train=\"all\")\n",
        "\n",
        "batch_size = 10000\n",
        "pre_dataloader = torch.utils.data.DataLoader(pre_dataset, batch_size=batch_size, shuffle=True)\n",
        "pre_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)#wgan_opt.glr)\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "losses = []\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = []\n",
        "    for i, (z, peps) in enumerate(pre_dataloader):\n",
        "        peps = Variable(peps.type(Tensor))\n",
        "        z = Variable(z.type(Tensor))\n",
        "\n",
        "        pre_optimizer.zero_grad()\n",
        "\n",
        "        gen_peps = generator(z)\n",
        "        loss = torch.sum(torch.abs(gen_peps - peps))\n",
        "        losses.append(loss.item())\n",
        "        avg_loss.append(loss.item())\n",
        "        # pdb.set_trace()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        pre_optimizer.step()\n",
        "\n",
        "    print(\"{}/{} loss: {:.4f}\".format(epoch+1, epochs, np.mean(avg_loss)))\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(losses, label='losses')\n",
        "plt.title('Losses')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-3a5d84b31a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOZnw5IhvyyP",
        "colab_type": "code",
        "outputId": "58b6eede-91ca-4b01-81d5-1c62aafdd70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "z = torch.Tensor(pre_dataset[1:5][0])\n",
        "z = z.cuda()\n",
        "peptides = generator(z)\n",
        "\n",
        "for i, pep in enumerate(peptides):\n",
        "    print(\"{} == {}\".format(decode_peptide(pep), decode_peptide(pre_dataset[i][1])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VFCVFCCS == CDCHRGFV\n",
            "FCCCCFCG == SRVSLVNI\n",
            "FCVCCCVC == RGLVLGHH\n",
            "FYCCCCGV == CVRGCCGD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsR36un89kDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom Loss\n",
        "def custom_loss(y_hat):\n",
        "  return torch.mean(torch.min(y_hat, dim=2)[0] - torch.max(y_hat, dim=2)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSE1qwWFvEkT",
        "colab_type": "text"
      },
      "source": [
        "## Train the WGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-0svUYN0Q7",
        "colab_type": "code",
        "outputId": "612e4b5d-3b45-48bc-ce20-aeda9219cf39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# --------------------------\n",
        "#  Washerstein GAN Training\n",
        "# --------------------------\n",
        "\n",
        "def was_gan_train():\n",
        "    for epoch in range(wgan_opt.n_epochs):\n",
        "\n",
        "        for i, (peps, _) in enumerate(dataloader):\n",
        "\n",
        "            # Configure input\n",
        "            real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], wgan_opt.latent_dim))))\n",
        "\n",
        "            # Generate a batch of peptides\n",
        "            gen_peps = generator(z).detach()\n",
        "            # Adversarial loss\n",
        "            d_loss = -torch.mean(discriminator(real_peps)) + torch.mean(discriminator(gen_peps))\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Clip weights of discriminator\n",
        "            for p in discriminator.parameters():\n",
        "                p.data.clamp_(-wgan_opt.clip_value, wgan_opt.clip_value)\n",
        "\n",
        "            # Train the generator every n_critic iterations\n",
        "            if i % wgan_opt.n_critic == 0:\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                optimizer_G.zero_grad()\n",
        "\n",
        "                # Generate a batch of images\n",
        "                gen_peps = generator(z)\n",
        "                # Adversarial loss\n",
        "                g_loss = -torch.mean(discriminator(gen_peps))\n",
        "\n",
        "                g_loss.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "        diff = torch.max(gen_peps[0][0]) - torch.min(gen_peps[0][0])\n",
        "        print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, diff {:.4f}\".format(epoch+1, wgan_opt.n_epochs, d_loss.item(), g_loss.item(), diff))\n",
        "\n",
        "was_gan_train()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/40 d loss -0.0566, g loss 0.0214, diff 0.3983\n",
            "epoch 2/40 d loss -0.0421, g loss 0.0316, diff 0.6394\n",
            "epoch 3/40 d loss -0.0482, g loss 0.0357, diff 0.9896\n",
            "epoch 4/40 d loss -0.0166, g loss 0.0305, diff 1.0000\n",
            "epoch 5/40 d loss -0.0458, g loss 0.0365, diff 0.6369\n",
            "epoch 6/40 d loss -0.0483, g loss 0.0284, diff 0.9997\n",
            "epoch 7/40 d loss -0.0331, g loss 0.0261, diff 1.0000\n",
            "epoch 8/40 d loss -0.0675, g loss 0.0232, diff 0.9969\n",
            "epoch 9/40 d loss -0.0255, g loss 0.0211, diff 0.9908\n",
            "epoch 10/40 d loss -0.0316, g loss 0.0306, diff 1.0000\n",
            "epoch 11/40 d loss -0.0511, g loss 0.0275, diff 0.9635\n",
            "epoch 12/40 d loss -0.0438, g loss 0.0282, diff 0.9983\n",
            "epoch 13/40 d loss -0.0413, g loss 0.0243, diff 0.5587\n",
            "epoch 14/40 d loss -0.0306, g loss 0.0221, diff 1.0000\n",
            "epoch 15/40 d loss -0.0303, g loss 0.0236, diff 0.5011\n",
            "epoch 16/40 d loss -0.0292, g loss 0.0155, diff 0.7047\n",
            "epoch 17/40 d loss -0.0235, g loss 0.0225, diff 0.9999\n",
            "epoch 18/40 d loss -0.0626, g loss 0.0277, diff 0.9975\n",
            "epoch 19/40 d loss 0.0052, g loss 0.0269, diff 0.6103\n",
            "epoch 20/40 d loss -0.0368, g loss 0.0312, diff 1.0000\n",
            "epoch 21/40 d loss -0.0499, g loss 0.0248, diff 0.8187\n",
            "epoch 22/40 d loss -0.0345, g loss 0.0210, diff 1.0000\n",
            "epoch 23/40 d loss -0.0294, g loss 0.0252, diff 1.0000\n",
            "epoch 24/40 d loss -0.0252, g loss 0.0196, diff 1.0000\n",
            "epoch 25/40 d loss -0.0559, g loss 0.0231, diff 1.0000\n",
            "epoch 26/40 d loss -0.0418, g loss 0.0242, diff 1.0000\n",
            "epoch 27/40 d loss -0.0157, g loss 0.0192, diff 0.9994\n",
            "epoch 28/40 d loss -0.0083, g loss 0.0241, diff 1.0000\n",
            "epoch 29/40 d loss -0.0339, g loss 0.0196, diff 1.0000\n",
            "epoch 30/40 d loss -0.0234, g loss 0.0187, diff 1.0000\n",
            "epoch 31/40 d loss -0.0422, g loss 0.0206, diff 1.0000\n",
            "epoch 32/40 d loss -0.0191, g loss 0.0211, diff 1.0000\n",
            "epoch 33/40 d loss -0.0223, g loss 0.0256, diff 0.9994\n",
            "epoch 34/40 d loss -0.0090, g loss 0.0221, diff 1.0000\n",
            "epoch 35/40 d loss -0.0250, g loss 0.0166, diff 1.0000\n",
            "epoch 36/40 d loss -0.0097, g loss 0.0264, diff 0.9999\n",
            "epoch 37/40 d loss -0.0520, g loss 0.0232, diff 1.0000\n",
            "epoch 38/40 d loss -0.0243, g loss 0.0220, diff 0.9999\n",
            "epoch 39/40 d loss -0.0516, g loss 0.0244, diff 0.9921\n",
            "epoch 40/40 d loss -0.0403, g loss 0.0255, diff 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GssW0u1kgJ8D",
        "colab_type": "code",
        "outputId": "55f11f0b-23a9-471c-9e5d-d18953d198eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# THIS DOES NOT WORK (Except now it does YA!)\n",
        "\n",
        "def train_generator():\n",
        "    for epoch in range(wgan_opt.n_epochs):\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (wgan_opt.batch_size, wgan_opt.latent_dim))))\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_peps = generator(z)\n",
        "\n",
        "        # Custom Loss\n",
        "        loss = custom_loss(gen_peps)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_G.step()\n",
        "    \n",
        "        print(\"{}/{} loss: {:.4f}\".format(epoch+1, wgan_opt.n_epochs, loss.item()))\n",
        "\n",
        "train_generator()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/300 loss: -0.0066\n",
            "2/300 loss: -0.0907\n",
            "3/300 loss: -0.6091\n",
            "4/300 loss: -0.9160\n",
            "5/300 loss: -0.9167\n",
            "6/300 loss: -0.9529\n",
            "7/300 loss: -0.9608\n",
            "8/300 loss: -0.9649\n",
            "9/300 loss: -0.9657\n",
            "10/300 loss: -0.9645\n",
            "11/300 loss: -0.9587\n",
            "12/300 loss: -0.9628\n",
            "13/300 loss: -0.9601\n",
            "14/300 loss: -0.9684\n",
            "15/300 loss: -0.9725\n",
            "16/300 loss: -0.9695\n",
            "17/300 loss: -0.9658\n",
            "18/300 loss: -0.9728\n",
            "19/300 loss: -0.9765\n",
            "20/300 loss: -0.9835\n",
            "21/300 loss: -0.9785\n",
            "22/300 loss: -0.9793\n",
            "23/300 loss: -0.9749\n",
            "24/300 loss: -0.9735\n",
            "25/300 loss: -0.9774\n",
            "26/300 loss: -0.9786\n",
            "27/300 loss: -0.9845\n",
            "28/300 loss: -0.9792\n",
            "29/300 loss: -0.9851\n",
            "30/300 loss: -0.9840\n",
            "31/300 loss: -0.9852\n",
            "32/300 loss: -0.9758\n",
            "33/300 loss: -0.9872\n",
            "34/300 loss: -0.9847\n",
            "35/300 loss: -0.9798\n",
            "36/300 loss: -0.9859\n",
            "37/300 loss: -0.9872\n",
            "38/300 loss: -0.9874\n",
            "39/300 loss: -0.9789\n",
            "40/300 loss: -0.9819\n",
            "41/300 loss: -0.9882\n",
            "42/300 loss: -0.9838\n",
            "43/300 loss: -0.9870\n",
            "44/300 loss: -0.9856\n",
            "45/300 loss: -0.9874\n",
            "46/300 loss: -0.9802\n",
            "47/300 loss: -0.9881\n",
            "48/300 loss: -0.9823\n",
            "49/300 loss: -0.9863\n",
            "50/300 loss: -0.9845\n",
            "51/300 loss: -0.9850\n",
            "52/300 loss: -0.9872\n",
            "53/300 loss: -0.9840\n",
            "54/300 loss: -0.9884\n",
            "55/300 loss: -0.9789\n",
            "56/300 loss: -0.9875\n",
            "57/300 loss: -0.9820\n",
            "58/300 loss: -0.9875\n",
            "59/300 loss: -0.9860\n",
            "60/300 loss: -0.9854\n",
            "61/300 loss: -0.9824\n",
            "62/300 loss: -0.9835\n",
            "63/300 loss: -0.9872\n",
            "64/300 loss: -0.9901\n",
            "65/300 loss: -0.9896\n",
            "66/300 loss: -0.9832\n",
            "67/300 loss: -0.9904\n",
            "68/300 loss: -0.9866\n",
            "69/300 loss: -0.9813\n",
            "70/300 loss: -0.9865\n",
            "71/300 loss: -0.9881\n",
            "72/300 loss: -0.9802\n",
            "73/300 loss: -0.9854\n",
            "74/300 loss: -0.9877\n",
            "75/300 loss: -0.9848\n",
            "76/300 loss: -0.9885\n",
            "77/300 loss: -0.9867\n",
            "78/300 loss: -0.9912\n",
            "79/300 loss: -0.9891\n",
            "80/300 loss: -0.9792\n",
            "81/300 loss: -0.9835\n",
            "82/300 loss: -0.9877\n",
            "83/300 loss: -0.9900\n",
            "84/300 loss: -0.9876\n",
            "85/300 loss: -0.9889\n",
            "86/300 loss: -0.9808\n",
            "87/300 loss: -0.9907\n",
            "88/300 loss: -0.9835\n",
            "89/300 loss: -0.9887\n",
            "90/300 loss: -0.9889\n",
            "91/300 loss: -0.9892\n",
            "92/300 loss: -0.9918\n",
            "93/300 loss: -0.9878\n",
            "94/300 loss: -0.9905\n",
            "95/300 loss: -0.9903\n",
            "96/300 loss: -0.9887\n",
            "97/300 loss: -0.9809\n",
            "98/300 loss: -0.9909\n",
            "99/300 loss: -0.9853\n",
            "100/300 loss: -0.9928\n",
            "101/300 loss: -0.9877\n",
            "102/300 loss: -0.9904\n",
            "103/300 loss: -0.9910\n",
            "104/300 loss: -0.9877\n",
            "105/300 loss: -0.9844\n",
            "106/300 loss: -0.9889\n",
            "107/300 loss: -0.9902\n",
            "108/300 loss: -0.9859\n",
            "109/300 loss: -0.9890\n",
            "110/300 loss: -0.9916\n",
            "111/300 loss: -0.9843\n",
            "112/300 loss: -0.9891\n",
            "113/300 loss: -0.9871\n",
            "114/300 loss: -0.9881\n",
            "115/300 loss: -0.9899\n",
            "116/300 loss: -0.9878\n",
            "117/300 loss: -0.9878\n",
            "118/300 loss: -0.9900\n",
            "119/300 loss: -0.9870\n",
            "120/300 loss: -0.9915\n",
            "121/300 loss: -0.9916\n",
            "122/300 loss: -0.9862\n",
            "123/300 loss: -0.9903\n",
            "124/300 loss: -0.9897\n",
            "125/300 loss: -0.9920\n",
            "126/300 loss: -0.9880\n",
            "127/300 loss: -0.9928\n",
            "128/300 loss: -0.9908\n",
            "129/300 loss: -0.9839\n",
            "130/300 loss: -0.9902\n",
            "131/300 loss: -0.9924\n",
            "132/300 loss: -0.9892\n",
            "133/300 loss: -0.9880\n",
            "134/300 loss: -0.9914\n",
            "135/300 loss: -0.9862\n",
            "136/300 loss: -0.9917\n",
            "137/300 loss: -0.9951\n",
            "138/300 loss: -0.9927\n",
            "139/300 loss: -0.9917\n",
            "140/300 loss: -0.9922\n",
            "141/300 loss: -0.9870\n",
            "142/300 loss: -0.9910\n",
            "143/300 loss: -0.9898\n",
            "144/300 loss: -0.9930\n",
            "145/300 loss: -0.9950\n",
            "146/300 loss: -0.9931\n",
            "147/300 loss: -0.9851\n",
            "148/300 loss: -0.9875\n",
            "149/300 loss: -0.9920\n",
            "150/300 loss: -0.9898\n",
            "151/300 loss: -0.9930\n",
            "152/300 loss: -0.9958\n",
            "153/300 loss: -0.9958\n",
            "154/300 loss: -0.9907\n",
            "155/300 loss: -0.9847\n",
            "156/300 loss: -0.9896\n",
            "157/300 loss: -0.9939\n",
            "158/300 loss: -0.9882\n",
            "159/300 loss: -0.9957\n",
            "160/300 loss: -0.9879\n",
            "161/300 loss: -0.9924\n",
            "162/300 loss: -0.9882\n",
            "163/300 loss: -0.9949\n",
            "164/300 loss: -0.9911\n",
            "165/300 loss: -0.9911\n",
            "166/300 loss: -0.9885\n",
            "167/300 loss: -0.9874\n",
            "168/300 loss: -0.9919\n",
            "169/300 loss: -0.9923\n",
            "170/300 loss: -0.9940\n",
            "171/300 loss: -0.9900\n",
            "172/300 loss: -0.9917\n",
            "173/300 loss: -0.9915\n",
            "174/300 loss: -0.9918\n",
            "175/300 loss: -0.9891\n",
            "176/300 loss: -0.9904\n",
            "177/300 loss: -0.9927\n",
            "178/300 loss: -0.9955\n",
            "179/300 loss: -0.9934\n",
            "180/300 loss: -0.9959\n",
            "181/300 loss: -0.9968\n",
            "182/300 loss: -0.9949\n",
            "183/300 loss: -0.9884\n",
            "184/300 loss: -0.9775\n",
            "185/300 loss: -0.9900\n",
            "186/300 loss: -0.9916\n",
            "187/300 loss: -0.9946\n",
            "188/300 loss: -0.9920\n",
            "189/300 loss: -0.9948\n",
            "190/300 loss: -0.9909\n",
            "191/300 loss: -0.9968\n",
            "192/300 loss: -0.9944\n",
            "193/300 loss: -0.9935\n",
            "194/300 loss: -0.9921\n",
            "195/300 loss: -0.9958\n",
            "196/300 loss: -0.9970\n",
            "197/300 loss: -0.9953\n",
            "198/300 loss: -0.9929\n",
            "199/300 loss: -0.9957\n",
            "200/300 loss: -0.9919\n",
            "201/300 loss: -0.9924\n",
            "202/300 loss: -0.9937\n",
            "203/300 loss: -0.9931\n",
            "204/300 loss: -0.9945\n",
            "205/300 loss: -0.9978\n",
            "206/300 loss: -0.9945\n",
            "207/300 loss: -0.9943\n",
            "208/300 loss: -0.9954\n",
            "209/300 loss: -0.9896\n",
            "210/300 loss: -0.9915\n",
            "211/300 loss: -0.9950\n",
            "212/300 loss: -0.9943\n",
            "213/300 loss: -0.9955\n",
            "214/300 loss: -0.9906\n",
            "215/300 loss: -0.9885\n",
            "216/300 loss: -0.9893\n",
            "217/300 loss: -0.9939\n",
            "218/300 loss: -0.9928\n",
            "219/300 loss: -0.9897\n",
            "220/300 loss: -0.9900\n",
            "221/300 loss: -0.9932\n",
            "222/300 loss: -0.9954\n",
            "223/300 loss: -0.9883\n",
            "224/300 loss: -0.9920\n",
            "225/300 loss: -0.9953\n",
            "226/300 loss: -0.9927\n",
            "227/300 loss: -0.9891\n",
            "228/300 loss: -0.9927\n",
            "229/300 loss: -0.9910\n",
            "230/300 loss: -0.9940\n",
            "231/300 loss: -0.9955\n",
            "232/300 loss: -0.9939\n",
            "233/300 loss: -0.9942\n",
            "234/300 loss: -0.9928\n",
            "235/300 loss: -0.9905\n",
            "236/300 loss: -0.9947\n",
            "237/300 loss: -0.9949\n",
            "238/300 loss: -0.9942\n",
            "239/300 loss: -0.9939\n",
            "240/300 loss: -0.9949\n",
            "241/300 loss: -0.9933\n",
            "242/300 loss: -0.9897\n",
            "243/300 loss: -0.9961\n",
            "244/300 loss: -0.9962\n",
            "245/300 loss: -0.9951\n",
            "246/300 loss: -0.9877\n",
            "247/300 loss: -0.9927\n",
            "248/300 loss: -0.9932\n",
            "249/300 loss: -0.9908\n",
            "250/300 loss: -0.9938\n",
            "251/300 loss: -0.9938\n",
            "252/300 loss: -0.9944\n",
            "253/300 loss: -0.9912\n",
            "254/300 loss: -0.9960\n",
            "255/300 loss: -0.9927\n",
            "256/300 loss: -0.9948\n",
            "257/300 loss: -0.9917\n",
            "258/300 loss: -0.9939\n",
            "259/300 loss: -0.9960\n",
            "260/300 loss: -0.9922\n",
            "261/300 loss: -0.9949\n",
            "262/300 loss: -0.9938\n",
            "263/300 loss: -0.9928\n",
            "264/300 loss: -0.9929\n",
            "265/300 loss: -0.9882\n",
            "266/300 loss: -0.9962\n",
            "267/300 loss: -0.9971\n",
            "268/300 loss: -0.9916\n",
            "269/300 loss: -0.9955\n",
            "270/300 loss: -0.9926\n",
            "271/300 loss: -0.9935\n",
            "272/300 loss: -0.9891\n",
            "273/300 loss: -0.9917\n",
            "274/300 loss: -0.9861\n",
            "275/300 loss: -0.9889\n",
            "276/300 loss: -0.9915\n",
            "277/300 loss: -0.9966\n",
            "278/300 loss: -0.9918\n",
            "279/300 loss: -0.9981\n",
            "280/300 loss: -0.9950\n",
            "281/300 loss: -0.9953\n",
            "282/300 loss: -0.9931\n",
            "283/300 loss: -0.9909\n",
            "284/300 loss: -0.9916\n",
            "285/300 loss: -0.9942\n",
            "286/300 loss: -0.9931\n",
            "287/300 loss: -0.9913\n",
            "288/300 loss: -0.9956\n",
            "289/300 loss: -0.9968\n",
            "290/300 loss: -0.9946\n",
            "291/300 loss: -0.9955\n",
            "292/300 loss: -0.9941\n",
            "293/300 loss: -0.9983\n",
            "294/300 loss: -0.9978\n",
            "295/300 loss: -0.9925\n",
            "296/300 loss: -0.9935\n",
            "297/300 loss: -0.9940\n",
            "298/300 loss: -0.9959\n",
            "299/300 loss: -0.9948\n",
            "300/300 loss: -0.9934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwVldk1gIcc",
        "colab_type": "text"
      },
      "source": [
        "# WGAN - Generate Z Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhNStyd72N5n",
        "colab_type": "code",
        "outputId": "005491c6-1faa-4a9c-a6a0-c782a0fc19ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "z = Variable(Tensor(np.random.normal(0, 1, (50, wgan_opt.latent_dim))))\n",
        "z.cuda()\n",
        "peps = generator(z)\n",
        "for pep in peps:\n",
        "  print(decode_peptide(pep))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVVSDSGD\n",
            "CFRYVVSC\n",
            "CCGCLCVG\n",
            "CCRFFCDC\n",
            "CFRYIVSC\n",
            "NDSVHRLG\n",
            "CDVRCVGD\n",
            "GIVSCCGF\n",
            "FSCGGSVF\n",
            "RVLDSIGL\n",
            "VGRNGVCI\n",
            "YVDGCGGL\n",
            "SCGYRSVV\n",
            "LHSLVCRG\n",
            "FCIGSFGL\n",
            "NDSLLRRG\n",
            "SNVLDSIH\n",
            "FCYLRYFG\n",
            "ISGNYSVV\n",
            "FYCCVYHY\n",
            "IRDNYCIS\n",
            "CCCDVCFC\n",
            "DGDISHRS\n",
            "FIICFNCC\n",
            "DGDSYNNC\n",
            "IGDNGNCF\n",
            "DGDNFNCV\n",
            "VVHSDRGR\n",
            "IFGCGSVV\n",
            "VCGCGVRV\n",
            "HFRHVVSD\n",
            "CYCVGVFG\n",
            "DGHIDLRS\n",
            "VDCVHRGR\n",
            "FIIFVSHL\n",
            "FIVFCCCC\n",
            "RLYDSIHL\n",
            "FCYFSFDY\n",
            "LHYHVYHN\n",
            "FFFCSFCV\n",
            "VDCVHRRG\n",
            "VIVRGSVF\n",
            "CFVGCGFC\n",
            "FFVVCGHC\n",
            "YCIGSIYH\n",
            "LRNFVYDN\n",
            "RNSHIGSH\n",
            "IHVFFCDC\n",
            "FYCCHVFR\n",
            "VIFCCFCR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "445EwtUigS-P",
        "colab_type": "code",
        "outputId": "f7cb1a45-8073-45b1-9517-681557de601d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Holder of all the z-vectors\n",
        "peptide_z_vector = []\n",
        "\n",
        "loss_func = nn.L1Loss()\n",
        "\n",
        "target, _ = dataset[:1]\n",
        "target = target.cuda()\n",
        "\n",
        "z = torch.Tensor(np.random.normal(0, 1, (2, wgan_opt.latent_dim)))\n",
        "z = z.cuda()\n",
        "\n",
        "optim = torch.optim.Adam([z.requires_grad_()], lr=0.001)\n",
        "\n",
        "epochs = 10000\n",
        "print_every = 1000\n",
        "for epoch in range(epochs):\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    pep = generator(z)\n",
        "    # loss = loss_func(pep, target)\n",
        "    loss = torch.sum(torch.abs(pep - target))\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "    if epoch%print_every == 0:\n",
        "      print(\"{} {:.6f}\".format(decode_peptide(pep[0]), loss.item()))\n",
        "\n",
        "print(\"{} == {}\".format(decode_peptide(pep[0]), decode_peptide(target[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FCCCCCCG 30.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG 28.000000\n",
            "FCCCCCCG == CDCHRGFV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkxVanRcBnQ",
        "colab_type": "code",
        "outputId": "ff7bc82d-0c08-4dc9-e4d7-389aa8a22e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "a = target = torch.stack([one_hot('FFFFFFFF'), one_hot('GGGGGGGG')])\n",
        "z = Variable(Tensor(np.random.normal(0, 1, (64, wgan_opt.latent_dim))))\n",
        "z = z.cuda()\n",
        "b = generator(z)\n",
        "\n",
        "for pep in b:\n",
        "  print(decode_peptide(pep))\n",
        "print(b[0][0])\n",
        "\n",
        "print(\"Loss: \", custom_loss(a))\n",
        "print(\"Loss: \", custom_loss(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YCCCFVYG\n",
            "LICCGFGV\n",
            "LYVFVCHL\n",
            "FFFCGCGG\n",
            "VCGCCVRF\n",
            "CGFCVSGI\n",
            "CYCVCRVR\n",
            "VGVCVGGG\n",
            "VFFFFVCG\n",
            "FCFCDDLG\n",
            "FCDGGDGC\n",
            "GGLFGGRG\n",
            "FFFCVCGG\n",
            "FCIGCVLC\n",
            "CGLFVCGC\n",
            "VCVVCGCC\n",
            "FYYFVYCL\n",
            "FFGYYVCR\n",
            "FVRFCYVF\n",
            "ICGCVGVR\n",
            "CVVSVGDV\n",
            "LICGCFLS\n",
            "GIGYCLCS\n",
            "CYCVRRVV\n",
            "FFCYYGVR\n",
            "VCFCCFGC\n",
            "FFRYVSGG\n",
            "GFGFVLCL\n",
            "CCGYVFCV\n",
            "CVSGLRFD\n",
            "GIVGGVRV\n",
            "FVRYVFCV\n",
            "LCCCGFGC\n",
            "CVCVCFVV\n",
            "GYLVGISV\n",
            "FYDVGRVV\n",
            "VCGCFGCC\n",
            "GIVGCLLG\n",
            "FCDVDCGC\n",
            "LVCGVFGV\n",
            "GGLGGIGC\n",
            "CCVVCFGC\n",
            "CGVGGSGI\n",
            "GVRGLGSD\n",
            "VCVVCGGC\n",
            "CCGRVFGV\n",
            "LFRRVCGG\n",
            "FVSGCRVD\n",
            "LCCCDCGF\n",
            "CFFCVSGG\n",
            "FYCVGCGV\n",
            "VCGVCGDC\n",
            "CVRYLGVD\n",
            "LFSIRCVL\n",
            "VFGFVGYG\n",
            "FCCCGDGG\n",
            "VFGFVGVC\n",
            "FSFGCVLG\n",
            "FICGGDGG\n",
            "GSLGGVIV\n",
            "VCGCFCCR\n",
            "FVSYCFVV\n",
            "GIIFVVIG\n",
            "VCGGLVLS\n",
            "tensor([0.0319, 0.0319, 0.2360, 0.0319, 0.0355, 0.0319, 0.0329, 0.0319, 0.0319,\n",
            "        0.0319, 0.2359, 0.2360], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Loss:  tensor(-1.)\n",
            "Loss:  tensor(-0.2528, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DL0wXh-3nSL",
        "colab_type": "text"
      },
      "source": [
        "# Pepto Finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2rkzqP4Aag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "475a6a1c-c45b-4ec7-aaed-0a52f6dd2abd"
      },
      "source": [
        "# -------------------------\n",
        "#  Train a translation net\n",
        "# -------------------------\n",
        "\n",
        "class TranslateNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TranslateNet, self).__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 1, kernel_size=(1, 3), padding=(0, 1)),\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    ret = self.net(x)\n",
        "    return F.softmax(ret, dim=3)\n",
        "\n",
        "# Define the t_net\n",
        "t_net = TranslateNet()\n",
        "t_net = t_net.cuda()\n",
        "\n",
        "# Define the optimizer\n",
        "t_optim = torch.optim.Adam(t_net.parameters(), lr=0.0001)\n",
        "\n",
        "print_every = 1000\n",
        "rounds = 50*print_every\n",
        "batch_size = 64\n",
        "\n",
        "for r in range(rounds):\n",
        "  if r == rounds*0.80:\n",
        "    t_optim.lr = 0.00001\n",
        "    print(\"Adjusting Learning Rate...\")\n",
        "\n",
        "  x = Variable(Tensor(np.random.random(size=(batch_size,8*12))))\n",
        "  x = x.view(batch_size, 8, -1)\n",
        "  y_truth = F.one_hot(torch.argmax(x, dim=2), 12)\n",
        "\n",
        "  x = x.unsqueeze(1)\n",
        "  y_truth = y_truth.unsqueeze(1)\n",
        "\n",
        "  t_optim.zero_grad()\n",
        "\n",
        "  y_hat = t_net(x)\n",
        "  loss = torch.mean(torch.max(torch.abs(y_hat-y_truth), dim=3)[0])\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  t_optim.step()\n",
        "\n",
        "  if r%print_every == 0:\n",
        "    print(\"{}/{} loss: {:.4f}\".format(r, rounds, loss.item()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/50000 loss: 0.9167\n",
            "1000/50000 loss: 0.1943\n",
            "2000/50000 loss: 0.2353\n",
            "3000/50000 loss: 0.1740\n",
            "4000/50000 loss: 0.1631\n",
            "5000/50000 loss: 0.1855\n",
            "6000/50000 loss: 0.2008\n",
            "7000/50000 loss: 0.1772\n",
            "8000/50000 loss: 0.1596\n",
            "9000/50000 loss: 0.1839\n",
            "10000/50000 loss: 0.1714\n",
            "11000/50000 loss: 0.1895\n",
            "12000/50000 loss: 0.1622\n",
            "13000/50000 loss: 0.1657\n",
            "14000/50000 loss: 0.1524\n",
            "15000/50000 loss: 0.1488\n",
            "16000/50000 loss: 0.1865\n",
            "17000/50000 loss: 0.1842\n",
            "18000/50000 loss: 0.2036\n",
            "19000/50000 loss: 0.1878\n",
            "20000/50000 loss: 0.1591\n",
            "21000/50000 loss: 0.1866\n",
            "22000/50000 loss: 0.2027\n",
            "23000/50000 loss: 0.1848\n",
            "24000/50000 loss: 0.1415\n",
            "25000/50000 loss: 0.1783\n",
            "26000/50000 loss: 0.1486\n",
            "27000/50000 loss: 0.1642\n",
            "28000/50000 loss: 0.1842\n",
            "29000/50000 loss: 0.1923\n",
            "30000/50000 loss: 0.1809\n",
            "31000/50000 loss: 0.1469\n",
            "32000/50000 loss: 0.1704\n",
            "33000/50000 loss: 0.1650\n",
            "34000/50000 loss: 0.1547\n",
            "35000/50000 loss: 0.1858\n",
            "36000/50000 loss: 0.1660\n",
            "37000/50000 loss: 0.1652\n",
            "38000/50000 loss: 0.1492\n",
            "39000/50000 loss: 0.2026\n",
            "Adjusting Learning Rate...\n",
            "40000/50000 loss: 0.1636\n",
            "41000/50000 loss: 0.1919\n",
            "42000/50000 loss: 0.1772\n",
            "43000/50000 loss: 0.1855\n",
            "44000/50000 loss: 0.1644\n",
            "45000/50000 loss: 0.1708\n",
            "46000/50000 loss: 0.1749\n",
            "47000/50000 loss: 0.1674\n",
            "48000/50000 loss: 0.1840\n",
            "49000/50000 loss: 0.1955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GhEtGDH5iEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f4bce686-6160-4881-db45-eaa6266bd789"
      },
      "source": [
        "# Test the trained translation network\n",
        "test = torch.Tensor(np.random.random(size=(5, 1, 8,12)))\n",
        "\n",
        "test = test.cuda()\n",
        "peps = t_net(test)\n",
        "for i, pep in enumerate(peps):\n",
        "  print(\"{}\\n{}\\n\".format(decode_peptide(pep.squeeze()), decode_peptide(test[i].squeeze())))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GVLVNINH\n",
            "GVCVNCNH\n",
            "\n",
            "\n",
            "SDHVSVSR\n",
            "YDHVSYCR\n",
            "\n",
            "\n",
            "VHGHHRRI\n",
            "VHGHHRRC\n",
            "\n",
            "\n",
            "SNGHGRLR\n",
            "SNGYGDLR\n",
            "\n",
            "\n",
            "GFSGRSNL\n",
            "GFSGRSCL\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e-N4N-C4aGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------------------\n",
        "#  Train a toxicity prediction network\n",
        "# -------------------------------------\n",
        "\n",
        "# Options\n",
        "amino_acids = 'CDFGHILNRSVY'\n",
        "peptide_length = 8\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(amino_acids) * peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Loss function\n",
        "toxicity_loss = nn.L1Loss()\n",
        "toxicity_loss.cuda()\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "tox_predictor.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tox = torch.optim.Adam(tox_predictor.parameters(), lr=lr)\n",
        "\n",
        "# Data Loaders\n",
        "tox_data_train = GenericDataloader(one_hot_s(np.load(\"All_Peptide_Sequences.npy\")), np.load(\"All_Peptide_Scores_Punished.npy\"), train=True)\n",
        "tox_dataloader_train = torch.utils.data.DataLoader(tox_data_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "tox_data_test = GenericDataloader(one_hot_s(np.load(\"All_Peptide_Sequences.npy\")), np.load(\"All_Peptide_Scores_Punished.npy\"), train=False)\n",
        "tox_dataloader_test = torch.utils.data.DataLoader(tox_data_test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def accuracy(y_hat, y_truth):\n",
        "  diff = torch.abs(y_hat-y_truth)\n",
        "  count = y_hat.size()[0]\n",
        "  return (count - torch.sum(diff))/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBwbpmem-Dhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "8658bb1b-a24f-4075-f6f3-f688c068f123"
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "# Options\n",
        "losses = []\n",
        "v_losses = []\n",
        "acc = []\n",
        "n_epochs = 5\n",
        "validate_every = 700\n",
        "\n",
        "def tox_train():\n",
        "  loop = tqdm(total=len(tox_dataloader_train) * n_epochs, position=0)\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "      for i, (peps, toxs) in enumerate(tox_dataloader_train):\n",
        "          peps = Variable(peps.type(Tensor))\n",
        "          toxs = Variable(toxs.type(Tensor))\n",
        "          optimizer_tox.zero_grad()\n",
        "\n",
        "          y_hat = tox_predictor(peps)\n",
        "\n",
        "          loss = torch.sum(torch.abs(y_hat.squeeze()-toxs))\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "          optimizer_tox.step()\n",
        "          last_acc = 0\n",
        "          if i % validate_every:\n",
        "            a = []\n",
        "            for v_peps, v_toxs in tox_dataloader_test:\n",
        "              v_peps = Variable(v_peps.type(Tensor))\n",
        "              v_toxs = Variable(v_toxs.type(Tensor))\n",
        "              v_y_hat = tox_predictor(v_peps)\n",
        "              a.append(accuracy(v_y_hat.squeeze(), v_toxs).item())\n",
        "            last_acc = np.mean(a)\n",
        "            acc.append((len(losses), last_acc))\n",
        "\n",
        "          loop.set_description(\"Epoch {}, Batch {}, Toxic_Loss {:.4f}, Accuracy {:.4f}\".format(epoch, i, loss.item(), last_acc))\n",
        "          loop.update()\n",
        "\n",
        "# No need to retrain if we can just load the network from a file\n",
        "tox_train()\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.plot(losses, label='losses')\n",
        "plt.title('Toxicicity Predictor Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "a, b = zip(*acc)\n",
        "plt.plot(a, b, label='accuracy')\n",
        "plt.title('Tocicity Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, Batch 1482, Toxic_Loss 3.4337, Accuracy 0.8578: 100%|| 7415/7415 [22:41<00:00,  5.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WaeOjij4ss1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "5bf8dbe3-6a9d-4325-c0dd-b50ac5f9cc74"
      },
      "source": [
        "# --------------------------\n",
        "#  Optimize a toxic peptide\n",
        "# --------------------------\n",
        "batch = 5\n",
        "\n",
        "# z = torch.Tensor(np.random.random(size=(batch, 1, 8, 12)))\n",
        "z = torch.Tensor(np.random.random(size=(batch, wgan_opt.latent_dim)))\n",
        "z = z.cuda()\n",
        "\n",
        "optim = torch.optim.Adam([z.requires_grad_()], lr=0.0001)\n",
        "\n",
        "epochs = 100000\n",
        "loop = tqdm(total=epochs, position=0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    # pep = t_net(z)\n",
        "    pep = generator(z)\n",
        "\n",
        "    loss = torch.mean(tox_predictor(pep)) * 10000000\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    z.data.clamp_(-3, 3)\n",
        "\n",
        "    loop.set_description(\"Epoch {}/{} Loss {:.4f}\".format(epoch+1, epochs, loss.item()))\n",
        "    loop.update()\n",
        "\n",
        "peps = decode_peptide_s(pep.squeeze(1))\n",
        "one_peps = one_hot_s(peps).unsqueeze(1)\n",
        "one_peps = one_peps.cuda()\n",
        "toxes = tox_predictor(one_peps)\n",
        "\n",
        "for i, tox in enumerate(toxes):\n",
        "  # if tox < 0:\n",
        "    print(\"Final Output: {} {:.6f}\".format(peps[i], tox.item()))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 19060/100000 Loss 2476887.0000:  19%|        | 19059/100000 [01:57<08:27, 159.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-a6a05c029314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-efcb9971d0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pep)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpep_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BElFwCH1tIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fd0d1163-da93-4545-9587-7907d50d97e1"
      },
      "source": [
        "peps = decode_peptide_s(pep.squeeze(1))\n",
        "one_peps = one_hot_s(peps).unsqueeze(1)\n",
        "one_peps = one_peps.cuda()\n",
        "toxes = tox_predictor(one_peps)\n",
        "\n",
        "for i, tox in enumerate(toxes):\n",
        "  # if tox < 0:\n",
        "    print(\"Final Output: {} {:.6f}\".format(peps[i], tox.item()))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Output: FDFGVCGG -0.173559\n",
            "Final Output: ILVFLFFD 2.036880\n",
            "Final Output: HCFCVNNF -0.307703\n",
            "Final Output: SNGVFHHV 0.137907\n",
            "Final Output: FFCCGYSY -0.423607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xQYqGhA85Q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aa22e525-e2c2-43bc-c7c1-82ffa935543b"
      },
      "source": [
        "pep[0]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.8128e-05, 2.8095e-38, 1.7601e-25, 1.0168e-21, 1.8329e-42, 0.0000e+00,\n",
              "         9.9996e-01, 7.4871e-42, 1.5522e-27, 0.0000e+00, 3.3837e-21, 0.0000e+00],\n",
              "        [1.0000e+00, 2.4549e-39, 4.4753e-13, 4.8412e-19, 3.5979e-22, 0.0000e+00,\n",
              "         5.9885e-33, 1.2297e-37, 0.0000e+00, 0.0000e+00, 5.7007e-22, 0.0000e+00],\n",
              "        [2.4161e-10, 7.3152e-04, 9.2720e-40, 6.6184e-36, 2.2421e-27, 0.0000e+00,\n",
              "         9.9927e-01, 1.0059e-10, 0.0000e+00, 1.2913e-16, 1.4013e-45, 2.2353e-32],\n",
              "        [1.7111e-22, 0.0000e+00, 1.3060e-08, 1.0214e-30, 0.0000e+00, 9.9956e-01,\n",
              "         2.5225e-36, 4.3704e-04, 4.0638e-44, 2.2201e-08, 7.9674e-12, 0.0000e+00],\n",
              "        [5.6714e-32, 1.4166e-37, 4.2173e-35, 5.0470e-19, 0.0000e+00, 1.2636e-28,\n",
              "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 6.0846e-20, 0.0000e+00],\n",
              "        [2.5347e-24, 1.8239e-09, 0.0000e+00, 1.0000e+00, 3.6474e-38, 8.3048e-30,\n",
              "         8.9254e-28, 5.2291e-24, 2.9668e-32, 4.9186e-43, 0.0000e+00, 0.0000e+00],\n",
              "        [5.2038e-12, 9.1994e-40, 1.7716e-31, 0.0000e+00, 5.4976e-31, 0.0000e+00,\n",
              "         9.9831e-01, 1.6902e-03, 1.1707e-16, 0.0000e+00, 3.5930e-07, 2.5642e-26],\n",
              "        [4.2326e-19, 2.6438e-04, 9.9851e-01, 1.9990e-05, 0.0000e+00, 0.0000e+00,\n",
              "         4.3863e-14, 5.2362e-17, 1.2030e-03, 3.6770e-38, 9.7600e-22, 1.2838e-10]],\n",
              "       device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkrLdkC8_iBR",
        "colab_type": "text"
      },
      "source": [
        "# Tests and junk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7Cj-Dv_nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TranslateNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TranslateNet, self).__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 1, kernel_size=(1, 3), padding=(0, 1)),\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    ret = self.net(x)\n",
        "    return F.softmax(ret, dim=3)\n",
        "\n",
        "# Define the t_net\n",
        "t_net = TranslateNet()\n",
        "t_net = t_net.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QArNPEFCZ4t",
        "colab_type": "code",
        "outputId": "a5365986-1927-4b57-8df5-8924f5f080f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print_every = 100\n",
        "rounds = 500*print_every\n",
        "batch_size = 64\n",
        "\n",
        "# Define the optimizer\n",
        "t_optim = torch.optim.Adam(t_net.parameters(), lr=0.0001)\n",
        "\n",
        "for r in range(rounds):\n",
        "  if r == 8000:\n",
        "    t_optim.lr = 0.00001\n",
        "    print(\"Adjusting Learning Rate...\")\n",
        "\n",
        "  x = Variable(Tensor(np.random.random(size=(batch_size,8*12))))\n",
        "  x = x.view(batch_size, 8, -1)\n",
        "  y_truth = F.one_hot(torch.argmax(x, dim=2), 12)\n",
        "\n",
        "  x = x.unsqueeze(1)\n",
        "  y_truth = y_truth.unsqueeze(1)\n",
        "\n",
        "  t_optim.zero_grad()\n",
        "\n",
        "  y_hat = t_net(x)\n",
        "  # loss = torch.sum((y_hat - y_truth)*(y_hat - y_truth))\n",
        "  loss = torch.max(torch.max(torch.abs(y_hat-y_truth), dim=3)[0])\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  t_optim.step()\n",
        "\n",
        "  if r%print_every == 0:\n",
        "    print(\"{}/{} loss: {:.4f}\".format(r+1, rounds, loss.item()))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50000 loss: 0.6769\n",
            "101/50000 loss: 0.7085\n",
            "201/50000 loss: 0.6848\n",
            "301/50000 loss: 0.6944\n",
            "401/50000 loss: 0.6562\n",
            "501/50000 loss: 0.7155\n",
            "601/50000 loss: 0.6701\n",
            "701/50000 loss: 0.7113\n",
            "801/50000 loss: 0.7030\n",
            "901/50000 loss: 0.6513\n",
            "1001/50000 loss: 0.6814\n",
            "1101/50000 loss: 0.7390\n",
            "1201/50000 loss: 0.7033\n",
            "1301/50000 loss: 0.6620\n",
            "1401/50000 loss: 0.7345\n",
            "1501/50000 loss: 0.6875\n",
            "1601/50000 loss: 0.6900\n",
            "1701/50000 loss: 0.6732\n",
            "1801/50000 loss: 0.6616\n",
            "1901/50000 loss: 0.6688\n",
            "2001/50000 loss: 0.6876\n",
            "2101/50000 loss: 0.6983\n",
            "2201/50000 loss: 0.7143\n",
            "2301/50000 loss: 0.7167\n",
            "2401/50000 loss: 0.7330\n",
            "2501/50000 loss: 0.6286\n",
            "2601/50000 loss: 0.7142\n",
            "2701/50000 loss: 0.6898\n",
            "2801/50000 loss: 0.6962\n",
            "2901/50000 loss: 0.6818\n",
            "3001/50000 loss: 0.7195\n",
            "3101/50000 loss: 0.7321\n",
            "3201/50000 loss: 0.6895\n",
            "3301/50000 loss: 0.7637\n",
            "3401/50000 loss: 0.6226\n",
            "3501/50000 loss: 0.7214\n",
            "3601/50000 loss: 0.6643\n",
            "3701/50000 loss: 0.7292\n",
            "3801/50000 loss: 0.7230\n",
            "3901/50000 loss: 0.6673\n",
            "4001/50000 loss: 0.7105\n",
            "4101/50000 loss: 0.6851\n",
            "4201/50000 loss: 0.7220\n",
            "4301/50000 loss: 0.7250\n",
            "4401/50000 loss: 0.6413\n",
            "4501/50000 loss: 0.7184\n",
            "4601/50000 loss: 0.7079\n",
            "4701/50000 loss: 0.7134\n",
            "4801/50000 loss: 0.6741\n",
            "4901/50000 loss: 0.6791\n",
            "5001/50000 loss: 0.6940\n",
            "5101/50000 loss: 0.7025\n",
            "5201/50000 loss: 0.7065\n",
            "5301/50000 loss: 0.7535\n",
            "5401/50000 loss: 0.7253\n",
            "5501/50000 loss: 0.6659\n",
            "5601/50000 loss: 0.7375\n",
            "5701/50000 loss: 0.8131\n",
            "5801/50000 loss: 0.6082\n",
            "5901/50000 loss: 0.6824\n",
            "6001/50000 loss: 0.7221\n",
            "6101/50000 loss: 0.7167\n",
            "6201/50000 loss: 0.6120\n",
            "6301/50000 loss: 0.6521\n",
            "6401/50000 loss: 0.7017\n",
            "6501/50000 loss: 0.6099\n",
            "6601/50000 loss: 0.6407\n",
            "6701/50000 loss: 0.7550\n",
            "6801/50000 loss: 0.6322\n",
            "6901/50000 loss: 0.6410\n",
            "7001/50000 loss: 0.6562\n",
            "7101/50000 loss: 0.6502\n",
            "7201/50000 loss: 0.7213\n",
            "7301/50000 loss: 0.6972\n",
            "7401/50000 loss: 0.7328\n",
            "7501/50000 loss: 0.7341\n",
            "7601/50000 loss: 0.7573\n",
            "7701/50000 loss: 0.6291\n",
            "7801/50000 loss: 0.6856\n",
            "7901/50000 loss: 0.6774\n",
            "Adjusting Learning Rate...\n",
            "8001/50000 loss: 0.6930\n",
            "8101/50000 loss: 0.7378\n",
            "8201/50000 loss: 0.6772\n",
            "8301/50000 loss: 0.6513\n",
            "8401/50000 loss: 0.6810\n",
            "8501/50000 loss: 0.6723\n",
            "8601/50000 loss: 0.6453\n",
            "8701/50000 loss: 0.6340\n",
            "8801/50000 loss: 0.6703\n",
            "8901/50000 loss: 0.6882\n",
            "9001/50000 loss: 0.6566\n",
            "9101/50000 loss: 0.6871\n",
            "9201/50000 loss: 0.6546\n",
            "9301/50000 loss: 0.6753\n",
            "9401/50000 loss: 0.6890\n",
            "9501/50000 loss: 0.6364\n",
            "9601/50000 loss: 0.6885\n",
            "9701/50000 loss: 0.7039\n",
            "9801/50000 loss: 0.7009\n",
            "9901/50000 loss: 0.7017\n",
            "10001/50000 loss: 0.6791\n",
            "10101/50000 loss: 0.6316\n",
            "10201/50000 loss: 0.7050\n",
            "10301/50000 loss: 0.6074\n",
            "10401/50000 loss: 0.6947\n",
            "10501/50000 loss: 0.6821\n",
            "10601/50000 loss: 0.6881\n",
            "10701/50000 loss: 0.6903\n",
            "10801/50000 loss: 0.6336\n",
            "10901/50000 loss: 0.6464\n",
            "11001/50000 loss: 0.6631\n",
            "11101/50000 loss: 0.6997\n",
            "11201/50000 loss: 0.6663\n",
            "11301/50000 loss: 0.5615\n",
            "11401/50000 loss: 0.7894\n",
            "11501/50000 loss: 0.6144\n",
            "11601/50000 loss: 0.7047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-03a231349c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mt_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# loss = torch.sum((y_hat - y_truth)*(y_hat - y_truth))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7c625dbce2ea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAE8aXMmOOVS",
        "colab_type": "code",
        "outputId": "49180727-2cf0-48f0-d23e-595cbd6a8a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "test = torch.Tensor(np.random.random(size=(5, 1, 8,12)))\n",
        "\n",
        "# print(torch.argmax(test, dim=2))\n",
        "test = test.cuda()\n",
        "peps = t_net(test)\n",
        "# print(torch.argmax(peps, dim=2))\n",
        "for i, pep in enumerate(peps):\n",
        "  print(\"{}\\n{}\\n\\n\".format(decode_peptide(pep.squeeze()), decode_peptide(test[i].squeeze())))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VNYRCDGI\n",
            "VNYRCDGI\n",
            "\n",
            "\n",
            "HGNLYLSL\n",
            "DGNLYLSL\n",
            "\n",
            "\n",
            "GIFSVCDY\n",
            "CIFSVCDY\n",
            "\n",
            "\n",
            "DRLGHLHV\n",
            "DRLGHLHV\n",
            "\n",
            "\n",
            "VGYRLNSS\n",
            "VGYRLNSS\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCfzbWcUxaVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f68efe73-5856-4611-877c-726a318dcade"
      },
      "source": [
        "# -------------------------------------\n",
        "#  Optimize towards a specific peptide\n",
        "# -------------------------------------\n",
        "\n",
        "dataset = GenericDataloader(one_hot_s(np.load(\"All_Peptide_Sequences.npy\")), np.load(\"All_Peptide_Scores_Punished.npy\"), train=\"all\")\n",
        "\n",
        "# Holder of all the z-vectors\n",
        "peptide_z_vector = []\n",
        "\n",
        "loss_func = nn.L1Loss()\n",
        "\n",
        "target, _ = dataset[:1]\n",
        "target_pep = decode_peptide(target[0])\n",
        "target = target.unsqueeze(0)\n",
        "target = target.cuda()\n",
        "\n",
        "z = torch.Tensor(np.random.random(size=(1, 1, 8, 12)))\n",
        "z = z.cuda()\n",
        "\n",
        "optim = torch.optim.Adam([z.requires_grad_()], lr=0.1)\n",
        "\n",
        "print_every = 500\n",
        "epochs = 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    pep = t_net(z)\n",
        "\n",
        "    loss = tox_predictor(pep)\n",
        "\n",
        "    # loss = torch.sum(torch.abs(pep - target))\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "    if epoch%print_every == 0:\n",
        "      print(\"{} {:.6f}\".format(decode_peptide(pep[0][0]), loss.item()))\n",
        "\n",
        "pep_name = decode_peptide(pep[0][0])\n",
        "encoded_pep = one_hot(pep_name).unsqueeze(0)\n",
        "encoded_pep = encoded_pep.cuda()\n",
        "\n",
        "print(\"{} {:.6f}\".format(pep_name, tox_predictor(encoded_pep).item()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FLHVVRNV -0.313570\n",
            "FFCRYFRF -1.931098\n",
            "FFCIYFRF -1.923435\n",
            "FFCRYFRF -1.910039\n",
            "FFCRFFRF -2.270261\n",
            "FFCRFFRF -2.270038\n",
            "FFCRFFRF -2.271517\n",
            "FFCRFFRF -2.271717\n",
            "FFCRFFRF -2.266730\n",
            "FFCRFFRF -2.264028\n",
            "FFCRFFRF -2.271335\n",
            "FFCRFFRF -2.271784\n",
            "FFCRFFRF -2.271149\n",
            "FFCRFFRF -2.271945\n",
            "FFCRFFRF -2.271731\n",
            "FFCRFFRF -2.272085\n",
            "FFCRFFRF -2.271779\n",
            "FFLRFFRF -2.388973\n",
            "FFLRFFRF -2.388940\n",
            "FFLRFFRF -2.388950\n",
            "FFLRFFRF -2.386034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCvM_nLwLGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "4f412a6a-586e-455d-a6f2-9224fd173dc0"
      },
      "source": [
        "# -------------------------\n",
        "# Toxic Peptide classifier\n",
        "# -------------------------\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(wgan_opt.amino_acids) * wgan_opt.peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "tox_predictor.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToxicityPredictor(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=96, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgV7YyjswrP6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17b9abe7-e6e7-4796-910d-cacf681c14e0"
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "load_model(tox_predictor, \"tox_classifier.pt\")\n",
        "print(\"Network Successfully Loaded!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Successfully Loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
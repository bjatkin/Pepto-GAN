{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pepto-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6TEhVUqY6BklFp3XKn0/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjatkin/Pepto-GAN/blob/master/Pepto_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t762oxqgVgiB",
        "colab_type": "text"
      },
      "source": [
        "# Imports for everything here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj0rDDkUNXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3z4ZQUyC-0",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "Genral functions that have broad use across differnt sections of the code base. Should be functions rather than classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65U29R9UyPnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(peptide):\n",
        "  encodings = []\n",
        "  for aa in peptide:\n",
        "    encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "    index = gan_opt.amino_acids.index(aa)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def one_hot_s(peptides):\n",
        "  all_encodings = []\n",
        "  for peptied in peptides:\n",
        "    all_encodings.append(one_hot(peptied))\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def score_peptide(peptide):\n",
        "  return tox_predictor(one_hot(peptide).unsqueeze(0).cuda()).item()\n",
        "\n",
        "def decode_peptide(peptide):\n",
        "  pep = \"\"\n",
        "  for p in peptide:\n",
        "    i = p.argmax()\n",
        "    pep = pep + gan_opt.amino_acids[i]\n",
        "  \n",
        "  return pep\n",
        "\n",
        "def decode_peptide_s(peptides):\n",
        "  peps = []\n",
        "  for peptide in peptides:\n",
        "    peps.append(decode_peptide(peptide))\n",
        "  \n",
        "  return np.asarray(peps)\n",
        "\n",
        "def generate_random_peptide():\n",
        "  encodings = []\n",
        "  l = len(gan_opt.amino_acids)\n",
        "  for _ in range(l):\n",
        "    encoding = torch.zeros(gan_opt.peptide_length)\n",
        "    index = random.randrange(0, gan_opt.peptide_length)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def generate_random_peptide_s(count):\n",
        "  all_encodings = []\n",
        "  for _ in range(count):\n",
        "    all_encodings.append(generate_random_peptide())\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def save_model(model, file_name):\n",
        "  torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name):\n",
        "  model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J559pSfKXP8K",
        "colab_type": "text"
      },
      "source": [
        "# Network Opts\n",
        "\n",
        "This section should contain global options that configure each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfplpNW_Xhff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Toxic Classifier Options\n",
        "\n",
        "class tox_opt():\n",
        "  n_epochs=10\n",
        "  batch_size=64\n",
        "  lr=0.0005\n",
        "  validate_every=500\n",
        "  load_network = True\n",
        "  load_model_file=\"tox_classifier.pt\"\n",
        "  save_model_file=\"tox_classifier.pt\"\n",
        "\n",
        "\n",
        "# GAN Options\n",
        "\n",
        "class gan_opt():\n",
        "  n_epochs=120\n",
        "  batch_size=64\n",
        "  lr=0.0002\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  peptide_length=8\n",
        "  amino_acids=\"CDFGHILNRSVY\"\n",
        "  d_update_every=100\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"tox_discriminatro.pt\"\n",
        "  save_disc_file=\"tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"tox_generator.pt\"\n",
        "  save_gen_file=\"tox_generator.pt\"\n",
        "  train_gan=True\n",
        "\n",
        "# Washerstein GAN Options\n",
        "\n",
        "class was_opt():\n",
        "  n_epochs=120\n",
        "  batch_size=64\n",
        "  n_critic=5\n",
        "  lr=0.001\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  clip_value=0.01\n",
        "  load_generator=False\n",
        "  load_gen_file=\"was_tox_generator.pt\"\n",
        "  save_gen_file=\"was_tox_generator.pt\"\n",
        "  train_gan=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbN0afPFWAUG",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Classifier\n",
        "\n",
        "the goal of this model is to predict the toxicity score of a peptide. This can then be used to access the success of the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3hy8qDIR2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------\n",
        "# Toxic Peptide classifier\n",
        "# -------------------------\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Loss function\n",
        "toxicity_loss = nn.L1Loss()\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "\n",
        "if cuda:\n",
        "  tox_predictor.cuda()\n",
        "  toxicity_loss.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tox = torch.optim.Adam(tox_predictor.parameters(), lr=tox_opt.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWkYFPZXZn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------\n",
        "# Peptides Dataloader\n",
        "# --------------------\n",
        "class ToxicPeptideDataset(Dataset):\n",
        "  def __init__(self, file_name, labels_file_name=\"\", train=True, soft=False):\n",
        "    if soft:\n",
        "      self.peptides = self.one_soft(np.load(file_name))\n",
        "    else:\n",
        "      self.peptides = self.one_hot(np.load(file_name))\n",
        "\n",
        "    self.use_labels = False\n",
        "    if labels_file_name != \"\":\n",
        "      self.use_labels = True\n",
        "      # These come in as strings and need to be floats\n",
        "      self.labels = [float(l) for l in np.load(labels_file_name)]\n",
        "\n",
        "      # Test train split\n",
        "      split = len(self.labels) // 10\n",
        "      if train:\n",
        "        self.peptides = self.peptides[split:]\n",
        "        self.labels = self.labels[split:]\n",
        "      else:\n",
        "        self.peptides = self.peptides[:split]\n",
        "        self.labels = self.labels[:split]\n",
        "\n",
        "  def one_hot(self, peptides):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1.0\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "  \n",
        "  def one_soft(self, peptides, alpha=0.5):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        cs = len(gan_opt.amino_acids)\n",
        "        encoding = torch.ones(cs) * (alpha/(cs-1))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1 - alpha\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    if self.use_labels:\n",
        "      return self.peptides[index], self.labels[index]\n",
        "    return self.peptides[index]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.peptides)\n",
        "\n",
        "if not tox_opt.load_network:\n",
        "  tox_data_train = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=True)\n",
        "  tox_dataloader_train = torch.utils.data.DataLoader(tox_data_train, batch_size=tox_opt.batch_size, shuffle=True)\n",
        "\n",
        "  tox_data_test = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=False)\n",
        "  tox_dataloader_test = torch.utils.data.DataLoader(tox_data_test, batch_size=tox_opt.batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Z_HRfNjuhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "  diff = torch.abs(y_hat-y_truth)\n",
        "  count = y_hat.size()[0]\n",
        "  return (count - torch.sum(diff))/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsvNAl4UVtt",
        "colab_type": "code",
        "outputId": "0677d082-0b3c-4790-e960-c520c35ba767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "losses = []\n",
        "v_losses = []\n",
        "acc = []\n",
        "def tox_train():\n",
        "  loop = tqdm(total=len(tox_dataloader_train) * tox_opt.n_epochs, position=0)\n",
        "  for epoch in range(tox_opt.n_epochs):\n",
        "\n",
        "      for i, (peps, toxs) in enumerate(tox_dataloader_train):\n",
        "          peps = Variable(peps.type(Tensor))\n",
        "          toxs = Variable(toxs.type(Tensor))\n",
        "          optimizer_tox.zero_grad()\n",
        "\n",
        "          y_hat = tox_predictor(peps)\n",
        "\n",
        "          # loss = toxicity_loss(y_hat.squeeze(), toxs)\n",
        "          # pdb.set_trace()\n",
        "          loss = torch.sum(torch.abs(y_hat.squeeze()-toxs))\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "          optimizer_tox.step()\n",
        "          last_acc = 0\n",
        "          if i % tox_opt.validate_every:\n",
        "            a = []\n",
        "            for v_peps, v_toxs in tox_dataloader_test:\n",
        "              v_peps = Variable(v_peps.type(Tensor))\n",
        "              v_toxs = Variable(v_toxs.type(Tensor))\n",
        "              v_y_hat = tox_predictor(v_peps)\n",
        "              a.append(accuracy(v_y_hat.squeeze(), v_toxs).item())\n",
        "            last_acc = np.mean(a)\n",
        "            acc.append((len(losses), last_acc))\n",
        "\n",
        "          loop.set_description(\"Epoch {}, Batch {}, Toxic_Loss {:.4f}, Accuracy {:.4f}\".format(epoch, i, loss.item(), last_acc))\n",
        "          loop.update()\n",
        "\n",
        "# No need to retrain if we can just load the network from a file\n",
        "if not tox_opt.load_network:\n",
        "  tox_train()\n",
        "  save_model(tox_predictor, tox_opt.save_model_file)\n",
        "\n",
        "  # Plot accuracy and loss\n",
        "  plt.plot(losses, label='losses')\n",
        "  plt.title('Toxicicity Predictor Losses')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  a, b = zip(*acc)\n",
        "  plt.plot(a, b, label='accuracy')\n",
        "  plt.title('Tocicity Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "if tox_opt.load_network:\n",
        "  load_model(tox_predictor, tox_opt.load_model_file)\n",
        "  print(\"Network Successfully Loaded!\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Successfully Loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLI4rR3MabTN",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN\n",
        "\n",
        "this section contains the code to run a simple gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the GAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "## Tweak #1\n",
        "\n",
        "**Problem:**The generator network was really struggling to learn how to produce peptides. The probelm apeard to be that the discriminator was learning too quickly and turning the learning rate for the discriminator down enough to allow the generator to catch up was preventing the discriminator from learning useful information that would help imporve the generator. I hypothesize that the reason for this is that the one hot encodings we were using were too high contrast. Before the discriminator learning for valid peptide configurations or toxicities it instead looked only at contrast. It learned this so quickly that the generator could not adjust in time and was burried by the discriminator.\n",
        "\n",
        "**Fix:**We fixed this by using label smoothing on our one hot peptide encoding.\n",
        "\n",
        "**Result:**This resulted in a marked improvement of the GAN. the discriminator learning rate was increase by a factor of 5 (every 500 steps to every 100 steps). also the number of training epochs that the GAN was able to sustain jumped by an order of magnitued (10 epochs to 100 epocsh)\n",
        "\n",
        "* Unique Peptides Generated: 15\n",
        "* Mean Score: -0.439821\n",
        "* Standard Deviation: 0.219031\n",
        "\n",
        "## Tweak #2\n",
        "**Problem:**(See tweak #1)\n",
        "\n",
        "**Fix:**Change the contrast on the latent z vector so it's very high to see if that promotes high contrast in the output.\n",
        "\n",
        "**Result:**This result did not improve the output in the same way that label smoothing did. Unfortunately, with the same learning rate (100 steps) the number of training epochs that were successful before the Discriminator ran away with things was only about 10.\n",
        "\n",
        "Wait Maybe this did work... Even though It cant train as long...\n",
        "\n",
        "* Unique Peptides Generated: 65\n",
        "* Mean Score: -0.732333\n",
        "* Standard Deviation: 0.177144\n",
        "\n",
        "## Tweak #3\n",
        "**Problem:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Zs2bIlQvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------\n",
        "# Define the Generator and Discriminator\n",
        "# ---------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(gan_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Dataset\n",
        "gan_data = ToxicPeptideDataset(\"newMostToxicNCSequences.npy\", soft=True)\n",
        "gan_dataloader = torch.utils.data.DataLoader(gan_data, batch_size=gan_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPkjy6hTkDZA",
        "colab_type": "code",
        "outputId": "662ad562-9403-485b-c0c8-0b7259757b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# --------------\n",
        "#  GAN Training\n",
        "# --------------\n",
        "\n",
        "def gan_train():\n",
        "  for epoch in range(gan_opt.n_epochs):\n",
        "\n",
        "      for i, peps in enumerate(gan_dataloader):\n",
        "\n",
        "          # Adversarial ground truths\n",
        "          valid = Variable(Tensor(peps.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "          fake = Variable(Tensor(peps.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "          # Configure input\n",
        "          real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Sample noise as generator input\n",
        "          z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Sample high constrast noise as generator input (Tweak #2)\n",
        "          # z = Variable(Tensor(np.random.randint(0, 2, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_peps = generator(z)\n",
        "\n",
        "          # Loss measures generator's ability to fool the discriminator\n",
        "          g_loss = adversarial_loss(discriminator(gen_peps), valid)\n",
        "\n",
        "          g_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator - every d_update_every steps\n",
        "          # ---------------------\n",
        "\n",
        "          batches_done = epoch * len(gan_dataloader) + i\n",
        "          if batches_done % gan_opt.d_update_every == 0:\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "            real_loss = adversarial_loss(discriminator(real_peps), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_peps.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "      print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}\".format(epoch+1, gan_opt.n_epochs, d_loss.item(), g_loss.item()))\n",
        "      for pep in gen_peps:\n",
        "        name = decode_peptide(pep)\n",
        "        score = score_peptide(name)\n",
        "        if score < -0.4 and score > -1:\n",
        "          print(\"{}: {:.6f}\".format(name, score))\n",
        "        if score <= -1:\n",
        "          print(\"#---------------------#\")\n",
        "          print(\"# {}: {:.6f} #\".format(name, score))\n",
        "          print(\"#---------------------#\")\n",
        "\n",
        "if gan_opt.load_discriminator:\n",
        "  load_model(discriminator, gan_opt.load_disc_file)\n",
        "\n",
        "if gan_opt.load_generator:\n",
        "  load_model(generator, gan_opt.load_gen_file)\n",
        "\n",
        "if gan_opt.train_gan:\n",
        "  gan_train()\n",
        "  save_model(discriminator, gan_opt.save_disc_file)\n",
        "  save_model(generator, gan_opt.save_gen_file)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/120 d loss 0.6822, g loss 0.6669\n",
            "FYIFDYDH: -0.487841\n",
            "epoch 2/120 d loss 0.6718, g loss 0.6563\n",
            "FIFFLLGC: -0.991104\n",
            "FIFFLLGC: -0.991104\n",
            "FIFFLSGC: -0.903147\n",
            "epoch 3/120 d loss 0.6569, g loss 0.6598\n",
            "epoch 4/120 d loss 0.6334, g loss 0.6822\n",
            "IYVVHYFV: -0.928313\n",
            "FYVVCNCC: -0.580193\n",
            "IFCVHYCV: -0.470415\n",
            "FYVVCNCC: -0.580193\n",
            "FFVVFNCV: -0.858832\n",
            "epoch 5/120 d loss 0.6282, g loss 0.6593\n",
            "FCLDVVRF: -0.424812\n",
            "FCLGVVRF: -0.417656\n",
            "#---------------------#\n",
            "# FCVFVVRC: -1.053175 #\n",
            "#---------------------#\n",
            "epoch 6/120 d loss 0.5953, g loss 0.7060\n",
            "CCFCVIYF: -0.876874\n",
            "#---------------------#\n",
            "# CCIYIIYF: -1.073743 #\n",
            "#---------------------#\n",
            "CCFCVFYF: -0.862477\n",
            "#---------------------#\n",
            "# VCFYIIYF: -1.098245 #\n",
            "#---------------------#\n",
            "VCICIFYF: -0.963212\n",
            "epoch 7/120 d loss 0.5424, g loss 0.7645\n",
            "#---------------------#\n",
            "# VLIYFFHY: -1.038467 #\n",
            "#---------------------#\n",
            "YHIISSVY: -0.470117\n",
            "YHIISSVY: -0.470117\n",
            "YHIISSVY: -0.470117\n",
            "epoch 8/120 d loss 0.5492, g loss 0.7479\n",
            "FYCFCCCG: -0.628265\n",
            "FYVFCCCG: -0.640252\n",
            "CYVFCCGG: -0.556929\n",
            "FYVFCCCG: -0.640252\n",
            "FYCICVVC: -0.977963\n",
            "epoch 9/120 d loss 0.4755, g loss 0.7188\n",
            "FCGVSVVY: -0.497446\n",
            "FYCICYVR: -0.996593\n",
            "FCVVSYVY: -0.611112\n",
            "epoch 10/120 d loss 0.4410, g loss 0.8499\n",
            "LIGVSVVY: -0.483346\n",
            "LIGVSYLY: -0.419519\n",
            "#---------------------#\n",
            "# CFCIVYLY: -1.000838 #\n",
            "#---------------------#\n",
            "epoch 11/120 d loss 0.4106, g loss 1.0321\n",
            "CFCCDFCF: -0.883468\n",
            "#---------------------#\n",
            "# CFFCVLFF: -1.188177 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# CFFCVLFF: -1.188177 #\n",
            "#---------------------#\n",
            "CFCCGLFF: -0.995793\n",
            "epoch 12/120 d loss 0.4085, g loss 1.0485\n",
            "FFYICGCL: -0.819226\n",
            "FFCCCGCC: -0.595057\n",
            "FFYLCGCL: -0.864594\n",
            "#---------------------#\n",
            "# YFLIFFYL: -1.075466 #\n",
            "#---------------------#\n",
            "FFCICGCC: -0.711736\n",
            "epoch 13/120 d loss 0.2797, g loss 1.3913\n",
            "FCYCFFCC: -0.774788\n",
            "#---------------------#\n",
            "# YFLIYFYL: -1.029025 #\n",
            "#---------------------#\n",
            "YCLLFFYL: -0.907308\n",
            "FCYCYFYC: -0.643729\n",
            "YCYCFFYC: -0.631291\n",
            "epoch 14/120 d loss 0.2902, g loss 1.4642\n",
            "GCVFCVIC: -0.916275\n",
            "#---------------------#\n",
            "# IVVYIVFV: -1.013278 #\n",
            "#---------------------#\n",
            "GVVFIVIC: -0.901123\n",
            "GVVFCVIC: -0.983254\n",
            "#---------------------#\n",
            "# IVVYIVFV: -1.013278 #\n",
            "#---------------------#\n",
            "epoch 15/120 d loss 0.2645, g loss 1.5528\n",
            "VCFLLCLY: -0.822493\n",
            "#---------------------#\n",
            "# FCFVCCLY: -1.016549 #\n",
            "#---------------------#\n",
            "VCFLICLY: -0.931396\n",
            "VCFVCCLY: -0.882598\n",
            "FCFLCCLY: -0.931038\n",
            "epoch 16/120 d loss 0.2352, g loss 1.6576\n",
            "CYCCYGCG: -0.579928\n",
            "CYCCFGCG: -0.584981\n",
            "CCCCFGCG: -0.475419\n",
            "epoch 17/120 d loss 0.2056, g loss 1.7720\n",
            "FYIYRRVV: -0.440558\n",
            "#---------------------#\n",
            "# LIICVIVV: -1.003785 #\n",
            "#---------------------#\n",
            "epoch 18/120 d loss 0.2071, g loss 1.6700\n",
            "YFYGVFYF: -0.957395\n",
            "#---------------------#\n",
            "# YFYFHFYF: -1.301116 #\n",
            "#---------------------#\n",
            "YFYFCFYD: -0.549915\n",
            "YFRFCDSD: -0.461469\n",
            "epoch 19/120 d loss 0.1258, g loss 2.2953\n",
            "YFYGVFDF: -0.426291\n",
            "YFYFCFDF: -0.924884\n",
            "#---------------------#\n",
            "# YFYFVFSF: -1.036039 #\n",
            "#---------------------#\n",
            "VFLFCVSC: -0.917618\n",
            "epoch 20/120 d loss 0.1505, g loss 1.9798\n",
            "CCLLYCYY: -0.776105\n",
            "CCVVFCYY: -0.775775\n",
            "CVLLYCYY: -0.872059\n",
            "CVVCCCYY: -0.640094\n",
            "CCVVCCYC: -0.677253\n",
            "epoch 21/120 d loss 0.1017, g loss 2.2802\n",
            "CCGCCCCC: -0.426150\n",
            "CCGCCCCC: -0.426150\n",
            "CCVCFCCC: -0.455302\n",
            "CCGCFCCC: -0.522161\n",
            "CCVCFCCC: -0.455302\n",
            "epoch 22/120 d loss 0.0629, g loss 2.8866\n",
            "CCGCFCCC: -0.522161\n",
            "CCGCCCCC: -0.426150\n",
            "CCGCCCCC: -0.426150\n",
            "CCGCFCCC: -0.522161\n",
            "CCGCCCCC: -0.426150\n",
            "epoch 23/120 d loss 0.1202, g loss 1.8342\n",
            "#---------------------#\n",
            "# FYGFCFFF: -1.061848 #\n",
            "#---------------------#\n",
            "FYGCCFFF: -0.979508\n",
            "FYFCGGCL: -0.528893\n",
            "#---------------------#\n",
            "# FFGFCFFF: -1.414433 #\n",
            "#---------------------#\n",
            "epoch 24/120 d loss 0.1004, g loss 2.8167\n",
            "FYFYGGGG: -0.462699\n",
            "FYFFGFGG: -0.591904\n",
            "FYFCGGGG: -0.447454\n",
            "#---------------------#\n",
            "# FYFYGFFL: -1.150737 #\n",
            "#---------------------#\n",
            "epoch 25/120 d loss 0.0680, g loss 2.9065\n",
            "FGCFGLFL: -0.738506\n",
            "epoch 26/120 d loss 0.0984, g loss 2.1704\n",
            "YFFVYVVY: -0.794636\n",
            "YCLVYYLY: -0.985912\n",
            "YCFVYVVY: -0.791636\n",
            "YFCVYVVY: -0.902062\n",
            "LFLVYYLC: -0.872410\n",
            "epoch 27/120 d loss 0.0710, g loss 2.9762\n",
            "LCLLYYVY: -0.939951\n",
            "CFCVYVVC: -0.955727\n",
            "LFCVYVVC: -0.972260\n",
            "LCLLYYVY: -0.939951\n",
            "CCLVYYVY: -0.901493\n",
            "epoch 28/120 d loss 0.0405, g loss 3.2992\n",
            "#---------------------#\n",
            "# FFFSVVVR: -1.138527 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# FFFSVVVR: -1.138527 #\n",
            "#---------------------#\n",
            "LFCSVVVC: -0.448806\n",
            "#---------------------#\n",
            "# LFFVVYVR: -1.103322 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LFFVVYVR: -1.103322 #\n",
            "#---------------------#\n",
            "epoch 29/120 d loss 0.0363, g loss 3.3764\n",
            "FFVSVVCC: -0.877759\n",
            "FFVSVVCC: -0.877759\n",
            "LFVSVVVC: -0.970939\n",
            "FFCSVVVY: -0.980130\n",
            "FFCSVVVY: -0.980130\n",
            "epoch 30/120 d loss 0.0388, g loss 3.5126\n",
            "#---------------------#\n",
            "# LFVCVFFF: -1.053992 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LFVCVFCF: -1.018961 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# FFVCVFFF: -1.115940 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LFVCVFFF: -1.053992 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LFVCVFCF: -1.018961 #\n",
            "#---------------------#\n",
            "epoch 31/120 d loss 0.0267, g loss 3.7816\n",
            "#---------------------#\n",
            "# FFFCVFFF: -1.312832 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# FFFCVFFF: -1.312832 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# FFFCVFFF: -1.312832 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# FFFCVFFF: -1.312832 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LYVCVFFF: -1.020149 #\n",
            "#---------------------#\n",
            "epoch 32/120 d loss 0.0529, g loss 3.4077\n",
            "HRVCCCFF: -0.693787\n",
            "#---------------------#\n",
            "# HYVCCFFF: -1.025440 #\n",
            "#---------------------#\n",
            "VRVCCCFF: -0.937426\n",
            "VRVCCCRF: -0.501295\n",
            "#---------------------#\n",
            "# VRVCICRF: -1.143888 #\n",
            "#---------------------#\n",
            "epoch 33/120 d loss 0.0409, g loss 3.3926\n",
            "VCSCCCYL: -0.586449\n",
            "CISCCCYF: -0.775635\n",
            "VIYYICYL: -0.902403\n",
            "VCYCICYL: -0.690637\n",
            "CISCCCYF: -0.775635\n",
            "epoch 34/120 d loss 0.0286, g loss 3.7788\n",
            "CCYCCCYL: -0.582965\n",
            "VCYFICYL: -0.813809\n",
            "VCYCICYL: -0.690637\n",
            "VCYFCCYL: -0.799277\n",
            "CCYFCCYL: -0.765334\n",
            "epoch 35/120 d loss 0.0206, g loss 3.4355\n",
            "VCYCICYL: -0.690637\n",
            "VCLCCGYL: -0.699058\n",
            "VCYCCCYL: -0.617993\n",
            "epoch 36/120 d loss 0.0244, g loss 3.9849\n",
            "epoch 37/120 d loss 0.0193, g loss 4.2135\n",
            "epoch 38/120 d loss 0.0246, g loss 3.9671\n",
            "CYIVYYCV: -0.880812\n",
            "IYIFFYCV: -0.947334\n",
            "IYIFYYCV: -0.931672\n",
            "FYIFYYCV: -0.961973\n",
            "epoch 39/120 d loss 0.0219, g loss 4.0099\n",
            "CYIFYFCV: -0.907296\n",
            "FYIFYFCV: -0.971461\n",
            "IYIFFFCV: -0.922407\n",
            "IYIFFFCV: -0.922407\n",
            "FYIFYFCY: -0.990116\n",
            "epoch 40/120 d loss 0.0182, g loss 3.9876\n",
            "FYIFYFGG: -0.866228\n",
            "FYIFYFGG: -0.866228\n",
            "CFIFYFGG: -0.878808\n",
            "IFIGFFCV: -0.958673\n",
            "IFIGFFCV: -0.958673\n",
            "epoch 41/120 d loss 0.0239, g loss 3.8225\n",
            "CLVGFFFI: -0.939007\n",
            "ILVGFFFI: -0.993960\n",
            "ILVGFFFI: -0.993960\n",
            "ILVGFFFI: -0.993960\n",
            "CLVGFFFI: -0.939007\n",
            "epoch 42/120 d loss 0.0170, g loss 4.4095\n",
            "ILVGFFYI: -0.929054\n",
            "ILGGFFYI: -0.892756\n",
            "epoch 43/120 d loss 0.0119, g loss 4.5494\n",
            "ILVVFSYI: -0.931022\n",
            "CLVGFFFI: -0.939007\n",
            "CLVGFFFI: -0.939007\n",
            "ILGGFFYI: -0.892756\n",
            "CLVGFFFI: -0.939007\n",
            "epoch 44/120 d loss 0.0098, g loss 4.8239\n",
            "YLGVFFYI: -0.855522\n",
            "ILVGFSYI: -0.605577\n",
            "YLVVFSYI: -0.810463\n",
            "ILGGFFYI: -0.892756\n",
            "epoch 45/120 d loss 0.0082, g loss 4.9680\n",
            "ILVVFSYI: -0.931022\n",
            "ILVVFSYI: -0.931022\n",
            "ILVGFFYI: -0.929054\n",
            "epoch 46/120 d loss 0.0076, g loss 5.0930\n",
            "ILGGFFYI: -0.892756\n",
            "ILVVFSYI: -0.931022\n",
            "ILGVFSYI: -0.496919\n",
            "epoch 47/120 d loss 0.0259, g loss 3.6308\n",
            "CCLICVGY: -0.758999\n",
            "CCLFCYLY: -0.887040\n",
            "CVLFCYGY: -0.706133\n",
            "CCLFCVLY: -0.920083\n",
            "epoch 48/120 d loss 0.0308, g loss 3.4644\n",
            "FFFRVVDC: -0.424124\n",
            "epoch 49/120 d loss 0.0138, g loss 4.5532\n",
            "FFFRVVDC: -0.424124\n",
            "FFFRDCDC: -0.469357\n",
            "FVFRCVDC: -0.444737\n",
            "FFFRDCDC: -0.469357\n",
            "epoch 50/120 d loss 0.0126, g loss 4.5239\n",
            "FFFRVVDC: -0.424124\n",
            "FFFRVVVD: -0.501489\n",
            "FFFRVVDC: -0.424124\n",
            "FFFRVVDC: -0.424124\n",
            "epoch 51/120 d loss 0.0097, g loss 4.8269\n",
            "YFVVVVDC: -0.867339\n",
            "YFFRVVDC: -0.510464\n",
            "#---------------------#\n",
            "# FFVRVCVC: -1.118418 #\n",
            "#---------------------#\n",
            "FFVVVVDC: -0.893975\n",
            "epoch 52/120 d loss 0.0100, g loss 4.6507\n",
            "FFVVCCDC: -0.585467\n",
            "YFVVVCVC: -0.913568\n",
            "FFVVCCDC: -0.585467\n",
            "YFVVCVVD: -0.816762\n",
            "YFVVVVDD: -0.461164\n",
            "epoch 53/120 d loss 0.0114, g loss 4.5974\n",
            "YFFLRDVR: -0.549291\n",
            "FFFVRCVR: -0.613373\n",
            "FFFVRCVR: -0.613373\n",
            "YFFLRDVR: -0.549291\n",
            "FFFVRCVR: -0.613373\n",
            "epoch 54/120 d loss 0.0072, g loss 4.9691\n",
            "YFVLRCVR: -0.701906\n",
            "YFFVRCVR: -0.542266\n",
            "epoch 55/120 d loss 0.0064, g loss 5.1562\n",
            "YFVVRCVR: -0.466856\n",
            "YVVLRCVR: -0.400193\n",
            "epoch 56/120 d loss 0.0048, g loss 5.4878\n",
            "FVFLRCVC: -0.682533\n",
            "FVFLRCVR: -0.456648\n",
            "FVFLRCVR: -0.456648\n",
            "FVFLRCVR: -0.456648\n",
            "FVFLRCVC: -0.682533\n",
            "epoch 57/120 d loss 0.0103, g loss 4.6206\n",
            "YFIGYFCV: -0.901854\n",
            "FLICYCIV: -0.896056\n",
            "FLICYFIV: -0.922585\n",
            "FFCCYCIV: -0.881044\n",
            "YFIGYFCV: -0.901854\n",
            "epoch 58/120 d loss 0.0199, g loss 4.0693\n",
            "FYCCGICF: -0.876162\n",
            "FYCCGICF: -0.876162\n",
            "FNCCGICF: -0.579842\n",
            "epoch 59/120 d loss 0.0127, g loss 4.7794\n",
            "FCDCGIYF: -0.410408\n",
            "GCCCLIYF: -0.814943\n",
            "GCCCLIYF: -0.814943\n",
            "GCCLLIYF: -0.962392\n",
            "epoch 60/120 d loss 0.0082, g loss 4.9403\n",
            "FYCCGICF: -0.876162\n",
            "FYCCGICF: -0.876162\n",
            "FYCCGGCF: -0.794390\n",
            "epoch 61/120 d loss 0.0065, g loss 5.1757\n",
            "FCCCLGYF: -0.743601\n",
            "epoch 62/120 d loss 0.0053, g loss 5.3317\n",
            "FCCCGGYF: -0.743680\n",
            "FYCCGGYF: -0.614421\n",
            "FYCLLIYF: -0.976155\n",
            "#---------------------#\n",
            "# FYCLLIVF: -1.002523 #\n",
            "#---------------------#\n",
            "FYCLLIYF: -0.976155\n",
            "epoch 63/120 d loss 0.0049, g loss 5.5024\n",
            "FCCCGGYF: -0.743680\n",
            "FYCCGIVF: -0.856839\n",
            "FYCLGIVF: -0.945058\n",
            "epoch 64/120 d loss 0.0074, g loss 5.0889\n",
            "VYCCGGCC: -0.595239\n",
            "FYCCGGCC: -0.752374\n",
            "FCCCGGCC: -0.617920\n",
            "VYCCGGCC: -0.595239\n",
            "FYCCGGCC: -0.752374\n",
            "epoch 65/120 d loss 0.0053, g loss 5.4562\n",
            "FYCCGGCC: -0.752374\n",
            "VYCCGGVC: -0.433389\n",
            "FCCCGGCC: -0.617920\n",
            "VYCCGGCC: -0.595239\n",
            "FYCCGGVC: -0.517868\n",
            "epoch 66/120 d loss 0.0037, g loss 5.6529\n",
            "VYCCGGCC: -0.595239\n",
            "FYCCGGCC: -0.752374\n",
            "VCCCGGCC: -0.522400\n",
            "VYCCGGCC: -0.595239\n",
            "VYCCGGCC: -0.595239\n",
            "epoch 67/120 d loss 0.0036, g loss 5.7731\n",
            "GYCCGGVC: -0.421805\n",
            "VCCCGGCC: -0.522400\n",
            "VCCCGGCC: -0.522400\n",
            "GYCCGGVC: -0.421805\n",
            "VYCCGGVC: -0.433389\n",
            "epoch 68/120 d loss 0.0033, g loss 5.8836\n",
            "GYCCGGVC: -0.421805\n",
            "FYCCGGVC: -0.517868\n",
            "FYCCGGCC: -0.752374\n",
            "GYCCGGVC: -0.421805\n",
            "FCCCGGCC: -0.617920\n",
            "epoch 69/120 d loss 0.0028, g loss 6.0130\n",
            "FYCCGGVC: -0.517868\n",
            "GYCCGGVC: -0.421805\n",
            "GYCCGGCC: -0.535881\n",
            "GYCCGGVC: -0.421805\n",
            "GYCCGGVC: -0.421805\n",
            "epoch 70/120 d loss 0.0097, g loss 4.5891\n",
            "CLCYFSYY: -0.738565\n",
            "CLCYFSYY: -0.738565\n",
            "CLCYFSYY: -0.738565\n",
            "CLCYFSYY: -0.738565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-73a6d4c632e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgan_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0mgan_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_disc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_gen_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-73a6d4c632e3>\u001b[0m in \u001b[0;36mgan_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_peps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m           \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m           \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oFZxNkWsdE_",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN - Washerstein GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhAjsusVm31m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------\n",
        "# Define the Washerstein Generator and Descriminator\n",
        "# ---------------------------------\n",
        "class WGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WGenerator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(was_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "class WDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WDiscriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "w_generator = WGenerator()\n",
        "w_discriminator = WDiscriminator()\n",
        "\n",
        "if cuda:\n",
        "    w_generator.cuda()\n",
        "    w_discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "w_gan_data = ToxicPeptideDataset(\"newMostToxicNCSequences.npy\", soft=True)\n",
        "w_gan_dataloader = torch.utils.data.DataLoader(w_gan_data, batch_size=was_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "w_optimizer_G = torch.optim.RMSprop(w_generator.parameters(), lr=was_opt.lr)\n",
        "w_optimizer_D = torch.optim.RMSprop(w_discriminator.parameters(), lr=was_opt.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1lLnDwssi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ad2ed6b-465d-4fc0-f557-e65c5b7f5891"
      },
      "source": [
        "# --------------------------\n",
        "#  Washerstein GAN Training\n",
        "# --------------------------\n",
        "\n",
        "for epoch in range(was_opt.n_epochs):\n",
        "\n",
        "    for i, peps in enumerate(w_gan_dataloader):\n",
        "\n",
        "        # Configure input\n",
        "        real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        w_optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], was_opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of peptides\n",
        "        gen_peps = w_generator(z).detach()\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(w_discriminator(real_peps)) + torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "        d_loss.backward()\n",
        "        w_optimizer_D.step()\n",
        "\n",
        "        # Clip weights of discriminator\n",
        "        for p in w_discriminator.parameters():\n",
        "            p.data.clamp_(-was_opt.clip_value, was_opt.clip_value)\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % was_opt.n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            w_optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_peps = w_generator(z)\n",
        "            # Adversarial loss\n",
        "            g_loss = -torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "            g_loss.backward()\n",
        "            w_optimizer_G.step()\n",
        "\n",
        "    unique, mean, std = evaluate_gan(w_generator)\n",
        "    print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, was_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "    for pep in gen_peps:\n",
        "        name = decode_peptide(pep)\n",
        "        score = score_peptide(name)\n",
        "        if score <= -1:\n",
        "            print(\"#---------------------#\")\n",
        "            print(\"# {}: {:.6f} #\".format(name, score))\n",
        "            print(\"#---------------------#\")\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/120 d loss 0.0035, g loss -0.0250, unique % 76.3000, mean: -0.6012, std: 0.2517\n",
            "YFYYVFHY: -0.830692\n",
            "FCYFCCGY: -0.720864\n",
            "CSCCGCLF: -0.738111\n",
            "FCCVLSCL: -0.662414\n",
            "FYFVCGCV: -0.704689\n",
            "epoch 2/120 d loss -0.0076, g loss -0.0242, unique % 70.0000, mean: -0.6072, std: 0.2796\n",
            "NYVFVVFF: -0.907981\n",
            "YIDCFVYF: -0.664242\n",
            "#---------------------#\n",
            "# FFYYCFGI: -1.093925 #\n",
            "#---------------------#\n",
            "SYFGLCLF: -0.576327\n",
            "FCLCCCGI: -0.668881\n",
            "epoch 3/120 d loss -0.0044, g loss -0.0247, unique % 70.7000, mean: -0.6380, std: 0.2853\n",
            "LFVCGCGY: -0.461138\n",
            "FCICCSNI: -0.634364\n",
            "ICCLLILG: -0.726356\n",
            "CFCCVFYV: -0.867083\n",
            "FYYFGCVY: -0.442133\n",
            "epoch 4/120 d loss -0.0305, g loss -0.0300, unique % 71.6000, mean: -0.6441, std: 0.2928\n",
            "VSLVCIYG: -0.572658\n",
            "#---------------------#\n",
            "# FYGFRFFV: -1.174257 #\n",
            "#---------------------#\n",
            "LFVCYYGY: -0.734607\n",
            "epoch 5/120 d loss -0.0339, g loss -0.0226, unique % 75.0000, mean: -0.6123, std: 0.2989\n",
            "#---------------------#\n",
            "# FFIVLSFI: -1.156998 #\n",
            "#---------------------#\n",
            "LYVFGVFY: -0.633343\n",
            "YCCCCFHC: -0.510412\n",
            "FFCCCFHN: -0.627751\n",
            "VCCHLYCI: -0.655985\n",
            "epoch 6/120 d loss -0.0193, g loss -0.0328, unique % 66.5000, mean: -0.6925, std: 0.2922\n",
            "FFFGCCLI: -0.934540\n",
            "YLDFVVFC: -0.662760\n",
            "CCCCFCHY: -0.501909\n",
            "VCLRCCVI: -0.607580\n",
            "epoch 7/120 d loss 0.0024, g loss -0.0505, unique % 75.7000, mean: -0.6554, std: 0.2701\n",
            "FYNYCNHY: -0.542188\n",
            "#---------------------#\n",
            "# VYLLCYLF: -1.003446 #\n",
            "#---------------------#\n",
            "CCCLICGF: -0.749813\n",
            "FFFVRSYC: -0.457980\n",
            "CLVFVVFC: -0.963657\n",
            "epoch 8/120 d loss -0.0375, g loss -0.0541, unique % 74.3000, mean: -0.5913, std: 0.2825\n",
            "FCCVGCCV: -0.664151\n",
            "CLFCVGYV: -0.670864\n",
            "VYYFLYVL: -0.983633\n",
            "CFYCVFHY: -0.852957\n",
            "FDFVHHLS: -0.430630\n",
            "epoch 9/120 d loss -0.0653, g loss -0.0221, unique % 75.6000, mean: -0.6692, std: 0.2511\n",
            "LICVCFLH: -0.807975\n",
            "FCNYFCVY: -0.489700\n",
            "CCCCYYGL: -0.689255\n",
            "VYVFVIHY: -0.864083\n",
            "epoch 10/120 d loss -0.0181, g loss -0.0234, unique % 76.9000, mean: -0.6643, std: 0.2582\n",
            "VCVCYYDI: -0.450968\n",
            "VYLFVGVY: -0.668364\n",
            "CIVCGFYV: -0.777450\n",
            "#---------------------#\n",
            "# IFLLCLLL: -1.039469 #\n",
            "#---------------------#\n",
            "epoch 11/120 d loss 0.0022, g loss -0.0219, unique % 75.8000, mean: -0.6310, std: 0.2816\n",
            "CVFCFGYV: -0.673168\n",
            "CFCVGFLG: -0.403327\n",
            "FCCYLCCL: -0.813776\n",
            "CCVCVCGY: -0.604127\n",
            "VYYFVIHY: -0.866891\n",
            "epoch 12/120 d loss -0.0268, g loss -0.0157, unique % 73.6000, mean: -0.6100, std: 0.2587\n",
            "FYVFYVCL: -0.891037\n",
            "VCCGCYLL: -0.750782\n",
            "FCNYFNNY: -0.468084\n",
            "epoch 13/120 d loss -0.0022, g loss -0.0139, unique % 73.6000, mean: -0.6284, std: 0.2380\n",
            "GCCCIYGL: -0.549892\n",
            "CYFCCCYC: -0.638548\n",
            "FCNFCFHY: -0.652810\n",
            "CLFCGGFV: -0.462022\n",
            "epoch 14/120 d loss -0.0489, g loss -0.0303, unique % 76.8000, mean: -0.6414, std: 0.2941\n",
            "FYNFCCDV: -0.498134\n",
            "VFYYVIVI: -0.927048\n",
            "YFVIVFGV: -0.910945\n",
            "YLVCVGVV: -0.806605\n",
            "IYCGYCCN: -0.624779\n",
            "epoch 15/120 d loss -0.0779, g loss -0.0478, unique % 71.4000, mean: -0.6578, std: 0.2607\n",
            "FCCFCCDN: -0.529877\n",
            "CVFCFGYC: -0.705352\n",
            "LCVFINVY: -0.847957\n",
            "FFHYVFNY: -0.623526\n",
            "YIVCYVYF: -0.936138\n",
            "epoch 16/120 d loss -0.0136, g loss -0.0136, unique % 78.3000, mean: -0.6237, std: 0.2580\n",
            "CGCVCGYC: -0.548531\n",
            "LLVCGVIF: -0.798043\n",
            "VFYYVLHH: -0.797372\n",
            "FCVYVYHY: -0.950522\n",
            "epoch 17/120 d loss -0.0296, g loss -0.0112, unique % 80.4000, mean: -0.5882, std: 0.2723\n",
            "VCCFCCHN: -0.611696\n",
            "YLDVHFYD: -0.404116\n",
            "CVFYGFVV: -0.699371\n",
            "LCVYGIVY: -0.472591\n",
            "epoch 18/120 d loss -0.0040, g loss -0.0087, unique % 80.5000, mean: -0.6054, std: 0.3113\n",
            "ILFFVSVY: -0.949462\n",
            "FFFFCFVN: -0.868351\n",
            "CCVCLCGF: -0.671312\n",
            "epoch 19/120 d loss -0.0182, g loss -0.0324, unique % 78.5000, mean: -0.6458, std: 0.3027\n",
            "CNVCICGV: -0.480916\n",
            "CVCCYVIC: -0.773101\n",
            "VYFFCCHN: -0.569422\n",
            "#---------------------#\n",
            "# FFLVRLFV: -1.503160 #\n",
            "#---------------------#\n",
            "#---------------------#\n",
            "# LFILHIIF: -1.184799 #\n",
            "#---------------------#\n",
            "epoch 20/120 d loss -0.0065, g loss -0.0123, unique % 77.1000, mean: -0.6353, std: 0.2992\n",
            "LSIISVIF: -0.940017\n",
            "CYFCVFFV: -0.991055\n",
            "FCCCLYGF: -0.794020\n",
            "YICLNFYH: -0.447612\n",
            "VYYFCYVN: -0.823058\n",
            "epoch 21/120 d loss -0.0194, g loss -0.0120, unique % 76.6000, mean: -0.6481, std: 0.3190\n",
            "IYLGLHCL: -0.417315\n",
            "LCVCLYGF: -0.865524\n",
            "VFYFRLHN: -0.416051\n",
            "CVVCGFVY: -0.601207\n",
            "epoch 22/120 d loss -0.0085, g loss -0.0594, unique % 80.7000, mean: -0.6520, std: 0.3087\n",
            "YCYSHNHF: -0.418410\n",
            "IFLLLDLG: -0.581289\n",
            "FYFFVYHN: -0.786138\n",
            "CNCCGGCV: -0.568287\n",
            "#---------------------#\n",
            "# YVVCVVIF: -1.022714 #\n",
            "#---------------------#\n",
            "epoch 23/120 d loss -0.0135, g loss -0.0476, unique % 78.3000, mean: -0.6692, std: 0.2979\n",
            "#---------------------#\n",
            "# VFIFRLVL: -1.085046 #\n",
            "#---------------------#\n",
            "CYFVCGCV: -0.630338\n",
            "CYNCGGFY: -0.405013\n",
            "FCVYIFNF: -0.964465\n",
            "GICSIFLL: -0.670555\n",
            "epoch 24/120 d loss -0.0658, g loss -0.0444, unique % 80.0000, mean: -0.6354, std: 0.2778\n",
            "CCVYIFGF: -0.933069\n",
            "#---------------------#\n",
            "# FFCFCLCI: -1.161632 #\n",
            "#---------------------#\n",
            "CYFCVVNC: -0.623996\n",
            "epoch 25/120 d loss 0.0111, g loss -0.0173, unique % 77.9000, mean: -0.6473, std: 0.3147\n",
            "CYFCGVYC: -0.677237\n",
            "FCVYIFHV: -0.846063\n",
            "#---------------------#\n",
            "# LYICYYVY: -1.025338 #\n",
            "#---------------------#\n",
            "epoch 26/120 d loss 0.0026, g loss -0.0247, unique % 78.8000, mean: -0.6719, std: 0.3237\n",
            "VYIFRLVL: -0.999536\n",
            "FNFVCFCV: -0.782534\n",
            "LYVIIVYF: -0.962536\n",
            "GLYVFIDR: -0.477444\n",
            "FCVYICHF: -0.872228\n",
            "epoch 27/120 d loss 0.0167, g loss -0.0232, unique % 77.8000, mean: -0.6454, std: 0.3147\n",
            "#---------------------#\n",
            "# VFIFRLCI: -1.216419 #\n",
            "#---------------------#\n",
            "FYVIICHF: -0.920601\n",
            "YLSLNIYD: -0.475822\n",
            "epoch 28/120 d loss -0.0138, g loss -0.0133, unique % 77.5000, mean: -0.6435, std: 0.3137\n",
            "FCVIICHF: -0.934889\n",
            "FFFVCSCN: -0.551688\n",
            "LYIFCLVL: -0.946309\n",
            "epoch 29/120 d loss -0.0021, g loss -0.0096, unique % 73.4000, mean: -0.6221, std: 0.3008\n",
            "FCCNCNGI: -0.498498\n",
            "YLDVRGFC: -0.569691\n",
            "VYIFYYCL: -0.883655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d2d46224ee0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_peps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_peps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mw_optimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84wV6USBCU8c",
        "colab_type": "text"
      },
      "source": [
        "# GAN evalutation\n",
        "\n",
        "Code to evaluate gan performance on several heuristics to see which setup provides the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQOH3EFyniU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# GAN evaluation\n",
        "# ---------------\n",
        "\n",
        "def evaluate_gan(test_model, draw_histogram=False):\n",
        "  count = 1000\n",
        "  z = Variable(Tensor(np.random.normal(0, 1, (count, gan_opt.latent_dim))))\n",
        "\n",
        "  gen = test_model(z)\n",
        "  peptides = decode_peptide_s(gen)\n",
        "  peptides = np.unique(peptides)\n",
        "  peptides = one_hot_s(peptides)\n",
        "\n",
        "  gen_scores = tox_predictor(peptides.cuda())\n",
        "\n",
        "  if draw_histogram:\n",
        "    print(\"Unique Peptides Generated: {}\".format(peptides.size(0)))\n",
        "    print(\"Mean Score: {:.6f}\".format(gen_scores.mean()))\n",
        "    print(\"Standard Deviation: {:.6f}\".format(gen_scores.std()))\n",
        "    # generate comparison data\n",
        "    data = generate_random_peptide_s(count)\n",
        "    data_scores = tox_predictor(data.cuda())\n",
        "\n",
        "    colors = ['red', 'blue']\n",
        "    labels = ['generated peptides', 'random peptides']\n",
        "    plt.hist(\n",
        "        [gen_scores.detach().cpu().numpy(), data_scores.detach().cpu().numpy()], \n",
        "        bins=10, density=True, histtype='bar', color=colors, label=labels)\n",
        "    plt.legend(prop={'size': 10})\n",
        "    plt.title('distribution of peptide data')\n",
        "\n",
        "  uniqueness = (peptides.size(0)/count)*100\n",
        "  return uniqueness, gen_scores.mean().item(), gen_scores.std().item()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMg33Z8dtOQu",
        "colab_type": "text"
      },
      "source": [
        "# Data Genreation\n",
        "\n",
        "code that takes the original dataset and modifies it to be better/ more usable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH04hDoEhtGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c579dcd9-962c-41ba-89cb-a5992ebdaa21"
      },
      "source": [
        "all_sequences = []\n",
        "all_scores = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "      line_count += 1\n",
        "      all_sequences.append(row[0])\n",
        "      all_scores.append(row[1])\n",
        "    \n",
        "    print(f'\\nProcessed {line_count} lines.\\n')\n",
        "  \n",
        "np.save(\"allPeptideSequences\", all_sequences)\n",
        "np.save(\"allPeptideScores\", all_scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 64%|   | 67729/105416 [00:00<00:00, 677279.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 105416 lines.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi8V7YR2WGj",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Hall Of Fame\n",
        "\n",
        "All the best most toxic peptides that get generated should get stuck here along with their score so we don't loose track of them\n",
        "\n",
        "* FRGRFFFL: -1.397936\n",
        "* FYFYSFCF: -1.116865\n",
        "* FYFHVFCF: -1.346310\n",
        "* FYVLCCFY: -1.095583\n",
        "* FFLYVCFG: -1.038978\n",
        "* FRLYVVVC: -1.035730\n",
        "* FFLYVCFG: -1.038978\n",
        "* FFYICFCC: -1.063583\n",
        "* FFFICFCF: -1.361509\n",
        "* VFLFSLFI: -1.535002\n",
        "* VFLFGLFI: -1.367290\n",
        "* VFVFGLFI: -1.190928\n",
        "* VFLFGLFI: -1.367290\n",
        "* LYLVCHFL: -1.002115\n",
        "* LVYFVLCF: -1.046022\n",
        "* LVYFFLLF: -1.109444\n",
        "* CCVIRVLI: -1.294149\n",
        "* FICFFSVF: -1.055895\n",
        "* YCCLFFFF: -1.043908\n",
        "* GICVICYV: -1.096067\n",
        "* YICVFRIV: -1.064286\n",
        "* LYCCVLFF: -1.020174\n",
        "* FYLVCCIV: -1.010827\n",
        "* FCLCICIY: -1.142711\n",
        "* FCLCICIV: -1.031946\n",
        "* VFVFYGCF: -1.045995\n",
        "* FFCCICIF: -1.163224\n",
        "* FYCCICIF: -1.138242\n",
        "* IIRLCFSY: -1.070690\n",
        "* LYIICLLI: -1.017955\n",
        "* GVLFVCCF: -1.033618\n",
        "* FFLCICFY: -1.167259\n",
        "* FYLCICFY: -1.154940\n",
        "* VYLCVCFY: -1.114934\n",
        "* FCVFFFRR: -1.077060\n",
        "* CFFVYRVL: -1.119970\n",
        "* LFCCVLIV: -1.043952\n",
        "* FFRFLVVV: -1.301919\n",
        "* FFRFLYVV: -1.355798\n",
        "* YFFFFYGF: -1.236266\n",
        "* ICLSVYFF: -1.224726\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0Dq3r4U1oj",
        "colab_type": "text"
      },
      "source": [
        "# Tests\n",
        "\n",
        "Please ignore everything bellow this line. This is garbage code just used for testing random things.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iEK8ku8U-7O",
        "colab_type": "code",
        "outputId": "1a2dd1ed-4013-42c3-e20f-7fe8cad765e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = np.load(\"mostToxicNCSequences.npy\")\n",
        "scores = np.load(\"mostToxicNCScores.npy\")\n",
        "print(len(sequences))\n",
        "print(sequences[:5]) #Why are these peptides 7 aa's long? That seems wrong?\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "# Repair old data\n",
        "new_sequences = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  pbar = tqdm(total=45957)\n",
        "  for row in csv_reader:\n",
        "    line_count += 1\n",
        "    for i, sequence in enumerate(sequences):\n",
        "      if sequence in row[0] and scores[i] == row[1]:\n",
        "        pbar.update()\n",
        "        new_sequences.append(row[0])\n",
        "\n",
        "  print(f'Processed {line_count} lines.')\n",
        "\n",
        "print(len(new_sequences))\n",
        "print(new_sequences[:5])\n",
        "\n",
        "np.save(\"newMostToxicNCSequences\", new_sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/45957 [00:00<21:02, 36.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45957\n",
            "['DCHRGFV' 'YRCCIIV' 'VCVHFLC' 'CCDIYVC' 'HCFCFDI']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 45954/45957 [25:52<00:00, 28.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 105416 lines.\n",
            "45957\n",
            "['CDCHRGFV', 'IYRCCIIV', 'HVCVHFLC', 'DCCDIYVC', 'FHCFCFDI']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
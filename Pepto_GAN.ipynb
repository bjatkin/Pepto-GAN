{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pepto-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMQAU6gP2cW+Rpsx3IIscK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjatkin/Pepto-GAN/blob/master/Pepto_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t762oxqgVgiB",
        "colab_type": "text"
      },
      "source": [
        "# Imports for everything here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj0rDDkUNXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3z4ZQUyC-0",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "Genral functions that have broad use across differnt sections of the code base. Should be functions rather than classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65U29R9UyPnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(peptide):\n",
        "    encodings = []\n",
        "    for aa in peptide:\n",
        "      encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "      index = gan_opt.amino_acids.index(aa)\n",
        "      encoding[index] = 1.0\n",
        "      encodings.append(encoding)\n",
        "    return torch.stack(encodings)\n",
        "\n",
        "def score_peptide(peptide):\n",
        "  return tox_predictor(one_hot(peptide).unsqueeze(0).cuda()).item()\n",
        "\n",
        "def decode_peptide(peptide):\n",
        "  pep = \"\"\n",
        "  for p in peptide:\n",
        "    i = p.argmax()\n",
        "    pep = pep + gan_opt.amino_acids[i]\n",
        "  \n",
        "  return pep\n",
        "\n",
        "def decode_peptides(peptides):\n",
        "  peps = []\n",
        "  for peptide in peptides:\n",
        "    peps.append(decode_peptide(peptide))\n",
        "  \n",
        "  return np.asarray(peps)\n",
        "\n",
        "def save_model(model, file_name):\n",
        "  torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name):\n",
        "  model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J559pSfKXP8K",
        "colab_type": "text"
      },
      "source": [
        "# Network Opts\n",
        "\n",
        "This section should contain global options that configure each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfplpNW_Xhff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Toxic Classifier Options\n",
        "\n",
        "class tox_opt():\n",
        "  n_epochs=10\n",
        "  batch_size=64\n",
        "  lr=0.0005\n",
        "  validate_every=500\n",
        "  load_network = False\n",
        "  load_model_file=\"tox_classifier.pt\"\n",
        "  save_model_file=\"tox_classifier.pt\"\n",
        "\n",
        "\n",
        "# GAN Options\n",
        "\n",
        "class gan_opt():\n",
        "  n_epochs=100\n",
        "  batch_size=64\n",
        "  lr=0.0002\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  peptide_length=7\n",
        "  amino_acids=\"CDFGHILNRSVY\"\n",
        "  d_update_every=100\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"tox_discriminatro.pt\"\n",
        "  save_disc_file=\"tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"tox_generator.pt\"\n",
        "  save_gen_file=\"tox_generator.pt\"\n",
        "  train_gan=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbN0afPFWAUG",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Classifier\n",
        "\n",
        "the goal of this model is to predict the toxicity score of a peptide. This can then be used to access the success of the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3hy8qDIR2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------\n",
        "# Toxic Peptide classifier\n",
        "# -------------------------\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Loss function\n",
        "toxicity_loss = nn.L1Loss()\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "\n",
        "if cuda:\n",
        "  tox_predictor.cuda()\n",
        "  toxicity_loss.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tox = torch.optim.Adam(tox_predictor.parameters(), lr=tox_opt.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWkYFPZXZn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------\n",
        "# Peptides Dataloader\n",
        "# --------------------\n",
        "class ToxicPeptideDataset(Dataset):\n",
        "  def __init__(self, file_name, labels_file_name=\"\", train=True, soft=False):\n",
        "    if soft:\n",
        "      self.peptides = self.one_soft(np.load(file_name))\n",
        "    else:\n",
        "      self.peptides = self.one_hot(np.load(file_name))\n",
        "\n",
        "    self.use_labels = False\n",
        "    if labels_file_name != \"\":\n",
        "      self.use_labels = True\n",
        "      # These come in as strings and need to be floats\n",
        "      self.labels = [float(l) for l in np.load(labels_file_name)]\n",
        "\n",
        "      # Test train split\n",
        "      split = len(self.labels) // 10\n",
        "      if train:\n",
        "        self.peptides = self.peptides[split:]\n",
        "        self.labels = self.labels[split:]\n",
        "      else:\n",
        "        self.peptides = self.peptides[:split]\n",
        "        self.labels = self.labels[:split]\n",
        "\n",
        "  def one_hot(self, peptides):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1.0\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "  \n",
        "  def one_soft(self, peptides, alpha=0.5):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        cs = len(gan_opt.amino_acids)\n",
        "        encoding = torch.ones(cs) * (alpha/(cs-1))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1 - alpha\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    if self.use_labels:\n",
        "      return self.peptides[index], self.labels[index]\n",
        "    return self.peptides[index]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.peptides)\n",
        "\n",
        "tox_data_train = ToxicPeptideDataset(\"mostToxicNCSequences.npy\", \"mostToxicNCScores.npy\", train=True)\n",
        "tox_dataloader_train = torch.utils.data.DataLoader(tox_data_train, batch_size=tox_opt.batch_size, shuffle=True)\n",
        "\n",
        "tox_data_test = ToxicPeptideDataset(\"mostToxicNCSequences.npy\", \"mostToxicNCScores.npy\", train=False)\n",
        "tox_dataloader_test = torch.utils.data.DataLoader(tox_data_test, batch_size=tox_opt.batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Z_HRfNjuhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "  diff = torch.abs(y_hat-y_truth)\n",
        "  count = y_hat.size()[0]\n",
        "  return (count - torch.sum(diff))/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsvNAl4UVtt",
        "colab_type": "code",
        "outputId": "8ad2921c-9d8c-471e-83bf-0128282f3448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "losses = []\n",
        "v_losses = []\n",
        "acc = []\n",
        "def tox_train():\n",
        "  loop = tqdm(total=len(tox_dataloader_train) * tox_opt.n_epochs, position=0)\n",
        "  for epoch in range(tox_opt.n_epochs):\n",
        "\n",
        "      for i, (peps, toxs) in enumerate(tox_dataloader_train):\n",
        "          peps = Variable(peps.type(Tensor))\n",
        "          toxs = Variable(toxs.type(Tensor))\n",
        "          optimizer_tox.zero_grad()\n",
        "\n",
        "          y_hat = tox_predictor(peps)\n",
        "\n",
        "          # loss = toxicity_loss(y_hat.squeeze(), toxs)\n",
        "          # pdb.set_trace()\n",
        "          loss = torch.sum(torch.abs(y_hat.squeeze()-toxs))\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "          optimizer_tox.step()\n",
        "          last_acc = 0\n",
        "          if i % tox_opt.validate_every:\n",
        "            a = []\n",
        "            for v_peps, v_toxs in tox_dataloader_test:\n",
        "              v_peps = Variable(v_peps.type(Tensor))\n",
        "              v_toxs = Variable(v_toxs.type(Tensor))\n",
        "              v_y_hat = tox_predictor(v_peps)\n",
        "              a.append(accuracy(v_y_hat.squeeze(), v_toxs).item())\n",
        "            last_acc = np.mean(a)\n",
        "            acc.append((len(losses), last_acc))\n",
        "\n",
        "          loop.set_description(\"Epoch {}, Batch {}, Toxic_Loss {:.4f}, Accuracy {:.4f}\".format(epoch, i, loss.item(), last_acc))\n",
        "          loop.update()\n",
        "\n",
        "# No need to retrain if we can just load the network from a file\n",
        "if tox_opt.load_network:\n",
        "  load_network(tox_predictor, tox_opt.load_model_file)\n",
        "  quit()\n",
        "\n",
        "tox_train()\n",
        "save_model(tox_predictor, tox_opt.save_model_file)\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.plot(losses, label='losses')\n",
        "plt.title('Toxicicity Predictor Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "a, b = zip(*acc)\n",
        "plt.plot(a, b, label='accuracy')\n",
        "plt.title('Tocicity Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 9, Batch 646, Toxic_Loss 2.4103, Accuracy 0.8782: 100%|█████████▉| 6469/6470 [07:24<00:00, 14.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-44b73a69eb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtox_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0msave_netwrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtox_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtox_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Plot accuracy and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_netwrok' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLI4rR3MabTN",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN\n",
        "\n",
        "this section contains the code to run a simple gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the GAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "## Tweak #1\n",
        "\n",
        "**Problem:**The generator network was really struggling to learn how to produce peptides. The probelm apeard to be that the discriminator was learning too quickly and turning the learning rate for the discriminator down enough to allow the generator to catch up was preventing the discriminator from learning useful information that would help imporve the generator. I hypothesize that the reason for this is that the one hot encodings we were using were too high contrast. Before the discriminator learning for valid peptide configurations or toxicities it instead looked only at contrast. It learned this so quickly that the generator could not adjust in time and was burried by the discriminator.\n",
        "\n",
        "**Fix:**We fixed this by using label smoothing on our one hot peptide encoding.\n",
        "\n",
        "**Result:**This resulted in a marked improvement of the GAN. the discriminator learning rate was increase by a factor of 5 (every 500 steps to every 100 steps). also the number of training epochs that the GAN was able to sustain jumped by an order of magnitued (10 epochs to 100 epocsh)\n",
        "\n",
        "## Tweak #2\n",
        "**Problem:**(See tweak #1)\n",
        "\n",
        "**Fix:**Change the contrast on the latent z vector so it's very high to see if that promotes high contrast in the output.\n",
        "\n",
        "**Result:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Zs2bIlQvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------\n",
        "# Define the Generator and Discriminator\n",
        "# ---------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(gan_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Dataset\n",
        "gan_data = ToxicPeptideDataset(\"mostToxicNCSequences.npy\", soft=True)\n",
        "gan_dataloader = torch.utils.data.DataLoader(gan_data, batch_size=gan_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPkjy6hTkDZA",
        "colab_type": "code",
        "outputId": "22388d92-73d9-4742-ffe7-1d94e07a2c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# --------------\n",
        "#  GAN Training\n",
        "# --------------\n",
        "\n",
        "def gan_train():\n",
        "  for epoch in range(gan_opt.n_epochs):\n",
        "\n",
        "      for i, peps in enumerate(gan_dataloader):\n",
        "\n",
        "          # Adversarial ground truths\n",
        "          valid = Variable(Tensor(peps.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "          fake = Variable(Tensor(peps.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "          # Configure input\n",
        "          real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Sample noise as generator input\n",
        "          z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Sample high constrast noise as generator input\n",
        "          # z = Variable(Tensor(np.random.randint(2, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_peps = generator(z)\n",
        "\n",
        "          # Loss measures generator's ability to fool the discriminator\n",
        "          g_loss = adversarial_loss(discriminator(gen_peps), valid)\n",
        "\n",
        "          g_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator - every d_update_every steps\n",
        "          # ---------------------\n",
        "\n",
        "          batches_done = epoch * len(gan_dataloader) + i\n",
        "          if batches_done % gan_opt.d_update_every == 0:\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "            real_loss = adversarial_loss(discriminator(real_peps), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_peps.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "      print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}\".format(epoch+1, gan_opt.n_epochs, d_loss.item(), g_loss.item()))\n",
        "      for pep in gen_peps:\n",
        "        name = decode_peptide(pep)\n",
        "        score = score_peptide(name)\n",
        "        if score < -0.4 and score > -1:\n",
        "          print(\"{}: {:.6f}\".format(name, score))\n",
        "        if score <= -1:\n",
        "          print(\"#--------------------#\")\n",
        "          print(\"# {}: {:.6f} #\".format(name, score))\n",
        "          print(\"#--------------------#\")\n",
        "\n",
        "if gan_opt.load_discriminator:\n",
        "  load_model(discriminator, gan_opt.load_disc_file)\n",
        "\n",
        "if gan_opt.load_generator:\n",
        "  load_model(generator, gan_opt.load_gen_file)\n",
        "\n",
        "if gan_opt.train_gan:\n",
        "  gan_train()\n",
        "  save_model(discriminator, gan_opt.save_disc_file)\n",
        "  save_model(generator, gan_opt.save_gen_file)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/100 d loss 0.6959, g loss 0.6901\n",
            "IHVHIGI: -0.487520\n",
            "YCFVVGC: -0.511137\n",
            "epoch 2/100 d loss 0.6964, g loss 0.6891\n",
            "LVGYFYG: -0.478704\n",
            "LVGYFYG: -0.478704\n",
            "CVIGCCS: -0.448950\n",
            "CVIGCCS: -0.448950\n",
            "epoch 3/100 d loss 0.6993, g loss 0.6761\n",
            "YCSNYVY: -0.413975\n",
            "RFHCHHL: -0.417949\n",
            "RFHCHHL: -0.417949\n",
            "RFHCHHL: -0.417949\n",
            "epoch 4/100 d loss 0.6951, g loss 0.6873\n",
            "CGGLIGF: -0.404582\n",
            "#--------------------#\n",
            "# VYFLICR: -1.005590 #\n",
            "#--------------------#\n",
            "VYFLIGR: -0.878897\n",
            "CYLLCGR: -0.551251\n",
            "epoch 5/100 d loss 0.6987, g loss 0.6720\n",
            "epoch 6/100 d loss 0.6954, g loss 0.6885\n",
            "FCCGYFS: -0.516822\n",
            "ICYGYFS: -0.427909\n",
            "ICYGYFS: -0.427909\n",
            "FFYHYFS: -0.502018\n",
            "CCYGFHI: -0.553137\n",
            "epoch 7/100 d loss 0.6932, g loss 0.6895\n",
            "VRFYFYN: -0.633927\n",
            "VRFYFYN: -0.633927\n",
            "VVFYFLN: -0.809342\n",
            "YRFYFLN: -0.725282\n",
            "VVFYFLN: -0.809342\n",
            "epoch 8/100 d loss 0.6986, g loss 0.6728\n",
            "NIDCHGY: -0.431471\n",
            "epoch 9/100 d loss 0.6885, g loss 0.6986\n",
            "YFCCGFC: -0.683560\n",
            "CFCCGFC: -0.653205\n",
            "CFDFGFC: -0.419078\n",
            "FFCFGFC: -0.878596\n",
            "epoch 10/100 d loss 0.6847, g loss 0.6937\n",
            "YSVNFRD: -0.471595\n",
            "epoch 11/100 d loss 0.6907, g loss 0.6971\n",
            "LCYIYYF: -0.862979\n",
            "SFIRCVV: -0.456536\n",
            "LCYIYYF: -0.862979\n",
            "LCYIYYF: -0.862979\n",
            "SCYIYVF: -0.933371\n",
            "epoch 12/100 d loss 0.6963, g loss 0.6850\n",
            "CYLLIFS: -0.654600\n",
            "CYLFDIY: -0.691244\n",
            "CYLLCFS: -0.593341\n",
            "CYLCDFS: -0.552534\n",
            "CYLLIFS: -0.654600\n",
            "epoch 13/100 d loss 0.6922, g loss 0.6830\n",
            "IYCLRLG: -0.651269\n",
            "YICGYVI: -0.432963\n",
            "RVCGCCG: -0.498045\n",
            "epoch 14/100 d loss 0.6782, g loss 0.6977\n",
            "LFSDVYV: -0.438832\n",
            "DRYFVYV: -0.495216\n",
            "DRYFVYC: -0.432338\n",
            "DRYFVYC: -0.432338\n",
            "LFFSVYV: -0.927471\n",
            "epoch 15/100 d loss 0.6902, g loss 0.6920\n",
            "CGGCRFF: -0.606356\n",
            "FHGCDIR: -0.448094\n",
            "CGGCRFL: -0.568549\n",
            "FGGVRFL: -0.441862\n",
            "epoch 16/100 d loss 0.6944, g loss 0.6909\n",
            "CVCCCGY: -0.638506\n",
            "YVCCIGC: -0.596902\n",
            "CVCCCGY: -0.638506\n",
            "CVCCCGY: -0.638506\n",
            "YVCCIGY: -0.639307\n",
            "epoch 17/100 d loss 0.6804, g loss 0.7087\n",
            "CCYGCLH: -0.598171\n",
            "NDYYFLH: -0.462508\n",
            "CCYYCLH: -0.570330\n",
            "CCYGCLH: -0.598171\n",
            "LCYGFLH: -0.479181\n",
            "epoch 18/100 d loss 0.6907, g loss 0.6957\n",
            "FFVSYHC: -0.516996\n",
            "HFVSFLG: -0.404221\n",
            "epoch 19/100 d loss 0.7071, g loss 0.6825\n",
            "FYDCDSY: -0.502142\n",
            "YIIVGRV: -0.893447\n",
            "YIIVGRV: -0.893447\n",
            "YIIVGRV: -0.893447\n",
            "YCIVGRV: -0.448278\n",
            "epoch 20/100 d loss 0.6939, g loss 0.6936\n",
            "epoch 21/100 d loss 0.6850, g loss 0.6887\n",
            "ICFVYDI: -0.485398\n",
            "DSCDFYF: -0.447407\n",
            "DSCDFYF: -0.447407\n",
            "ICFVCDI: -0.665479\n",
            "ICFVCDV: -0.601440\n",
            "epoch 22/100 d loss 0.6918, g loss 0.6945\n",
            "#--------------------#\n",
            "# CFLFFFC: -1.002163 #\n",
            "#--------------------#\n",
            "CFGFFGC: -0.537666\n",
            "CILLFFC: -0.851750\n",
            "epoch 23/100 d loss 0.6978, g loss 0.6828\n",
            "YNILGCY: -0.453166\n",
            "GNICHCC: -0.433182\n",
            "GLICHCY: -0.511568\n",
            "YLILGCC: -0.757961\n",
            "YLILGCY: -0.856497\n",
            "epoch 24/100 d loss 0.6944, g loss 0.6911\n",
            "HVYNLHL: -0.418902\n",
            "FVHCIIL: -0.995306\n",
            "HYYNLVL: -0.472044\n",
            "HVYNLHL: -0.418902\n",
            "epoch 25/100 d loss 0.6800, g loss 0.7124\n",
            "CYFYCLR: -0.802204\n",
            "#--------------------#\n",
            "# VYFYFLF: -1.111768 #\n",
            "#--------------------#\n",
            "CYFYCLR: -0.802204\n",
            "CYFYCLR: -0.802204\n",
            "CYFYCLR: -0.802204\n",
            "epoch 26/100 d loss 0.6898, g loss 0.6793\n",
            "YFFVIYH: -0.762588\n",
            "epoch 27/100 d loss 0.6825, g loss 0.7167\n",
            "CFLFYSD: -0.854887\n",
            "CFLFYSD: -0.854887\n",
            "CFLFYSD: -0.854887\n",
            "epoch 28/100 d loss 0.6838, g loss 0.7056\n",
            "YVCIGVI: -0.605555\n",
            "YVCIGVI: -0.605555\n",
            "RVCCGVI: -0.495695\n",
            "YVCCGVI: -0.569254\n",
            "epoch 29/100 d loss 0.7008, g loss 0.6592\n",
            "VIYCNCN: -0.426801\n",
            "epoch 30/100 d loss 0.6869, g loss 0.7005\n",
            "FLRDCGF: -0.423465\n",
            "FLRDCGF: -0.423465\n",
            "VGRCFFF: -0.497022\n",
            "epoch 31/100 d loss 0.6723, g loss 0.6931\n",
            "VIRCFLN: -0.848144\n",
            "epoch 32/100 d loss 0.6871, g loss 0.6919\n",
            "VIVCFLN: -0.889681\n",
            "DDFFCHS: -0.402400\n",
            "epoch 33/100 d loss 0.7237, g loss 0.6365\n",
            "FRLLGCY: -0.558657\n",
            "FRLLGDY: -0.481785\n",
            "FRLLSCY: -0.466199\n",
            "FRLLGCY: -0.558657\n",
            "epoch 34/100 d loss 0.6973, g loss 0.6816\n",
            "IIVLSYV: -0.868755\n",
            "LHNYRYY: -0.509700\n",
            "epoch 35/100 d loss 0.6859, g loss 0.7030\n",
            "CSIRYVI: -0.532568\n",
            "GSIYLDI: -0.410242\n",
            "#--------------------#\n",
            "# LIVIRYV: -1.023903 #\n",
            "#--------------------#\n",
            "epoch 36/100 d loss 0.6964, g loss 0.6989\n",
            "RDFFFSH: -0.454518\n",
            "CIVYRCV: -0.508449\n",
            "RDFFFSH: -0.454518\n",
            "epoch 37/100 d loss 0.6913, g loss 0.7067\n",
            "YFSCCIC: -0.739776\n",
            "YVVHCIC: -0.486860\n",
            "YVVHCIC: -0.486860\n",
            "epoch 38/100 d loss 0.6904, g loss 0.7173\n",
            "FYYGVFY: -0.530305\n",
            "FFHGVGY: -0.410397\n",
            "epoch 39/100 d loss 0.6680, g loss 0.7355\n",
            "CCCYRFL: -0.735877\n",
            "CCCYRFL: -0.735877\n",
            "#--------------------#\n",
            "# FCYYRFL: -1.011835 #\n",
            "#--------------------#\n",
            "LCYYRFL: -0.930309\n",
            "CCCYRFL: -0.735877\n",
            "epoch 40/100 d loss 0.6923, g loss 0.7084\n",
            "CICFCVF: -0.656195\n",
            "CICFNVF: -0.668641\n",
            "CLGLCHF: -0.531067\n",
            "CLGLCHF: -0.531067\n",
            "epoch 41/100 d loss 0.6946, g loss 0.6994\n",
            "CICFNGF: -0.572230\n",
            "YFLLSYD: -0.438956\n",
            "YFLLSYD: -0.438956\n",
            "YFLLSYD: -0.438956\n",
            "YFLLHYD: -0.407422\n",
            "epoch 42/100 d loss 0.6713, g loss 0.7026\n",
            "SYCRFVC: -0.511864\n",
            "SYCRFVY: -0.528241\n",
            "SYCRFVY: -0.528241\n",
            "CYCFLIG: -0.689584\n",
            "SYCRFVY: -0.528241\n",
            "epoch 43/100 d loss 0.6717, g loss 0.7271\n",
            "VGVFYIR: -0.851950\n",
            "ICVFCDF: -0.787734\n",
            "ISVIFLF: -0.906115\n",
            "ICVFCDF: -0.787734\n",
            "ICVFCDF: -0.787734\n",
            "epoch 44/100 d loss 0.7102, g loss 0.6706\n",
            "FIIVLCI: -0.976955\n",
            "#--------------------#\n",
            "# CISFLLF: -1.183742 #\n",
            "#--------------------#\n",
            "epoch 45/100 d loss 0.6695, g loss 0.7539\n",
            "CIHFLCV: -0.993382\n",
            "CCRVIRC: -0.452978\n",
            "CIHCLCV: -0.627454\n",
            "CFHCLCF: -0.652450\n",
            "epoch 46/100 d loss 0.6636, g loss 0.7406\n",
            "RVFGFVG: -0.411673\n",
            "RVFGFVG: -0.411673\n",
            "RVFGFVG: -0.411673\n",
            "#--------------------#\n",
            "# CIHFLCY: -1.034926 #\n",
            "#--------------------#\n",
            "RVFGFVG: -0.411673\n",
            "epoch 47/100 d loss 0.6724, g loss 0.7252\n",
            "CIHCYGY: -0.684446\n",
            "epoch 48/100 d loss 0.6578, g loss 0.7584\n",
            "YIYCYFC: -0.895047\n",
            "YIYCYFC: -0.895047\n",
            "CIHCYGG: -0.429384\n",
            "YIYCYFC: -0.895047\n",
            "YIYCYFC: -0.895047\n",
            "epoch 49/100 d loss 0.7025, g loss 0.7340\n",
            "CLGVSVL: -0.420709\n",
            "CIHVNLY: -0.714024\n",
            "CLHVNVL: -0.443426\n",
            "epoch 50/100 d loss 0.6713, g loss 0.7720\n",
            "CYHVLCR: -0.496595\n",
            "FYGGNCY: -0.558504\n",
            "FLGGNLY: -0.477775\n",
            "epoch 51/100 d loss 0.6793, g loss 0.7544\n",
            "#--------------------#\n",
            "# FYCVLVF: -1.072024 #\n",
            "#--------------------#\n",
            "CYVVLLY: -0.853983\n",
            "#--------------------#\n",
            "# CFVFLLF: -1.021909 #\n",
            "#--------------------#\n",
            "IVCFCDF: -0.723569\n",
            "IVFFCDF: -0.785983\n",
            "epoch 52/100 d loss 0.7045, g loss 0.6466\n",
            "CYIVLLY: -0.935606\n",
            "epoch 53/100 d loss 0.6831, g loss 0.6915\n",
            "CLCVLLY: -0.895662\n",
            "CCGSGLY: -0.455911\n",
            "CLCVLLY: -0.895662\n",
            "CLCVLLY: -0.895662\n",
            "epoch 54/100 d loss 0.6575, g loss 0.7563\n",
            "#--------------------#\n",
            "# CFHVIVL: -1.004750 #\n",
            "#--------------------#\n",
            "#--------------------#\n",
            "# CLCVLVF: -1.128534 #\n",
            "#--------------------#\n",
            "CLCVLVL: -0.886481\n",
            "CFVVIVL: -0.921430\n",
            "CLCVLVL: -0.886481\n",
            "epoch 55/100 d loss 0.7350, g loss 0.6905\n",
            "VGSFYCI: -0.401153\n",
            "CVSFYCI: -0.882419\n",
            "YGYYYFN: -0.411547\n",
            "YGYYYFN: -0.411547\n",
            "YGYYYFN: -0.411547\n",
            "epoch 56/100 d loss 0.6753, g loss 0.7453\n",
            "FYRCDFV: -0.581090\n",
            "FYRCDFV: -0.581090\n",
            "FGFCCGV: -0.428457\n",
            "FYYCDFV: -0.520789\n",
            "epoch 57/100 d loss 0.6848, g loss 0.7655\n",
            "FCRCRGY: -0.605882\n",
            "FYYCDGV: -0.445174\n",
            "FCRCRYY: -0.706435\n",
            "epoch 58/100 d loss 0.6804, g loss 0.7080\n",
            "CFHRCHC: -0.491410\n",
            "CFHGCCC: -0.617628\n",
            "CFHRCHC: -0.491410\n",
            "CFHGCCC: -0.617628\n",
            "FYFCGGY: -0.476309\n",
            "epoch 59/100 d loss 0.7292, g loss 0.6931\n",
            "LILLHRR: -0.450471\n",
            "LIGLHRF: -0.459620\n",
            "LIGLHRF: -0.459620\n",
            "CYCCGGY: -0.641511\n",
            "LIGLHRF: -0.459620\n",
            "epoch 60/100 d loss 0.6824, g loss 0.7647\n",
            "VVFVFDF: -0.838178\n",
            "FYCCGGY: -0.504820\n",
            "CYFCGGY: -0.595560\n",
            "DRYVFDN: -0.441230\n",
            "DRYVFDN: -0.441230\n",
            "epoch 61/100 d loss 0.6506, g loss 0.7544\n",
            "FYCCGGY: -0.504820\n",
            "FYCCGGY: -0.504820\n",
            "FCFYGYC: -0.608329\n",
            "FCFYGYC: -0.608329\n",
            "FYCCGGY: -0.504820\n",
            "epoch 62/100 d loss 0.6988, g loss 0.7260\n",
            "FYYCDGY: -0.431185\n",
            "epoch 63/100 d loss 0.7195, g loss 0.7330\n",
            "CYYFDIY: -0.683884\n",
            "ISYGICL: -0.433573\n",
            "epoch 64/100 d loss 0.7051, g loss 0.6972\n",
            "CYYFDIY: -0.683884\n",
            "SFIIYHF: -0.899224\n",
            "SFIIYHF: -0.899224\n",
            "SFIIVHF: -0.844294\n",
            "SFIIVHF: -0.844294\n",
            "epoch 65/100 d loss 0.6841, g loss 0.7147\n",
            "CYCFNLV: -0.546671\n",
            "CYSFDLY: -0.450262\n",
            "CYCFNLV: -0.546671\n",
            "CYCFNLV: -0.546671\n",
            "epoch 66/100 d loss 0.7052, g loss 0.8362\n",
            "FYCFCLY: -0.793622\n",
            "FVCFCLL: -0.791564\n",
            "FVHVCRR: -0.504147\n",
            "FVHVRRR: -0.444277\n",
            "epoch 67/100 d loss 0.6297, g loss 0.8725\n",
            "FCFGCFC: -0.819207\n",
            "FVFGCFC: -0.867461\n",
            "FVFGCFC: -0.867461\n",
            "FCCVCFC: -0.600350\n",
            "FCFDCFC: -0.686346\n",
            "epoch 68/100 d loss 0.6591, g loss 0.7955\n",
            "YCGYVCF: -0.705669\n",
            "LLGYGCC: -0.436586\n",
            "YVLCGCF: -0.750267\n",
            "YVSCCCY: -0.700696\n",
            "YVLCGCD: -0.480353\n",
            "epoch 69/100 d loss 0.6704, g loss 0.8226\n",
            "VVVCFCY: -0.934603\n",
            "CVVCLVG: -0.785339\n",
            "VVVCCCY: -0.911741\n",
            "VVVCCCY: -0.911741\n",
            "VVVCCCC: -0.783040\n",
            "epoch 70/100 d loss 0.6595, g loss 0.7615\n",
            "VVVCDIY: -0.754994\n",
            "IVYLSSV: -0.632306\n",
            "IVYLSSV: -0.632306\n",
            "IVYLSSV: -0.632306\n",
            "IVYLSSV: -0.632306\n",
            "epoch 71/100 d loss 0.6518, g loss 0.7602\n",
            "CYRGHYL: -0.525244\n",
            "CYRGHYL: -0.525244\n",
            "YGSYDIY: -0.480113\n",
            "CYRGHYY: -0.594516\n",
            "CYRGHYL: -0.525244\n",
            "epoch 72/100 d loss 0.6468, g loss 0.9399\n",
            "YGSYCLY: -0.427474\n",
            "YCCHCHR: -0.448744\n",
            "YGCYCVY: -0.596084\n",
            "VGSYDLY: -0.548594\n",
            "epoch 73/100 d loss 0.7859, g loss 0.7755\n",
            "YFCFDCC: -0.666981\n",
            "VGSYRLY: -0.451316\n",
            "HFDSYCC: -0.448811\n",
            "HFDSYNC: -0.487278\n",
            "epoch 74/100 d loss 0.5831, g loss 0.9015\n",
            "YGSYRLY: -0.501922\n",
            "CGSYRLD: -0.501271\n",
            "CGFYVLD: -0.496168\n",
            "SFFSICC: -0.676576\n",
            "CGFYVLD: -0.496168\n",
            "epoch 75/100 d loss 0.6184, g loss 0.9315\n",
            "YIGFRCG: -0.415940\n",
            "CSGFFFF: -0.866471\n",
            "epoch 76/100 d loss 0.6635, g loss 0.8085\n",
            "YIVFRCG: -0.492385\n",
            "epoch 77/100 d loss 0.5991, g loss 0.7737\n",
            "YICCRGG: -0.458329\n",
            "RCCCGVY: -0.557559\n",
            "RICCNVY: -0.456064\n",
            "RCCCGVY: -0.557559\n",
            "RICCGVY: -0.489896\n",
            "epoch 78/100 d loss 0.6714, g loss 0.8163\n",
            "IHHVYFH: -0.448319\n",
            "YICCLGG: -0.454633\n",
            "epoch 79/100 d loss 0.6547, g loss 0.7920\n",
            "CNNGSIF: -0.401928\n",
            "CNNGSIF: -0.401928\n",
            "CLCCRYG: -0.634507\n",
            "CNNGSIF: -0.401928\n",
            "CFCCRGV: -0.620954\n",
            "epoch 80/100 d loss 0.6611, g loss 0.7824\n",
            "VCVCCCY: -0.700048\n",
            "VYDCCYY: -0.473455\n",
            "VCRFCCR: -0.709805\n",
            "#--------------------#\n",
            "# VLYFCCR: -1.034867 #\n",
            "#--------------------#\n",
            "FYCCLGR: -0.526731\n",
            "epoch 81/100 d loss 0.5854, g loss 0.8811\n",
            "FYCCLLG: -0.604466\n",
            "epoch 82/100 d loss 0.7115, g loss 0.6254\n",
            "FYCFLLG: -0.628999\n",
            "epoch 83/100 d loss 0.5890, g loss 0.8631\n",
            "CGLYFLF: -0.972567\n",
            "FYCFLNG: -0.528532\n",
            "HGLYFLF: -0.961942\n",
            "CFLGFLC: -0.762334\n",
            "#--------------------#\n",
            "# CFLYFLF: -1.053174 #\n",
            "#--------------------#\n",
            "epoch 84/100 d loss 0.6652, g loss 0.7392\n",
            "FCGFNCI: -0.489265\n",
            "YDGFNCY: -0.450234\n",
            "CDGFNCI: -0.438264\n",
            "FCFFLSG: -0.656585\n",
            "epoch 85/100 d loss 0.6342, g loss 1.0041\n",
            "YHCLCFI: -0.755152\n",
            "YCCLCVV: -0.695048\n",
            "YVCLCVI: -0.896590\n",
            "YVCLCVI: -0.896590\n",
            "FCCFCFG: -0.693940\n",
            "epoch 86/100 d loss 0.6439, g loss 1.0199\n",
            "DVVVIRL: -0.812209\n",
            "FVYCHRL: -0.407324\n",
            "FVCCHSG: -0.402440\n",
            "FVYCHRL: -0.407324\n",
            "epoch 87/100 d loss 0.6157, g loss 1.0233\n",
            "YLCCFFG: -0.838919\n",
            "ILSHFYF: -0.442190\n",
            "CVCCHFL: -0.672051\n",
            "epoch 88/100 d loss 0.6200, g loss 0.7870\n",
            "HCGRVDY: -0.463636\n",
            "epoch 89/100 d loss 0.6058, g loss 0.9051\n",
            "FCCCDCG: -0.489398\n",
            "epoch 90/100 d loss 0.6128, g loss 0.8779\n",
            "FYCFCSG: -0.553830\n",
            "epoch 91/100 d loss 0.6399, g loss 0.7811\n",
            "RICFYSG: -0.500445\n",
            "RRLICVL: -0.781865\n",
            "RVLICVL: -0.983957\n",
            "RVLICVL: -0.983957\n",
            "RVLICVL: -0.983957\n",
            "epoch 92/100 d loss 0.6117, g loss 0.8700\n",
            "LVYSFNG: -0.409922\n",
            "LVYFFNG: -0.729166\n",
            "LVYFFNG: -0.729166\n",
            "FVVFYFY: -0.831546\n",
            "epoch 93/100 d loss 0.5689, g loss 0.9966\n",
            "CCCVGDC: -0.428061\n",
            "CCCVGDC: -0.428061\n",
            "CCCVGFC: -0.529866\n",
            "CCCVGFC: -0.529866\n",
            "epoch 94/100 d loss 0.6235, g loss 1.1540\n",
            "CIFCSGN: -0.509241\n",
            "CIFFVGN: -0.548061\n",
            "CIFCSGN: -0.509241\n",
            "FICCYGY: -0.730841\n",
            "epoch 95/100 d loss 0.6344, g loss 0.9286\n",
            "epoch 96/100 d loss 0.5902, g loss 1.0610\n",
            "IVINCLL: -0.996009\n",
            "FRCFLNR: -0.407142\n",
            "YVCGCCG: -0.629348\n",
            "#--------------------#\n",
            "# YVHICLL: -1.019731 #\n",
            "#--------------------#\n",
            "FRCFCCG: -0.563983\n",
            "epoch 97/100 d loss 0.6380, g loss 0.9245\n",
            "FFYYFDV: -0.733216\n",
            "FCYFLCG: -0.594188\n",
            "FRCFLCG: -0.520896\n",
            "FCYFFSG: -0.624191\n",
            "FCYYFDV: -0.503795\n",
            "epoch 98/100 d loss 0.5523, g loss 0.9233\n",
            "FCCCYVG: -0.631181\n",
            "CCYCLCR: -0.630662\n",
            "CCCCNVR: -0.572151\n",
            "FCCCYVC: -0.654136\n",
            "CCFCLCR: -0.674779\n",
            "epoch 99/100 d loss 0.5196, g loss 0.8787\n",
            "CGFCCVC: -0.684230\n",
            "CGLFVCG: -0.454041\n",
            "CRYFVCG: -0.449361\n",
            "CRYFVCG: -0.449361\n",
            "CGLFVCG: -0.454041\n",
            "epoch 100/100 d loss 0.6569, g loss 1.3058\n",
            "CVCGIRG: -0.427557\n",
            "FVYFHIY: -0.750773\n",
            "CRYFLCG: -0.487361\n",
            "FGYFHIY: -0.556207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQOH3EFyniU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "c58e3bcf-9f6b-4984-8404-22570c5f5072"
      },
      "source": [
        "# ---------------\n",
        "# GAN evaluation\n",
        "# ---------------\n",
        "\n",
        "# TODO: turn this into a general function and move it to its own section\n",
        "# generate 50,000 peptides\n",
        "z = Variable(Tensor(np.random.normal(0, 1, (len(tox_data_train), gan_opt.latent_dim))))\n",
        "\n",
        "peptides = generator(z)\n",
        "\n",
        "scores = tox_predictor(peptides)\n",
        "hist, bin_edges = np.histogram(scores.detach().cpu(), bins=30)\n",
        "\n",
        "data = torch.stack(tox_data_train[:len(tox_data_train)][0])\n",
        "normal_scores = tox_predictor(data.cuda())\n",
        "hist2, bin_edges_2 = np.histogram(normal_scores.detach().cpu(), bins=30)\n",
        "\n",
        "generated_count = len(peptides)\n",
        "unique_count = len(np.unique(decode_peptides(peptides)))\n",
        "print(\"Genrated Average: {:.6f}, Dataset Average: {:.6f}, Unique Peptides: {}/{}, Unique Percent {:.6f}%\".format(\n",
        "      scores.mean(), normal_scores.mean(), unique_count, generated_count, (unique_count/generated_count)*100))\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.bar(bin_edges[1:], hist, align='center', alpha=0.5, width=0.04)\n",
        "plt.bar(bin_edges_2[1:], hist2, align='center', alpha=0.5, width=0.04)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genrated Average: -0.430687, Dataset Average: -0.545215, Unique Peptides: 31/41362, Unique Percent 0.074948%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 30 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGbCAYAAAB09LxeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMg33Z8dtOQu",
        "colab_type": "text"
      },
      "source": [
        "# Data Genreation\n",
        "\n",
        "code that takes the original dataset and modifies it to be better/ more usable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi8V7YR2WGj",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Hall Of Fame\n",
        "\n",
        "All the best most toxic peptides that get generated should get stuck here along with their score so we don't loose track of them\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0Dq3r4U1oj",
        "colab_type": "text"
      },
      "source": [
        "# Tests\n",
        "\n",
        "Please ignore everything bellow this line. This is garbage code just used for testing random things.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iEK8ku8U-7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1a2dd1ed-4013-42c3-e20f-7fe8cad765e1"
      },
      "source": [
        "sequences = np.load(\"mostToxicNCSequences.npy\")\n",
        "scores = np.load(\"mostToxicNCScores.npy\")\n",
        "print(len(sequences))\n",
        "print(sequences[:5]) #Why are these peptides 7 aa's long? That seems wrong?\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "# Repair old data\n",
        "new_sequences = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  pbar = tqdm(total=45957)\n",
        "  for row in csv_reader:\n",
        "    line_count += 1\n",
        "    for i, sequence in enumerate(sequences):\n",
        "      if sequence in row[0] and scores[i] == row[1]:\n",
        "        pbar.update()\n",
        "        new_sequences.append(row[0])\n",
        "\n",
        "  print(f'Processed {line_count} lines.')\n",
        "\n",
        "print(len(new_sequences))\n",
        "print(new_sequences[:5])\n",
        "\n",
        "np.save(\"newMostToxicNCSequences\", new_sequences)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/45957 [00:00<21:02, 36.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45957\n",
            "['DCHRGFV' 'YRCCIIV' 'VCVHFLC' 'CCDIYVC' 'HCFCFDI']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 45954/45957 [25:52<00:00, 28.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 105416 lines.\n",
            "45957\n",
            "['CDCHRGFV', 'IYRCCIIV', 'HVCVHFLC', 'DCCDIYVC', 'FHCFCFDI']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
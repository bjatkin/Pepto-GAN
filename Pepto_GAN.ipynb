{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pepto-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqoxq3Z58AuGRdlfBrx3DX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjatkin/Pepto-GAN/blob/multi-WGAN/Pepto_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t762oxqgVgiB",
        "colab_type": "text"
      },
      "source": [
        "# Imports for everything here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj0rDDkUNXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3z4ZQUyC-0",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "Genral functions that have broad use across differnt sections of the code base. Should be functions rather than classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65U29R9UyPnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(peptide):\n",
        "  encodings = []\n",
        "  for aa in peptide:\n",
        "    encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "    index = gan_opt.amino_acids.index(aa)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def one_hot_s(peptides):\n",
        "  all_encodings = []\n",
        "  for peptied in peptides:\n",
        "    all_encodings.append(one_hot(peptied))\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def score_peptide(peptide):\n",
        "  return tox_predictor(one_hot(peptide).unsqueeze(0).cuda()).item()\n",
        "\n",
        "def decode_peptide(peptide):\n",
        "  pep = \"\"\n",
        "  for p in peptide:\n",
        "    i = p.argmax()\n",
        "    pep = pep + gan_opt.amino_acids[i]\n",
        "  \n",
        "  return pep\n",
        "\n",
        "def decode_peptide_s(peptides):\n",
        "  peps = []\n",
        "  for peptide in peptides:\n",
        "    peps.append(decode_peptide(peptide))\n",
        "  \n",
        "  return np.asarray(peps)\n",
        "\n",
        "def generate_random_peptide():\n",
        "  encodings = []\n",
        "  l = len(gan_opt.amino_acids)\n",
        "  for _ in range(l):\n",
        "    encoding = torch.zeros(gan_opt.peptide_length)\n",
        "    index = random.randrange(0, gan_opt.peptide_length)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def generate_random_peptide_s(count):\n",
        "  all_encodings = []\n",
        "  for _ in range(count):\n",
        "    all_encodings.append(generate_random_peptide())\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def save_model(model, file_name):\n",
        "  torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name):\n",
        "  model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J559pSfKXP8K",
        "colab_type": "text"
      },
      "source": [
        "# Network Opts\n",
        "\n",
        "This section should contain global options that configure each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfplpNW_Xhff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Toxic Classifier Options\n",
        "\n",
        "class tox_opt():\n",
        "  n_epochs=10\n",
        "  batch_size=64\n",
        "  lr=0.0005\n",
        "  validate_every=500\n",
        "  load_network = True\n",
        "  load_model_file=\"tox_classifier.pt\"\n",
        "  save_model_file=\"tox_classifier.pt\"\n",
        "\n",
        "\n",
        "# GAN Options\n",
        "\n",
        "class gan_opt():\n",
        "  n_epochs=120\n",
        "  batch_size=64\n",
        "  lr=0.0002\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  peptide_length=8\n",
        "  amino_acids=\"CDFGHILNRSVY\"\n",
        "  d_update_every=10\n",
        "  label_smoothing=True\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"tox_discriminator.pt\"\n",
        "  save_disc_file=\"tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"tox_generator.pt\"\n",
        "  save_gen_file=\"tox_generator.pt\"\n",
        "  train_gan=True\n",
        "\n",
        "# Washerstein GAN Options\n",
        "\n",
        "class was_opt():\n",
        "  n_epochs=50\n",
        "  batch_size=64\n",
        "  lr=0.001\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  clip_value=0.01\n",
        "  n_critic=5\n",
        "  label_smoothing=False\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"was_tox_discriminator.pt\"\n",
        "  save_disc_file=\"was_tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"was_tox_generator.pt\"\n",
        "  save_gen_file=\"was_tox_generator.pt\"\n",
        "  train_gan=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wdYcsKdlBYA",
        "colab_type": "text"
      },
      "source": [
        "# Peptide Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWkYFPZXZn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------\n",
        "# Peptides Dataloader\n",
        "# --------------------\n",
        "class ToxicPeptideDataset(Dataset):\n",
        "  def __init__(self, file_name, labels_file_name=\"\", train=True, soft=False):\n",
        "    if soft:\n",
        "      self.peptides = self.one_soft(np.load(file_name))\n",
        "    else:\n",
        "      self.peptides = self.one_hot(np.load(file_name))\n",
        "\n",
        "    self.use_labels = False\n",
        "    if labels_file_name != \"\":\n",
        "      self.use_labels = True\n",
        "      # These come in as strings and need to be floats\n",
        "      self.labels = [float(l) for l in np.load(labels_file_name)]\n",
        "\n",
        "      # Test train split\n",
        "      split = len(self.labels) // 10\n",
        "      if train:\n",
        "        self.peptides = self.peptides[split:]\n",
        "        self.labels = self.labels[split:]\n",
        "      else:\n",
        "        self.peptides = self.peptides[:split]\n",
        "        self.labels = self.labels[:split]\n",
        "\n",
        "  def one_hot(self, peptides):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1.0\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "  \n",
        "  def one_soft(self, peptides, alpha=0.3):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        cs = len(gan_opt.amino_acids)\n",
        "        encoding = torch.ones(cs) * (alpha/(cs-1))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1 - alpha\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    if self.use_labels:\n",
        "      return self.peptides[index], self.labels[index]\n",
        "    return self.peptides[index]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.peptides)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbN0afPFWAUG",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Classifier\n",
        "\n",
        "the goal of this model is to predict the toxicity score of a peptide. This can then be used to access the success of the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3hy8qDIR2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------\n",
        "# Toxic Peptide classifier\n",
        "# -------------------------\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Loss function\n",
        "toxicity_loss = nn.L1Loss()\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "\n",
        "if cuda:\n",
        "  tox_predictor.cuda()\n",
        "  toxicity_loss.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tox = torch.optim.Adam(tox_predictor.parameters(), lr=tox_opt.lr)\n",
        "\n",
        "\n",
        "# Data Loaders\n",
        "if not tox_opt.load_network:\n",
        "  tox_data_train = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=True)\n",
        "  tox_dataloader_train = torch.utils.data.DataLoader(tox_data_train, batch_size=tox_opt.batch_size, shuffle=True)\n",
        "\n",
        "  tox_data_test = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=False)\n",
        "  tox_dataloader_test = torch.utils.data.DataLoader(tox_data_test, batch_size=tox_opt.batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Z_HRfNjuhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "  diff = torch.abs(y_hat-y_truth)\n",
        "  count = y_hat.size()[0]\n",
        "  return (count - torch.sum(diff))/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsvNAl4UVtt",
        "colab_type": "code",
        "outputId": "bbc0933c-d30a-4a52-de7c-bb4185972429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "losses = []\n",
        "v_losses = []\n",
        "acc = []\n",
        "def tox_train():\n",
        "  loop = tqdm(total=len(tox_dataloader_train) * tox_opt.n_epochs, position=0)\n",
        "  for epoch in range(tox_opt.n_epochs):\n",
        "\n",
        "      for i, (peps, toxs) in enumerate(tox_dataloader_train):\n",
        "          peps = Variable(peps.type(Tensor))\n",
        "          toxs = Variable(toxs.type(Tensor))\n",
        "          optimizer_tox.zero_grad()\n",
        "\n",
        "          y_hat = tox_predictor(peps)\n",
        "\n",
        "          # loss = toxicity_loss(y_hat.squeeze(), toxs)\n",
        "          # pdb.set_trace()\n",
        "          loss = torch.sum(torch.abs(y_hat.squeeze()-toxs))\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "          optimizer_tox.step()\n",
        "          last_acc = 0\n",
        "          if i % tox_opt.validate_every:\n",
        "            a = []\n",
        "            for v_peps, v_toxs in tox_dataloader_test:\n",
        "              v_peps = Variable(v_peps.type(Tensor))\n",
        "              v_toxs = Variable(v_toxs.type(Tensor))\n",
        "              v_y_hat = tox_predictor(v_peps)\n",
        "              a.append(accuracy(v_y_hat.squeeze(), v_toxs).item())\n",
        "            last_acc = np.mean(a)\n",
        "            acc.append((len(losses), last_acc))\n",
        "\n",
        "          loop.set_description(\"Epoch {}, Batch {}, Toxic_Loss {:.4f}, Accuracy {:.4f}\".format(epoch, i, loss.item(), last_acc))\n",
        "          loop.update()\n",
        "\n",
        "# No need to retrain if we can just load the network from a file\n",
        "if not tox_opt.load_network:\n",
        "  tox_train()\n",
        "  save_model(tox_predictor, tox_opt.save_model_file)\n",
        "\n",
        "  # Plot accuracy and loss\n",
        "  plt.plot(losses, label='losses')\n",
        "  plt.title('Toxicicity Predictor Losses')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  a, b = zip(*acc)\n",
        "  plt.plot(a, b, label='accuracy')\n",
        "  plt.title('Tocicity Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "if tox_opt.load_network:\n",
        "  load_model(tox_predictor, tox_opt.load_model_file)\n",
        "  print(\"Network Successfully Loaded!\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Successfully Loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84wV6USBCU8c",
        "colab_type": "text"
      },
      "source": [
        "# GAN evalutation\n",
        "\n",
        "Code to evaluate gan performance on several heuristics to see which setup provides the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQOH3EFyniU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# GAN evaluation\n",
        "# ---------------\n",
        "\n",
        "def evaluate_gan(test_model, count=1000, draw_histogram=False):\n",
        "  z = Variable(Tensor(np.random.normal(0, 1, (count, gan_opt.latent_dim))))\n",
        "\n",
        "  gen = test_model(z)\n",
        "  peptides = decode_peptide_s(gen)\n",
        "  peptides = np.unique(peptides)\n",
        "  peptides = one_hot_s(peptides)\n",
        "\n",
        "  gen_scores = tox_predictor(peptides.cuda())\n",
        "\n",
        "  if draw_histogram:\n",
        "    print(\"Unique Peptides Generated: {}\".format(peptides.size(0)))\n",
        "    print(\"Mean Score: {:.6f}\".format(gen_scores.mean()))\n",
        "    print(\"Standard Deviation: {:.6f}\".format(gen_scores.std()))\n",
        "    # generate comparison data\n",
        "    data = generate_random_peptide_s(count)\n",
        "    data_scores = tox_predictor(data.cuda())\n",
        "    print(\"Distance Between Means: {:.6f}\".format(data_scores.mean().item() - gen_scores.mean().item()))\n",
        "\n",
        "    colors = ['red', 'blue']\n",
        "    labels = ['generated peptides', 'random peptides']\n",
        "    plt.figure(figsize=(16,9))\n",
        "    plt.hist(\n",
        "        [gen_scores.detach().cpu().numpy(), data_scores.detach().cpu().numpy()], \n",
        "        bins=10, density=True, histtype='bar', color=colors, label=labels)\n",
        "    plt.legend(prop={'size': 10})\n",
        "    plt.title('distribution of peptide data')\n",
        "\n",
        "  uniqueness = (peptides.size(0)/count)*100\n",
        "  return uniqueness, gen_scores.mean().item(), gen_scores.std().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjZ38DKIB3qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Top N peptides\n",
        "# ---------------\n",
        "\n",
        "#generate and display the top N toxic peptides\n",
        "def top_n_peptides(generator, count=20):\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (count*100, gan_opt.latent_dim))))\n",
        "\n",
        "    gen = generator(z)\n",
        "    peptides = decode_peptide_s(gen)\n",
        "    peptides = np.unique(peptides)\n",
        "    peptides = one_hot_s(peptides)\n",
        "\n",
        "    gen_scores = tox_predictor(peptides.cuda())\n",
        "    score = np.array(gen_scores.squeeze().tolist())\n",
        "    peptide_data = [(pep, score[i]) for i, pep in enumerate(decode_peptide_s(peptides))]\n",
        "    peptide_data.sort(key=lambda tup: tup[1]) \n",
        "    return peptide_data[:count]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLI4rR3MabTN",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN\n",
        "\n",
        "this section contains the code to run a simple gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the GAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "## Experiment 1\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=100\n",
        "  * label_smoothing=False\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0014, g loss 6.6079, unique 1.5000%, mean: -1.1950, std: 0.3212**\n",
        "\n",
        "## Experiment 2\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=10\n",
        "  * label_smoothing=False\n",
        "\n",
        "the final epoch was not chosen as the performance fell drastically after epoch 107 and did not have time to recover. It's also interesting to note that d loss dropped to 0 after only 50 epochs but the network seemed to still be able to train.\n",
        "\n",
        "**Results: epoch 107/120 d loss 0.0000, g loss 13.2041, unique 1.7000%, mean: -1.1205, std: 0.1167**\n",
        "\n",
        "## Experiment 3\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=100\n",
        "  * label_smoothing=True\n",
        "\n",
        "the training seemed to be a little bit smoother here.\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0069, g loss 5.0971, unique: 0.9000%, mean: -1.0141, std: 0.2059**\n",
        "\n",
        "## Experiment 4\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=10\n",
        "  * label_smoothing=True\n",
        "\n",
        "epoch 114 was chosen because pefromance fell drastically after wards and didnt have time to recover. Here d loss dropped to 0 at around epoch 70. Training seemed smooth but ultimately unsuccessful.\n",
        "\n",
        "**Results: epoch 114/120 d loss 0.0000, g loss 13.2736, unique: 2.2000%, mean: -0.7249, std: 0.2899**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Zs2bIlQvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------\n",
        "# Define the Generator and Discriminator\n",
        "# ---------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(gan_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Dataset\n",
        "gan_data = ToxicPeptideDataset(\"newMostToxicNCSequences.npy\", soft=gan_opt.label_smoothing)\n",
        "gan_dataloader = torch.utils.data.DataLoader(gan_data, batch_size=gan_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPkjy6hTkDZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "#  GAN Training\n",
        "# --------------\n",
        "\n",
        "def gan_train():\n",
        "  for epoch in range(gan_opt.n_epochs):\n",
        "\n",
        "      for i, peps in enumerate(gan_dataloader):\n",
        "\n",
        "          # Adversarial ground truths\n",
        "          valid = Variable(Tensor(peps.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "          fake = Variable(Tensor(peps.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "          # Configure input\n",
        "          real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Sample noise as generator input\n",
        "          z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Sample high constrast noise as generator input (Tweak #2)\n",
        "          # z = Variable(Tensor(np.random.randint(0, 2, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_peps = generator(z)\n",
        "\n",
        "          # Loss measures generator's ability to fool the discriminator\n",
        "          g_loss = adversarial_loss(discriminator(gen_peps), valid)\n",
        "\n",
        "          g_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator - every d_update_every steps\n",
        "          # ---------------------\n",
        "\n",
        "          batches_done = epoch * len(gan_dataloader) + i\n",
        "          if batches_done % gan_opt.d_update_every == 0:\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "            real_loss = adversarial_loss(discriminator(real_peps), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_peps.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "      unique, mean, std = evaluate_gan(generator)\n",
        "      print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique: {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, gan_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "\n",
        "if gan_opt.load_discriminator:\n",
        "  load_model(discriminator, gan_opt.load_disc_file)\n",
        "\n",
        "if gan_opt.load_generator:\n",
        "  load_model(generator, gan_opt.load_gen_file)\n",
        "\n",
        "if gan_opt.train_gan:\n",
        "  gan_train()\n",
        "  save_model(discriminator, gan_opt.save_disc_file)\n",
        "  save_model(generator, gan_opt.save_gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oFZxNkWsdE_",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN - Washerstein GAN\n",
        "\n",
        "This section contains the code to run a washerstein gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output.\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the WGAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "Note: Experiments 1- 4 use the newMostToxicNCSeuences.npy dataset\n",
        "\n",
        "## Experiment 1\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=5\n",
        " * label_smoothing=False\n",
        "\n",
        "It looks like the washerstein gan fixes the modal colapse. Also it looks like the WGAN reduces its loss specifically by generating a variety of peptides rather than highly toxic peptides.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.1048, g loss 0.0275, unique 99.3000%, mean: -0.7451, std: 0.2461**\n",
        "\n",
        "## Experiment 2\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=5\n",
        " * label_smoothing=True\n",
        "\n",
        "Label Smoothing does not help the WGAN at all.\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0093, g loss 0.0040, unique 88.4000%, mean: -0.6492, std: 0.2866**\n",
        "\n",
        "## Experiment 3\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=20\n",
        " * label_smoothing=False\n",
        "\n",
        "Slowing the generator learning rate seems to help toxicity scores but makes the generator produce slightly less varried results.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0817, g loss 0.0534, unique 89.1000%, mean: -0.7930, std: 0.2221**\n",
        "\n",
        "## Experiment 4\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=100\n",
        " * label_smoothing=False\n",
        "\n",
        "This seems like this has gone too far\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0813, g loss 0.0418, unique 72.0000%, mean: -0.7263, std: 0.2341**\n",
        "\n",
        "## Experiment 5\n",
        "  * n_epochs=120\n",
        "  * batch_size=64\n",
        "  * lr=0.001\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * clip_value=0.01\n",
        "  * n_critic=5\n",
        "  * label_smoothing=False\n",
        "\n",
        "I ran this experiment by generating a top 40k, 30k, 20k, 10k, 5k, 2k, 1k toxic non-canonical peptides. I then trained the WGAN first on the 40k dataset, followed by the 30k dataset and so on until I finally trained it on the 1k dataset. There was very little different between training it on the 30 and 40 k datasets. However by the time it finished training on this dataset it's unique value was 99% or above. this uniqeness remained untill I started training it on the 2k and 1k dataset when uniqueness dropped quickly even as the mean toxicity improved. Ultimately the greatest shifts in toxicity came from training on the 2k and 1k datasets. We should probably investigate the mean and std of each of these datasets as my assumption is that the WGAN will match the statistics of the underlying datasets quite well. Whether this will pose a practical limit on Pepto GAN or not remains to be seen.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0847, g loss 0.0231, unique 49.0000%, mean: -0.9063, std: 0.1430**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhAjsusVm31m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------\n",
        "# Define the Washerstein Generator and Descriminator\n",
        "# ---------------------------------------------------\n",
        "class WGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WGenerator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(was_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "class WDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WDiscriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "w_generator = WGenerator()\n",
        "w_discriminator = WDiscriminator()\n",
        "\n",
        "if cuda:\n",
        "    w_generator.cuda()\n",
        "    w_discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "w_gan_data = ToxicPeptideDataset(\"allPeptideSequences.npy\", soft=was_opt.label_smoothing)\n",
        "w_gan_dataloader = torch.utils.data.DataLoader(w_gan_data, batch_size=was_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "w_optimizer_G = torch.optim.RMSprop(w_generator.parameters(), lr=was_opt.lr)\n",
        "w_optimizer_D = torch.optim.RMSprop(w_discriminator.parameters(), lr=was_opt.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1lLnDwssi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "ea138281-e664-46db-8cce-5286b1a2702f"
      },
      "source": [
        "# --------------------------\n",
        "#  Washerstein GAN Training\n",
        "# --------------------------\n",
        "\n",
        "def was_gan_train():\n",
        "    for epoch in range(was_opt.n_epochs):\n",
        "\n",
        "        for i, peps in enumerate(w_gan_dataloader):\n",
        "\n",
        "            # Configure input\n",
        "            real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            w_optimizer_D.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], was_opt.latent_dim))))\n",
        "\n",
        "            # Generate a batch of peptides\n",
        "            gen_peps = w_generator(z).detach()\n",
        "            # Adversarial loss\n",
        "            d_loss = -torch.mean(w_discriminator(real_peps)) + torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "            d_loss.backward()\n",
        "            w_optimizer_D.step()\n",
        "\n",
        "            # Clip weights of discriminator\n",
        "            for p in w_discriminator.parameters():\n",
        "                p.data.clamp_(-was_opt.clip_value, was_opt.clip_value)\n",
        "\n",
        "            # Train the generator every n_critic iterations\n",
        "            if i % was_opt.n_critic == 0:\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                w_optimizer_G.zero_grad()\n",
        "\n",
        "                # Generate a batch of images\n",
        "                gen_peps = w_generator(z)\n",
        "                # Adversarial loss\n",
        "                g_loss = -torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "                g_loss.backward()\n",
        "                w_optimizer_G.step()\n",
        "\n",
        "        unique, mean, std = evaluate_gan(w_generator)\n",
        "        print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, was_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "\n",
        "if was_opt.load_discriminator:\n",
        "  load_model(w_discriminator, was_opt.load_disc_file)\n",
        "\n",
        "if was_opt.load_generator:\n",
        "  load_model(w_generator, was_opt.load_gen_file)\n",
        "\n",
        "if was_opt.train_gan:\n",
        "  was_gan_train()\n",
        "  save_model(w_discriminator, was_opt.save_disc_file)\n",
        "  save_model(w_generator, was_opt.save_gen_file)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/50 d loss -0.0680, g loss 0.0510, unique 45.7000%, mean: -0.3008, std: 0.2569\n",
            "epoch 2/50 d loss -0.0714, g loss 0.0255, unique 63.1000%, mean: -0.3900, std: 0.3056\n",
            "epoch 3/50 d loss -0.0894, g loss 0.0663, unique 70.1000%, mean: -0.4493, std: 0.3381\n",
            "epoch 4/50 d loss -0.0643, g loss 0.0475, unique 77.1000%, mean: -0.3869, std: 0.3241\n",
            "epoch 5/50 d loss -0.0667, g loss -0.0028, unique 79.3000%, mean: -0.3717, std: 0.3210\n",
            "epoch 6/50 d loss -0.0556, g loss 0.0421, unique 82.9000%, mean: -0.4287, std: 0.3149\n",
            "epoch 7/50 d loss -0.0759, g loss 0.0300, unique 84.3000%, mean: -0.4189, std: 0.3112\n",
            "epoch 8/50 d loss -0.0732, g loss 0.0516, unique 82.7000%, mean: -0.4267, std: 0.3411\n",
            "epoch 9/50 d loss -0.0618, g loss 0.0249, unique 84.7000%, mean: -0.4163, std: 0.3394\n",
            "epoch 10/50 d loss -0.0745, g loss 0.0534, unique 86.1000%, mean: -0.3974, std: 0.3385\n",
            "epoch 11/50 d loss -0.0719, g loss 0.0333, unique 88.8000%, mean: -0.4254, std: 0.3303\n",
            "epoch 12/50 d loss -0.0611, g loss 0.0650, unique 91.1000%, mean: -0.4278, std: 0.3411\n",
            "epoch 13/50 d loss -0.0704, g loss 0.0588, unique 90.5000%, mean: -0.4007, std: 0.3331\n",
            "epoch 14/50 d loss -0.0816, g loss 0.0526, unique 94.6000%, mean: -0.3782, std: 0.3204\n",
            "epoch 15/50 d loss -0.0787, g loss 0.0638, unique 94.8000%, mean: -0.4362, std: 0.3158\n",
            "epoch 16/50 d loss -0.0703, g loss 0.0535, unique 94.0000%, mean: -0.4246, std: 0.3304\n",
            "epoch 17/50 d loss -0.0818, g loss 0.0075, unique 94.0000%, mean: -0.4105, std: 0.3434\n",
            "epoch 18/50 d loss -0.0665, g loss 0.0558, unique 95.4000%, mean: -0.4139, std: 0.3337\n",
            "epoch 19/50 d loss -0.0704, g loss 0.0584, unique 94.1000%, mean: -0.3952, std: 0.3368\n",
            "epoch 20/50 d loss -0.0534, g loss 0.0530, unique 94.9000%, mean: -0.3955, std: 0.3322\n",
            "epoch 21/50 d loss -0.0720, g loss 0.0607, unique 95.4000%, mean: -0.3855, std: 0.3299\n",
            "epoch 22/50 d loss -0.0770, g loss 0.0274, unique 94.3000%, mean: -0.3950, std: 0.3213\n",
            "epoch 23/50 d loss -0.0718, g loss 0.0335, unique 95.7000%, mean: -0.4147, std: 0.3262\n",
            "epoch 24/50 d loss -0.0767, g loss 0.0546, unique 96.6000%, mean: -0.3864, std: 0.3300\n",
            "epoch 25/50 d loss -0.0749, g loss 0.0263, unique 96.5000%, mean: -0.3522, std: 0.3160\n",
            "epoch 26/50 d loss -0.0534, g loss 0.0423, unique 96.3000%, mean: -0.4179, std: 0.3118\n",
            "epoch 27/50 d loss -0.0479, g loss 0.0554, unique 95.7000%, mean: -0.4008, std: 0.3139\n",
            "epoch 28/50 d loss -0.0693, g loss 0.0563, unique 96.6000%, mean: -0.3689, std: 0.3205\n",
            "epoch 29/50 d loss -0.0582, g loss 0.0525, unique 97.4000%, mean: -0.3600, std: 0.3133\n",
            "epoch 30/50 d loss -0.0912, g loss 0.0658, unique 97.4000%, mean: -0.3472, std: 0.3509\n",
            "epoch 31/50 d loss -0.0772, g loss 0.0736, unique 97.5000%, mean: -0.4113, std: 0.3286\n",
            "epoch 32/50 d loss -0.0643, g loss 0.0576, unique 97.1000%, mean: -0.3977, std: 0.3374\n",
            "epoch 33/50 d loss -0.0706, g loss 0.0384, unique 98.3000%, mean: -0.3777, std: 0.3237\n",
            "epoch 34/50 d loss -0.0851, g loss 0.0379, unique 96.8000%, mean: -0.3780, std: 0.3292\n",
            "epoch 35/50 d loss -0.0748, g loss 0.0059, unique 97.5000%, mean: -0.3986, std: 0.3432\n",
            "epoch 36/50 d loss -0.0651, g loss 0.0597, unique 98.0000%, mean: -0.3931, std: 0.3565\n",
            "epoch 37/50 d loss -0.0523, g loss 0.0535, unique 96.8000%, mean: -0.3625, std: 0.3317\n",
            "epoch 38/50 d loss -0.0765, g loss 0.0532, unique 97.9000%, mean: -0.3972, std: 0.3512\n",
            "epoch 39/50 d loss -0.0647, g loss 0.0565, unique 97.3000%, mean: -0.4183, std: 0.3229\n",
            "epoch 40/50 d loss -0.0740, g loss 0.0370, unique 97.7000%, mean: -0.3763, std: 0.3368\n",
            "epoch 41/50 d loss -0.0633, g loss 0.0516, unique 97.5000%, mean: -0.3943, std: 0.3355\n",
            "epoch 42/50 d loss -0.0645, g loss 0.0428, unique 96.8000%, mean: -0.3799, std: 0.3356\n",
            "epoch 43/50 d loss -0.0698, g loss 0.0497, unique 98.3000%, mean: -0.4024, std: 0.3328\n",
            "epoch 44/50 d loss -0.0529, g loss 0.0585, unique 97.3000%, mean: -0.3733, std: 0.3416\n",
            "epoch 45/50 d loss -0.0662, g loss 0.0577, unique 97.9000%, mean: -0.3902, std: 0.3427\n",
            "epoch 46/50 d loss -0.0594, g loss 0.0581, unique 97.4000%, mean: -0.3817, std: 0.3264\n",
            "epoch 47/50 d loss -0.0506, g loss 0.0442, unique 97.8000%, mean: -0.4090, std: 0.3488\n",
            "epoch 48/50 d loss -0.0596, g loss 0.0531, unique 98.3000%, mean: -0.4175, std: 0.3519\n",
            "epoch 49/50 d loss -0.0522, g loss 0.0406, unique 97.4000%, mean: -0.3727, std: 0.3278\n",
            "epoch 50/50 d loss -0.0486, g loss 0.0390, unique 97.2000%, mean: -0.3516, std: 0.3312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMg33Z8dtOQu",
        "colab_type": "text"
      },
      "source": [
        "# Data Genreation\n",
        "\n",
        "code that takes the original dataset and modifies it to be better/ more usable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH04hDoEhtGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c5a8cd85-ea0b-453e-a5ea-3c26b32d29b6"
      },
      "source": [
        "# -------------------\n",
        "# Generate Base Data\n",
        "# -------------------\n",
        "\n",
        "all_sequences = []\n",
        "all_scores = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "      line_count += 1\n",
        "      all_sequences.append(row[0])\n",
        "      all_scores.append(row[1])\n",
        "    \n",
        "    print(f'\\nProcessed {line_count} lines.\\n')\n",
        "  \n",
        "#This is the base dataset that everything is based on\n",
        "np.save(\"allPeptideSequences\", all_sequences)\n",
        "np.save(\"allPeptideScores\", all_scores)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 105416 lines.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZclKItdvQabI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "65b83c47-5a01-4960-eadb-46871c20f891"
      },
      "source": [
        "# -----------------------------------\n",
        "# Top-N Non Canonical Toxic Peptides\n",
        "# -----------------------------------\n",
        "import pdb\n",
        "def toxic_non_cannon(count):\n",
        "    peptides = []\n",
        "    non_cannon_peps = []\n",
        "\n",
        "    # Read in all the peptides and scores\n",
        "    with open('all_data_filtered.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        for row in csv_reader:\n",
        "            peptides.append((row[0], row[1]))\n",
        "\n",
        "    for p in peptides:\n",
        "        sequence = p[0]\n",
        "\n",
        "        hydrophobicity = 0\n",
        "        cysteines = 0\n",
        "        for aa in sequence:\n",
        "            if aa in [\"F\",\"I\",\"V\",\"L\",\"W\"]:\n",
        "                hydrophobicity = hydrophobicity + 1\n",
        "            if aa == \"C\":\n",
        "                cysteines = cysteines + 1\n",
        "\n",
        "        if hydrophobicity < 4 and cysteines < 3:\n",
        "            non_cannon_peps.append((sequence, float(p[1])))\n",
        "\n",
        "    non_cannon_peps.sort(key=lambda tup: tup[1]) \n",
        "    ret = np.array(non_cannon_peps[:count])\n",
        "    return ret[:,0], ret[:,1]\n",
        "    \n",
        "dataset_size = [40, 30, 20, 10, 5, 2, 1] #Size is in thousands\n",
        "for s in dataset_size:\n",
        "  sk = s * 1000\n",
        "  print(\"Generating the top {}K toxic NC peptide...\".format(s))\n",
        "  peptides, scores = toxic_non_cannon(sk)\n",
        "  print(\"Saving Data...\".format(s))\n",
        "  np.save(\"{}K_NC_Toxic_Sequences\".format(s), peptides)\n",
        "  np.save(\"{}K_NC_Toxic_Scores\".format(s), scores)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating the top 40K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 30K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 20K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 10K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 5K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 2K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 1K toxic NC peptide...\n",
            "Saving Data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi8V7YR2WGj",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Hall Of Fame\n",
        "\n",
        "All the best most toxic peptides that get generated should get stuck here along with their score so we don't loose track of them\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NejtSWCbE8FI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51aa665f-2c94-4e39-fac0-115ff9acf761"
      },
      "source": [
        "best_toxic_non_canonical_peptides = [\n",
        "  ('YYYLRIYL', -1.3079304695129395),\n",
        "  ('IYYLYIRY', -1.2074322700500488),\n",
        "  ('LYYYYIYL', -1.206214427947998),\n",
        "  ('IYYLYIRL', -1.201981544494629),\n",
        "  ('FYLYYYLY', -1.1993763446807861),\n",
        "  ('LYIYIYRY', -1.1973598003387451),\n",
        "  ('YLYYYIRY', -1.1969962120056152),\n",
        "  ('YYIIFYRY', -1.1729531288146973),\n",
        "  ('FYLYLYLL', -1.1697323322296143),\n",
        "  ('IYYYYIRY', -1.1666827201843262),\n",
        "  ('RYYLVIYL', -1.1643898487091064),\n",
        "  ('FYLYYIRY', -1.1631367206573486),\n",
        "  ('YYYLRYYL', -1.1611418724060059),\n",
        "  ('LHIIFYYY', -1.1596765518188477),\n",
        "  ('YYYIYYRL', -1.1584758758544922),\n",
        "  ('LYILYYRY', -1.1553714275360107),\n",
        "  ('YYYLYYRL', -1.1538212299346924),\n",
        "  ('IYIYYIRY', -1.1495006084442139),\n",
        "  ('YYYYYYLL', -1.1484143733978271),\n",
        "  ('FYLYLILL', -1.1483674049377441),\n",
        "  ('FRGRFFFL', -1.397936),\n",
        "  ('FYFYSFCF', -1.116865),\n",
        "  ('FYFHVFCF', -1.346310),\n",
        "  ('FYVLCCFY', -1.095583),\n",
        "  ('FFLYVCFG', -1.038978),\n",
        "  ('FRLYVVVC', -1.035730),\n",
        "  ('FFLYVCFG', -1.038978),\n",
        "  ('FFYICFCC', -1.063583),\n",
        "  ('FFFICFCF', -1.361509),\n",
        "  ('VFLFSLFI', -1.535002),\n",
        "  ('VFLFGLFI', -1.367290),\n",
        "  ('VFVFGLFI', -1.190928),\n",
        "  ('VFLFGLFI', -1.367290),\n",
        "  ('LYLVCHFL', -1.002115),\n",
        "  ('LVYFVLCF', -1.046022),\n",
        "  ('LVYFFLLF', -1.109444),\n",
        "  ('CCVIRVLI', -1.294149),\n",
        "  ('FICFFSVF', -1.055895),\n",
        "  ('YCCLFFFF', -1.043908),\n",
        "  ('GICVICYV', -1.096067),\n",
        "  ('YICVFRIV', -1.064286),\n",
        "  ('LYCCVLFF', -1.020174),\n",
        "  ('FYLVCCIV', -1.010827),\n",
        "  ('FCLCICIY', -1.142711),\n",
        "  ('FCLCICIV', -1.031946),\n",
        "  ('VFVFYGCF', -1.045995),\n",
        "  ('FFCCICIF', -1.163224),\n",
        "  ('FYCCICIF', -1.138242),\n",
        "  ('IIRLCFSY', -1.070690),\n",
        "  ('LYIICLLI', -1.017955),\n",
        "  ('GVLFVCCF', -1.033618),\n",
        "  ('FFLCICFY', -1.167259),\n",
        "  ('FYLCICFY', -1.154940),\n",
        "  ('VYLCVCFY', -1.114934),\n",
        "  ('FCVFFFRR', -1.077060),\n",
        "  ('CFFVYRVL', -1.119970),\n",
        "  ('LFCCVLIV', -1.043952),\n",
        "  ('FFRFLVVV', -1.301919),\n",
        "  ('FFRFLYVV', -1.355798),\n",
        "  ('YFFFFYGF', -1.236266),\n",
        "  ('ICLSVYFF', -1.224726 ),\n",
        "  ('(N)FYFRFFRF', -2.279006242752075),\n",
        "  ('(N)FFLRFLFF', -2.2751502990722656),\n",
        "  ('(N)FRFRFFFF', -2.2271339893341064),\n",
        "  ('(N)FRFLFFRF', -2.194873809814453),\n",
        "  ('(N)FFLRFLLF', -2.105029582977295),\n",
        "  ('(N)FLFFFRRC', -2.081968069076538),\n",
        "  ('(N)FFLRCLFF', -2.0716805458068848),\n",
        "  ('(N)FFLLRLFF', -2.0159332752227783),\n",
        "  ('(N)FFYRFLRF', -2.0036509037017822),\n",
        "  ('(N)FYFFFRRC', -1.9803197383880615),\n",
        "  ('(N)FFLRRLFF', -1.9393908977508545),\n",
        "  ('(N)FLFFLFRF', -1.915808916091919),\n",
        "  ('(N)FFYRFLFF', -1.8953557014465332),\n",
        "  ('(N)FLFLFIRF', -1.8581163883209229),\n",
        "  ('(N)IRFLRLFF', -1.8479580879211426),\n",
        "  ('(N)FFYRFLLF', -1.8454816341400146),\n",
        "  ('(N)FFLRCLLF', -1.8096561431884766),\n",
        "  ('(N)FLFFLRLC', -1.7975740432739258),\n",
        "  ('(N)FLFFLRRC', -1.7772648334503174),\n",
        "  ('(N)FLFCLRFC', -1.771681308746338),\n",
        "  ]\n",
        "\n",
        "# Remove any acidental duplicates\n",
        "# best_toxic_non_canonical_peptides = np.unique(best_toxic_non_canonical_peptides)\n",
        "# Sort by most toxic\n",
        "best_toxic_non_canonical_peptides.sort(key=lambda tup: tup[1])\n",
        "# Display the results!\n",
        "for (pep,score) in best_toxic_non_canonical_peptides:\n",
        "  print(\"{}: {:.6f}\".format(pep, score)) "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(N)FYFRFFRF: -2.279006\n",
            "(N)FFLRFLFF: -2.275150\n",
            "(N)FRFRFFFF: -2.227134\n",
            "(N)FRFLFFRF: -2.194874\n",
            "(N)FFLRFLLF: -2.105030\n",
            "(N)FLFFFRRC: -2.081968\n",
            "(N)FFLRCLFF: -2.071681\n",
            "(N)FFLLRLFF: -2.015933\n",
            "(N)FFYRFLRF: -2.003651\n",
            "(N)FYFFFRRC: -1.980320\n",
            "(N)FFLRRLFF: -1.939391\n",
            "(N)FLFFLFRF: -1.915809\n",
            "(N)FFYRFLFF: -1.895356\n",
            "(N)FLFLFIRF: -1.858116\n",
            "(N)IRFLRLFF: -1.847958\n",
            "(N)FFYRFLLF: -1.845482\n",
            "(N)FFLRCLLF: -1.809656\n",
            "(N)FLFFLRLC: -1.797574\n",
            "(N)FLFFLRRC: -1.777265\n",
            "(N)FLFCLRFC: -1.771681\n",
            "VFLFSLFI: -1.535002\n",
            "FRGRFFFL: -1.397936\n",
            "VFLFGLFI: -1.367290\n",
            "VFLFGLFI: -1.367290\n",
            "FFFICFCF: -1.361509\n",
            "FFRFLYVV: -1.355798\n",
            "FYFHVFCF: -1.346310\n",
            "YYYLRIYL: -1.307930\n",
            "FFRFLVVV: -1.301919\n",
            "CCVIRVLI: -1.294149\n",
            "YFFFFYGF: -1.236266\n",
            "ICLSVYFF: -1.224726\n",
            "IYYLYIRY: -1.207432\n",
            "LYYYYIYL: -1.206214\n",
            "IYYLYIRL: -1.201982\n",
            "FYLYYYLY: -1.199376\n",
            "LYIYIYRY: -1.197360\n",
            "YLYYYIRY: -1.196996\n",
            "VFVFGLFI: -1.190928\n",
            "YYIIFYRY: -1.172953\n",
            "FYLYLYLL: -1.169732\n",
            "FFLCICFY: -1.167259\n",
            "IYYYYIRY: -1.166683\n",
            "RYYLVIYL: -1.164390\n",
            "FFCCICIF: -1.163224\n",
            "FYLYYIRY: -1.163137\n",
            "YYYLRYYL: -1.161142\n",
            "LHIIFYYY: -1.159677\n",
            "YYYIYYRL: -1.158476\n",
            "LYILYYRY: -1.155371\n",
            "FYLCICFY: -1.154940\n",
            "YYYLYYRL: -1.153821\n",
            "IYIYYIRY: -1.149501\n",
            "YYYYYYLL: -1.148414\n",
            "FYLYLILL: -1.148367\n",
            "FCLCICIY: -1.142711\n",
            "FYCCICIF: -1.138242\n",
            "CFFVYRVL: -1.119970\n",
            "FYFYSFCF: -1.116865\n",
            "VYLCVCFY: -1.114934\n",
            "LVYFFLLF: -1.109444\n",
            "GICVICYV: -1.096067\n",
            "FYVLCCFY: -1.095583\n",
            "FCVFFFRR: -1.077060\n",
            "IIRLCFSY: -1.070690\n",
            "YICVFRIV: -1.064286\n",
            "FFYICFCC: -1.063583\n",
            "FICFFSVF: -1.055895\n",
            "LVYFVLCF: -1.046022\n",
            "VFVFYGCF: -1.045995\n",
            "LFCCVLIV: -1.043952\n",
            "YCCLFFFF: -1.043908\n",
            "FFLYVCFG: -1.038978\n",
            "FFLYVCFG: -1.038978\n",
            "FRLYVVVC: -1.035730\n",
            "GVLFVCCF: -1.033618\n",
            "FCLCICIV: -1.031946\n",
            "LYCCVLFF: -1.020174\n",
            "LYIICLLI: -1.017955\n",
            "FYLVCCIV: -1.010827\n",
            "LYLVCHFL: -1.002115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0Dq3r4U1oj",
        "colab_type": "text"
      },
      "source": [
        "# Tests\n",
        "\n",
        "Please ignore everything bellow this line. This is garbage code just used for testing random things.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iEK8ku8U-7O",
        "colab_type": "code",
        "outputId": "1a2dd1ed-4013-42c3-e20f-7fe8cad765e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = np.load(\"mostToxicNCSequences.npy\")\n",
        "scores = np.load(\"mostToxicNCScores.npy\")\n",
        "print(len(sequences))\n",
        "print(sequences[:5]) #Why are these peptides 7 aa's long? That seems wrong?\n",
        "\n",
        "import csv\n",
        "\n",
        "# Repair old data\n",
        "new_sequences = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  pbar = tqdm(total=45957)\n",
        "  for row in csv_reader:\n",
        "    line_count += 1\n",
        "    for i, sequence in enumerate(sequences):\n",
        "      if sequence in row[0] and scores[i] == row[1]:\n",
        "        pbar.update()\n",
        "        new_sequences.append(row[0])\n",
        "\n",
        "  print(f'Processed {line_count} lines.')\n",
        "\n",
        "print(len(new_sequences))\n",
        "print(new_sequences[:5])\n",
        "\n",
        "np.save(\"newMostToxicNCSequences\", new_sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/45957 [00:00<21:02, 36.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45957\n",
            "['DCHRGFV' 'YRCCIIV' 'VCVHFLC' 'CCDIYVC' 'HCFCFDI']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 45954/45957 [25:52<00:00, 28.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 105416 lines.\n",
            "45957\n",
            "['CDCHRGFV', 'IYRCCIIV', 'HVCVHFLC', 'DCCDIYVC', 'FHCFCFDI']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaIkVwFeDC7U",
        "colab_type": "text"
      },
      "source": [
        "Process:\n",
        "  I took the fully trained washerstein gan using the experiment 1 setup (WGAN) and the allPepetideSequences dataset and trained it for 120 epochs. I then ran the bellow code training a toxicity classifier as the discriminator and using the mean score of the generated peptides as the loss for the generator\n",
        "\n",
        "note: i pulled epoch 1 because Imediatly after that uniqueness tanked and mean didnt improve that much.\n",
        "\n",
        "**Results: epoch 1/120 d loss 0.1334, g loss -0.6715, unique 59.9000%, mean: -1.0249, std: 0.1067**\n",
        "\n",
        "Process(2):\n",
        "  Same as above but the WGAN only ran for 50 epochs and I used the minimum of all the scored peptides as the loss for the generator. This seems to have encouraged maintaining diversity longer and kept the model from colapsing like the previous test. It will be important to compare the 2 models with evaluate gan.\n",
        "\n",
        "**Results: epoch 56/50 d loss 0.0411, g loss -1.5041, unique 23.5000%, mean: -0.8047, std: 0.5633**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKT7TyLyvxuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43fa8fae-9f83-47cf-e655-776d12216380"
      },
      "source": [
        "# --------------\n",
        "# Toxifier Test\n",
        "# --------------\n",
        "def was_gan_train():\n",
        "    epochs=120\n",
        "    batch_size=64\n",
        "    lr=0.0001\n",
        "    clip_value=0.01\n",
        "    n_critic = 5\n",
        "\n",
        "    data = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=True, soft=False)\n",
        "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    discriminator = ToxicityPredictor()\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "    discriminator_loss = nn.L1Loss()\n",
        "\n",
        "    discriminator.cuda()\n",
        "    discriminator_loss.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (peps, scores) in enumerate(dataloader):\n",
        "            # Configure input\n",
        "            peps = Variable(peps.type(Tensor))\n",
        "            scores = Variable(scores.type(Tensor))\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            y_hat = discriminator(peps)\n",
        "\n",
        "            # Traditional Loss\n",
        "            d_loss = discriminator_loss(y_hat.squeeze(), scores)\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Clip weights of discriminator\n",
        "            for p in w_discriminator.parameters():\n",
        "                p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "            # Train the generator every n_critic iterations\n",
        "            if i % n_critic == 0:\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                w_optimizer_G.zero_grad()\n",
        "\n",
        "                # Sample noise as generator input\n",
        "                z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], was_opt.latent_dim))))\n",
        "\n",
        "                # Generate a batch of images\n",
        "                gen_peps = w_generator(z)\n",
        "                # Adversarial loss\n",
        "                g_loss = torch.min(discriminator(gen_peps))\n",
        "\n",
        "                g_loss.backward()\n",
        "                w_optimizer_G.step()\n",
        "\n",
        "        unique, mean, std = evaluate_gan(w_generator)\n",
        "        print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, was_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "\n",
        "was_gan_train()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/50 d loss 0.1521, g loss -0.8606, unique 82.9000%, mean: -0.4608, std: 0.4656\n",
            "epoch 2/50 d loss 0.1156, g loss -0.9120, unique 70.5000%, mean: -0.5308, std: 0.4371\n",
            "epoch 3/50 d loss 0.1095, g loss -0.9880, unique 66.0000%, mean: -0.5372, std: 0.4154\n",
            "epoch 4/50 d loss 0.0913, g loss -1.0759, unique 61.5000%, mean: -0.5214, std: 0.4659\n",
            "epoch 5/50 d loss 0.1446, g loss -1.1397, unique 53.1000%, mean: -0.5090, std: 0.4611\n",
            "epoch 6/50 d loss 0.0894, g loss -1.1291, unique 48.9000%, mean: -0.5522, std: 0.4544\n",
            "epoch 7/50 d loss 0.0915, g loss -1.1234, unique 45.4000%, mean: -0.5493, std: 0.4569\n",
            "epoch 8/50 d loss 0.0752, g loss -1.1640, unique 41.3000%, mean: -0.5263, std: 0.4747\n",
            "epoch 9/50 d loss 0.1360, g loss -1.2267, unique 38.1000%, mean: -0.5458, std: 0.4627\n",
            "epoch 10/50 d loss 0.0665, g loss -1.2668, unique 33.5000%, mean: -0.5866, std: 0.4853\n",
            "epoch 11/50 d loss 0.1233, g loss -1.2621, unique 36.4000%, mean: -0.5963, std: 0.5267\n",
            "epoch 12/50 d loss 0.0705, g loss -1.2510, unique 33.6000%, mean: -0.6072, std: 0.4989\n",
            "epoch 13/50 d loss 0.0803, g loss -1.3320, unique 33.7000%, mean: -0.5962, std: 0.5466\n",
            "epoch 14/50 d loss 0.0846, g loss -1.3611, unique 32.7000%, mean: -0.6467, std: 0.5713\n",
            "epoch 15/50 d loss 0.0989, g loss -1.3549, unique 31.6000%, mean: -0.6299, std: 0.5872\n",
            "epoch 16/50 d loss 0.0941, g loss -1.3896, unique 30.2000%, mean: -0.5839, std: 0.5853\n",
            "epoch 17/50 d loss 0.0621, g loss -1.3475, unique 25.8000%, mean: -0.6035, std: 0.5600\n",
            "epoch 18/50 d loss 0.1110, g loss -1.3357, unique 25.8000%, mean: -0.7550, std: 0.5134\n",
            "epoch 19/50 d loss 0.1219, g loss -1.4333, unique 24.0000%, mean: -0.6887, std: 0.5846\n",
            "epoch 20/50 d loss 0.0508, g loss -1.3949, unique 27.3000%, mean: -0.7605, std: 0.5589\n",
            "epoch 21/50 d loss 0.0533, g loss -1.4617, unique 27.0000%, mean: -0.6548, std: 0.5572\n",
            "epoch 22/50 d loss 0.0646, g loss -1.3780, unique 27.6000%, mean: -0.6625, std: 0.5762\n",
            "epoch 23/50 d loss 0.0889, g loss -1.3745, unique 27.9000%, mean: -0.6640, std: 0.5516\n",
            "epoch 24/50 d loss 0.0414, g loss -1.4482, unique 30.7000%, mean: -0.6499, std: 0.5329\n",
            "epoch 25/50 d loss 0.0994, g loss -1.2870, unique 28.4000%, mean: -0.6320, std: 0.5463\n",
            "epoch 26/50 d loss 0.1168, g loss -1.3784, unique 27.6000%, mean: -0.6908, std: 0.5583\n",
            "epoch 27/50 d loss 0.0572, g loss -1.3996, unique 30.2000%, mean: -0.6364, std: 0.5779\n",
            "epoch 28/50 d loss 0.0482, g loss -1.3857, unique 26.5000%, mean: -0.7029, std: 0.5724\n",
            "epoch 29/50 d loss 0.0602, g loss -1.4083, unique 23.1000%, mean: -0.7644, std: 0.5444\n",
            "epoch 30/50 d loss 0.0784, g loss -1.4281, unique 25.7000%, mean: -0.7329, std: 0.5522\n",
            "epoch 31/50 d loss 0.0783, g loss -1.4691, unique 21.3000%, mean: -0.6840, std: 0.5704\n",
            "epoch 32/50 d loss 0.0429, g loss -1.4375, unique 21.8000%, mean: -0.7653, std: 0.5979\n",
            "epoch 33/50 d loss 0.0487, g loss -1.4588, unique 23.9000%, mean: -0.7366, std: 0.5989\n",
            "epoch 34/50 d loss 0.0787, g loss -1.4399, unique 21.0000%, mean: -0.8236, std: 0.6120\n",
            "epoch 35/50 d loss 0.0645, g loss -1.4197, unique 21.3000%, mean: -0.7289, std: 0.5963\n",
            "epoch 36/50 d loss 0.0435, g loss -1.4915, unique 25.0000%, mean: -0.7849, std: 0.6088\n",
            "epoch 37/50 d loss 0.0380, g loss -1.4355, unique 24.5000%, mean: -0.8117, std: 0.5862\n",
            "epoch 38/50 d loss 0.1177, g loss -1.5147, unique 24.2000%, mean: -0.7946, std: 0.5901\n",
            "epoch 39/50 d loss 0.0495, g loss -1.4465, unique 23.4000%, mean: -0.7356, std: 0.6124\n",
            "epoch 40/50 d loss 0.0414, g loss -1.5240, unique 20.7000%, mean: -0.7636, std: 0.6110\n",
            "epoch 41/50 d loss 0.0317, g loss -1.4823, unique 21.5000%, mean: -0.7486, std: 0.6209\n",
            "epoch 42/50 d loss 0.0539, g loss -1.4702, unique 21.1000%, mean: -0.7017, std: 0.6007\n",
            "epoch 43/50 d loss 0.0802, g loss -1.4770, unique 23.0000%, mean: -0.8031, std: 0.6128\n",
            "epoch 44/50 d loss 0.0533, g loss -1.4922, unique 17.8000%, mean: -0.7895, std: 0.6058\n",
            "epoch 45/50 d loss 0.0771, g loss -1.4929, unique 18.6000%, mean: -0.8539, std: 0.6038\n",
            "epoch 46/50 d loss 0.0880, g loss -1.4913, unique 20.3000%, mean: -0.8279, std: 0.6248\n",
            "epoch 47/50 d loss 0.0589, g loss -1.4848, unique 21.8000%, mean: -0.8419, std: 0.6131\n",
            "epoch 48/50 d loss 0.0269, g loss -1.4783, unique 20.6000%, mean: -0.8519, std: 0.5884\n",
            "epoch 49/50 d loss 0.0335, g loss -1.4796, unique 20.6000%, mean: -0.8256, std: 0.5704\n",
            "epoch 50/50 d loss 0.0286, g loss -1.5370, unique 21.7000%, mean: -0.7976, std: 0.5725\n",
            "epoch 51/50 d loss 0.0361, g loss -1.5160, unique 22.3000%, mean: -0.7693, std: 0.5752\n",
            "epoch 52/50 d loss 0.0342, g loss -1.4910, unique 23.1000%, mean: -0.8223, std: 0.5566\n",
            "epoch 53/50 d loss 0.0821, g loss -1.4762, unique 21.9000%, mean: -0.7979, std: 0.5473\n",
            "epoch 54/50 d loss 0.0645, g loss -1.5085, unique 22.2000%, mean: -0.8101, std: 0.5844\n",
            "epoch 55/50 d loss 0.0683, g loss -1.5096, unique 21.4000%, mean: -0.8039, std: 0.5711\n",
            "epoch 56/50 d loss 0.0411, g loss -1.5041, unique 23.5000%, mean: -0.8047, std: 0.5633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-257fc33d95be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique {:.4f}%, mean: {:.4f}, std: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwas_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mwas_gan_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-257fc33d95be>\u001b[0m in \u001b[0;36mwas_gan_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Clip weights of discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWQlhoqF-eW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "ea402a7a-fa45-4707-8cd7-687dee3cdba5"
      },
      "source": [
        "evaluate_gan(w_generator, count=30000, draw_histogram=True)\n",
        "top_n_peptides(w_generator)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Peptides Generated: 821\n",
            "Mean Score: -0.865027\n",
            "Standard Deviation: 0.554366\n",
            "Distance Between Means: 0.672137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('FYFRFFRF', -2.279006242752075),\n",
              " ('FFLRFLFF', -2.2751502990722656),\n",
              " ('FRFRFFFF', -2.2271339893341064),\n",
              " ('FRFLFFRF', -2.194873809814453),\n",
              " ('FFLRFLLF', -2.105029582977295),\n",
              " ('FLFFFRRC', -2.081968069076538),\n",
              " ('FFLRCLFF', -2.0716805458068848),\n",
              " ('FFLLRLFF', -2.0159332752227783),\n",
              " ('FFYRFLRF', -2.0036509037017822),\n",
              " ('FYFFFRRC', -1.9803197383880615),\n",
              " ('FFLRRLFF', -1.9393908977508545),\n",
              " ('FLFFLFRF', -1.915808916091919),\n",
              " ('FFYRFLFF', -1.8953557014465332),\n",
              " ('FLFLFIRF', -1.8581163883209229),\n",
              " ('IRFLRLFF', -1.8479580879211426),\n",
              " ('FFYRFLLF', -1.8454816341400146),\n",
              " ('FFLRCLLF', -1.8096561431884766),\n",
              " ('FLFFLRLC', -1.7975740432739258),\n",
              " ('FLFFLRRC', -1.7772648334503174),\n",
              " ('FLFCLRFC', -1.771681308746338)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAIYCAYAAAB33lEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pepto-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+nS3lXI1MV9aTnc+XvAK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjatkin/Pepto-GAN/blob/master/Pepto_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t762oxqgVgiB",
        "colab_type": "text"
      },
      "source": [
        "# Imports for everything here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj0rDDkUNXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3z4ZQUyC-0",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "Genral functions that have broad use across differnt sections of the code base. Should be functions rather than classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65U29R9UyPnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(peptide):\n",
        "  encodings = []\n",
        "  for aa in peptide:\n",
        "    encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "    index = gan_opt.amino_acids.index(aa)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def one_hot_s(peptides):\n",
        "  all_encodings = []\n",
        "  for peptied in peptides:\n",
        "    all_encodings.append(one_hot(peptied))\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def score_peptide(peptide):\n",
        "  return tox_predictor(one_hot(peptide).unsqueeze(0).cuda()).item()\n",
        "\n",
        "def decode_peptide(peptide):\n",
        "  pep = \"\"\n",
        "  for p in peptide:\n",
        "    i = p.argmax()\n",
        "    pep = pep + gan_opt.amino_acids[i]\n",
        "  \n",
        "  return pep\n",
        "\n",
        "def decode_peptide_s(peptides):\n",
        "  peps = []\n",
        "  for peptide in peptides:\n",
        "    peps.append(decode_peptide(peptide))\n",
        "  \n",
        "  return np.asarray(peps)\n",
        "\n",
        "def generate_random_peptide():\n",
        "  encodings = []\n",
        "  l = len(gan_opt.amino_acids)\n",
        "  for _ in range(l):\n",
        "    encoding = torch.zeros(gan_opt.peptide_length)\n",
        "    index = random.randrange(0, gan_opt.peptide_length)\n",
        "    encoding[index] = 1.0\n",
        "    encodings.append(encoding)\n",
        "  return torch.stack(encodings)\n",
        "\n",
        "def generate_random_peptide_s(count):\n",
        "  all_encodings = []\n",
        "  for _ in range(count):\n",
        "    all_encodings.append(generate_random_peptide())\n",
        "  return torch.stack(all_encodings)\n",
        "\n",
        "def save_model(model, file_name):\n",
        "  torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name):\n",
        "  model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J559pSfKXP8K",
        "colab_type": "text"
      },
      "source": [
        "# Network Opts\n",
        "\n",
        "This section should contain global options that configure each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfplpNW_Xhff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Toxic Classifier Options\n",
        "\n",
        "class tox_opt():\n",
        "  n_epochs=10\n",
        "  batch_size=64\n",
        "  lr=0.0005\n",
        "  validate_every=500\n",
        "  load_network = True\n",
        "  load_model_file=\"tox_classifier.pt\"\n",
        "  save_model_file=\"tox_classifier.pt\"\n",
        "\n",
        "\n",
        "# GAN Options\n",
        "\n",
        "class gan_opt():\n",
        "  n_epochs=120\n",
        "  batch_size=64\n",
        "  lr=0.0002\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  peptide_length=8\n",
        "  amino_acids=\"CDFGHILNRSVY\"\n",
        "  d_update_every=10\n",
        "  label_smoothing=True\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"tox_discriminator.pt\"\n",
        "  save_disc_file=\"tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"tox_generator.pt\"\n",
        "  save_gen_file=\"tox_generator.pt\"\n",
        "  train_gan=True\n",
        "\n",
        "# Washerstein GAN Options\n",
        "\n",
        "class was_opt():\n",
        "  n_epochs=120\n",
        "  batch_size=64\n",
        "  lr=0.001\n",
        "  b1=0.5\n",
        "  b2=0.999\n",
        "  latent_dim=100\n",
        "  clip_value=0.01\n",
        "  n_critic=5\n",
        "  label_smoothing=False\n",
        "  load_discriminator=False\n",
        "  load_disc_file=\"was_tox_discriminator.pt\"\n",
        "  save_disc_file=\"was_tox_discriminator.pt\"\n",
        "  load_generator=False\n",
        "  load_gen_file=\"was_tox_generator.pt\"\n",
        "  save_gen_file=\"was_tox_generator.pt\"\n",
        "  train_gan=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wdYcsKdlBYA",
        "colab_type": "text"
      },
      "source": [
        "# Peptide Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWkYFPZXZn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------\n",
        "# Peptides Dataloader\n",
        "# --------------------\n",
        "class ToxicPeptideDataset(Dataset):\n",
        "  def __init__(self, file_name, labels_file_name=\"\", train=True, soft=False):\n",
        "    if soft:\n",
        "      self.peptides = self.one_soft(np.load(file_name))\n",
        "    else:\n",
        "      self.peptides = self.one_hot(np.load(file_name))\n",
        "\n",
        "    self.use_labels = False\n",
        "    if labels_file_name != \"\":\n",
        "      self.use_labels = True\n",
        "      # These come in as strings and need to be floats\n",
        "      self.labels = [float(l) for l in np.load(labels_file_name)]\n",
        "\n",
        "      # Test train split\n",
        "      split = len(self.labels) // 10\n",
        "      if train:\n",
        "        self.peptides = self.peptides[split:]\n",
        "        self.labels = self.labels[split:]\n",
        "      else:\n",
        "        self.peptides = self.peptides[:split]\n",
        "        self.labels = self.labels[:split]\n",
        "\n",
        "  def one_hot(self, peptides):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        encoding = torch.zeros(len(gan_opt.amino_acids))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1.0\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "  \n",
        "  def one_soft(self, peptides, alpha=0.3):\n",
        "    all_encodings = []\n",
        "    for peptide in peptides:\n",
        "      encodings = []\n",
        "      for aa in peptide:\n",
        "        cs = len(gan_opt.amino_acids)\n",
        "        encoding = torch.ones(cs) * (alpha/(cs-1))\n",
        "        index = gan_opt.amino_acids.index(aa)\n",
        "        encoding[index] = 1 - alpha\n",
        "        encodings.append(encoding)\n",
        "      all_encodings.append(torch.stack(encodings))\n",
        "\n",
        "    return all_encodings\n",
        "\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    if self.use_labels:\n",
        "      return self.peptides[index], self.labels[index]\n",
        "    return self.peptides[index]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.peptides)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbN0afPFWAUG",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Classifier\n",
        "\n",
        "the goal of this model is to predict the toxicity score of a peptide. This can then be used to access the success of the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3hy8qDIR2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------\n",
        "# Toxic Peptide classifier\n",
        "# -------------------------\n",
        "\n",
        "class ToxicityPredictor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ToxicityPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, pep):\n",
        "    pep_flat = pep.view(pep.size(0), -1)\n",
        "    tox = self.model(pep_flat)\n",
        "\n",
        "    return tox\n",
        "\n",
        "# Loss function\n",
        "toxicity_loss = nn.L1Loss()\n",
        "\n",
        "# Initialize Predictor\n",
        "tox_predictor = ToxicityPredictor()\n",
        "\n",
        "if cuda:\n",
        "  tox_predictor.cuda()\n",
        "  toxicity_loss.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tox = torch.optim.Adam(tox_predictor.parameters(), lr=tox_opt.lr)\n",
        "\n",
        "\n",
        "# Data Loaders\n",
        "if not tox_opt.load_network:\n",
        "  tox_data_train = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=True)\n",
        "  tox_dataloader_train = torch.utils.data.DataLoader(tox_data_train, batch_size=tox_opt.batch_size, shuffle=True)\n",
        "\n",
        "  tox_data_test = ToxicPeptideDataset(\"allPeptideSequences.npy\", \"allPeptideScores.npy\", train=False)\n",
        "  tox_dataloader_test = torch.utils.data.DataLoader(tox_data_test, batch_size=tox_opt.batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Z_HRfNjuhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "  diff = torch.abs(y_hat-y_truth)\n",
        "  count = y_hat.size()[0]\n",
        "  return (count - torch.sum(diff))/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsvNAl4UVtt",
        "colab_type": "code",
        "outputId": "a93c8c3d-dfac-4a45-90ef-147471d35f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ------------------------------\n",
        "#  Toxicicty Predictor Training\n",
        "# ------------------------------\n",
        "\n",
        "losses = []\n",
        "v_losses = []\n",
        "acc = []\n",
        "def tox_train():\n",
        "  loop = tqdm(total=len(tox_dataloader_train) * tox_opt.n_epochs, position=0)\n",
        "  for epoch in range(tox_opt.n_epochs):\n",
        "\n",
        "      for i, (peps, toxs) in enumerate(tox_dataloader_train):\n",
        "          peps = Variable(peps.type(Tensor))\n",
        "          toxs = Variable(toxs.type(Tensor))\n",
        "          optimizer_tox.zero_grad()\n",
        "\n",
        "          y_hat = tox_predictor(peps)\n",
        "\n",
        "          # loss = toxicity_loss(y_hat.squeeze(), toxs)\n",
        "          # pdb.set_trace()\n",
        "          loss = torch.sum(torch.abs(y_hat.squeeze()-toxs))\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "          optimizer_tox.step()\n",
        "          last_acc = 0\n",
        "          if i % tox_opt.validate_every:\n",
        "            a = []\n",
        "            for v_peps, v_toxs in tox_dataloader_test:\n",
        "              v_peps = Variable(v_peps.type(Tensor))\n",
        "              v_toxs = Variable(v_toxs.type(Tensor))\n",
        "              v_y_hat = tox_predictor(v_peps)\n",
        "              a.append(accuracy(v_y_hat.squeeze(), v_toxs).item())\n",
        "            last_acc = np.mean(a)\n",
        "            acc.append((len(losses), last_acc))\n",
        "\n",
        "          loop.set_description(\"Epoch {}, Batch {}, Toxic_Loss {:.4f}, Accuracy {:.4f}\".format(epoch, i, loss.item(), last_acc))\n",
        "          loop.update()\n",
        "\n",
        "# No need to retrain if we can just load the network from a file\n",
        "if not tox_opt.load_network:\n",
        "  tox_train()\n",
        "  save_model(tox_predictor, tox_opt.save_model_file)\n",
        "\n",
        "  # Plot accuracy and loss\n",
        "  plt.plot(losses, label='losses')\n",
        "  plt.title('Toxicicity Predictor Losses')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  a, b = zip(*acc)\n",
        "  plt.plot(a, b, label='accuracy')\n",
        "  plt.title('Tocicity Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "if tox_opt.load_network:\n",
        "  load_model(tox_predictor, tox_opt.load_model_file)\n",
        "  print(\"Network Successfully Loaded!\")\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Successfully Loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLI4rR3MabTN",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN\n",
        "\n",
        "this section contains the code to run a simple gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the GAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "## Experiment 1\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=100\n",
        "  * label_smoothing=False\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0014, g loss 6.6079, unique 1.5000%, mean: -1.1950, std: 0.3212**\n",
        "\n",
        "## Experiment 2\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=10\n",
        "  * label_smoothing=False\n",
        "\n",
        "the final epoch was not chosen as the performance fell drastically after epoch 107 and did not have time to recover. It's also interesting to note that d loss dropped to 0 after only 50 epochs but the network seemed to still be able to train.\n",
        "\n",
        "**Results: epoch 107/120 d loss 0.0000, g loss 13.2041, unique 1.7000%, mean: -1.1205, std: 0.1167**\n",
        "\n",
        "## Experiment 3\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=100\n",
        "  * label_smoothing=True\n",
        "\n",
        "the training seemed to be a little bit smoother here.\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0069, g loss 5.0971, unique: 0.9000%, mean: -1.0141, std: 0.2059**\n",
        "\n",
        "## Experiment 4\n",
        "  * lr=0.0002\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * d_update_every=10\n",
        "  * label_smoothing=True\n",
        "\n",
        "epoch 114 was chosen because pefromance fell drastically after wards and didnt have time to recover. Here d loss dropped to 0 at around epoch 70. Training seemed smooth but ultimately unsuccessful.\n",
        "\n",
        "**Results: epoch 114/120 d loss 0.0000, g loss 13.2736, unique: 2.2000%, mean: -0.7249, std: 0.2899**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Zs2bIlQvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------\n",
        "# Define the Generator and Discriminator\n",
        "# ---------------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(gan_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Dataset\n",
        "gan_data = ToxicPeptideDataset(\"newMostToxicNCSequences.npy\", soft=gan_opt.label_smoothing)\n",
        "gan_dataloader = torch.utils.data.DataLoader(gan_data, batch_size=gan_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=gan_opt.lr, betas=(gan_opt.b1, gan_opt.b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPkjy6hTkDZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "#  GAN Training\n",
        "# --------------\n",
        "\n",
        "def gan_train():\n",
        "  for epoch in range(gan_opt.n_epochs):\n",
        "\n",
        "      for i, peps in enumerate(gan_dataloader):\n",
        "\n",
        "          # Adversarial ground truths\n",
        "          valid = Variable(Tensor(peps.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "          fake = Variable(Tensor(peps.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "          # Configure input\n",
        "          real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Sample noise as generator input\n",
        "          z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Sample high constrast noise as generator input (Tweak #2)\n",
        "          # z = Variable(Tensor(np.random.randint(0, 2, (peps.shape[0], gan_opt.latent_dim))))\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_peps = generator(z)\n",
        "\n",
        "          # Loss measures generator's ability to fool the discriminator\n",
        "          g_loss = adversarial_loss(discriminator(gen_peps), valid)\n",
        "\n",
        "          g_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          # ---------------------\n",
        "          #  Train Discriminator - every d_update_every steps\n",
        "          # ---------------------\n",
        "\n",
        "          batches_done = epoch * len(gan_dataloader) + i\n",
        "          if batches_done % gan_opt.d_update_every == 0:\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "            real_loss = adversarial_loss(discriminator(real_peps), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_peps.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "      unique, mean, std = evaluate_gan(generator)\n",
        "      print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique: {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, gan_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "\n",
        "if gan_opt.load_discriminator:\n",
        "  load_model(discriminator, gan_opt.load_disc_file)\n",
        "\n",
        "if gan_opt.load_generator:\n",
        "  load_model(generator, gan_opt.load_gen_file)\n",
        "\n",
        "if gan_opt.train_gan:\n",
        "  gan_train()\n",
        "  save_model(discriminator, gan_opt.save_disc_file)\n",
        "  save_model(generator, gan_opt.save_gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oFZxNkWsdE_",
        "colab_type": "text"
      },
      "source": [
        "# Pepto GAN - Washerstein GAN\n",
        "\n",
        "This section contains the code to run a washerstein gan forward on the peptide dataset. Ultimately it should produce toxic peptides as its output.\n",
        "\n",
        "*be sure to re-run all the cells in this section each time the WGAN is run otherwise the Generator and the Discriminator will not be re-initalized*\n",
        "\n",
        "Note: Experiments 1- 4 use the newMostToxicNCSeuences.npy dataset\n",
        "\n",
        "## Experiment 1\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=5\n",
        " * label_smoothing=False\n",
        "\n",
        "It looks like the washerstein gan fixes the modal colapse. Also it looks like the WGAN reduces its loss specifically by generating a variety of peptides rather than highly toxic peptides.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.1048, g loss 0.0275, unique 99.3000%, mean: -0.7451, std: 0.2461**\n",
        "\n",
        "## Experiment 2\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=5\n",
        " * label_smoothing=True\n",
        "\n",
        "Label Smoothing does not help the WGAN at all.\n",
        "\n",
        "**Results: epoch 120/120 d loss 0.0093, g loss 0.0040, unique 88.4000%, mean: -0.6492, std: 0.2866**\n",
        "\n",
        "## Experiment 3\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=20\n",
        " * label_smoothing=False\n",
        "\n",
        "Slowing the generator learning rate seems to help toxicity scores but makes the generator produce slightly less varried results.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0817, g loss 0.0534, unique 89.1000%, mean: -0.7930, std: 0.2221**\n",
        "\n",
        "## Experiment 4\n",
        " * n_epochs=120\n",
        " * batch_size=64\n",
        " * lr=0.001\n",
        " * b1=0.5\n",
        " * b2=0.999\n",
        " * latent_dim=100\n",
        " * clip_value=0.01\n",
        " * n_critic=100\n",
        " * label_smoothing=False\n",
        "\n",
        "This seems like this has gone too far\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0813, g loss 0.0418, unique 72.0000%, mean: -0.7263, std: 0.2341**\n",
        "\n",
        "## Experiment 5\n",
        "  * n_epochs=120\n",
        "  * batch_size=64\n",
        "  * lr=0.001\n",
        "  * b1=0.5\n",
        "  * b2=0.999\n",
        "  * latent_dim=100\n",
        "  * clip_value=0.01\n",
        "  * n_critic=5\n",
        "  * label_smoothing=False\n",
        "\n",
        "I ran this experiment by generating a top 40k, 30k, 20k, 10k, 5k, 2k, 1k toxic non-canonical peptides. I then trained the WGAN first on the 40k dataset, followed by the 30k dataset and so on until I finally trained it on the 1k dataset. There was very little different between training it on the 30 and 40 k datasets. However by the time it finished training on this dataset it's unique value was 99% or above. this uniqeness remained untill I started training it on the 2k and 1k dataset when uniqueness dropped quickly even as the mean toxicity improved. Ultimately the greatest shifts in toxicity came from training on the 2k and 1k datasets. We should probably investigate the mean and std of each of these datasets as my assumption is that the WGAN will match the statistics of the underlying datasets quite well. Whether this will pose a practical limit on Pepto GAN or not remains to be seen.\n",
        "\n",
        "**Results: epoch 120/120 d loss -0.0847, g loss 0.0231, unique 49.0000%, mean: -0.9063, std: 0.1430**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhAjsusVm31m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------\n",
        "# Define the Washerstein Generator and Descriminator\n",
        "# ---------------------------------------------------\n",
        "class WGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WGenerator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(was_opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(gan_opt.peptide_length * len(gan_opt.amino_acids))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        pep = self.model(z)\n",
        "        pep = pep.view(pep.size()[0], gan_opt.peptide_length, len(gan_opt.amino_acids))\n",
        "        pep = F.softmax(pep, dim=2)\n",
        "        return pep\n",
        "\n",
        "class WDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WDiscriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(len(gan_opt.amino_acids) * gan_opt.peptide_length), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, pep):\n",
        "        pep_flat = pep.view(pep.size(0), -1)\n",
        "        validity = self.model(pep_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "w_generator = WGenerator()\n",
        "w_discriminator = WDiscriminator()\n",
        "\n",
        "if cuda:\n",
        "    w_generator.cuda()\n",
        "    w_discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "w_gan_data = ToxicPeptideDataset(\"40K_NC_Toxic_Sequences.npy\", soft=was_opt.label_smoothing)\n",
        "w_gan_dataloader = torch.utils.data.DataLoader(w_gan_data, batch_size=was_opt.batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "w_optimizer_G = torch.optim.RMSprop(w_generator.parameters(), lr=was_opt.lr)\n",
        "w_optimizer_D = torch.optim.RMSprop(w_discriminator.parameters(), lr=was_opt.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1lLnDwssi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e753e64b-c048-483a-8af1-41a9dfda8412"
      },
      "source": [
        "# --------------------------\n",
        "#  Washerstein GAN Training\n",
        "# --------------------------\n",
        "\n",
        "# Configure data loader\n",
        "print(\"Loading New Dataset...\")\n",
        "w_gan_data = ToxicPeptideDataset(\"1K_NC_Toxic_Sequences.npy\", soft=was_opt.label_smoothing)\n",
        "w_gan_dataloader = torch.utils.data.DataLoader(w_gan_data, batch_size=was_opt.batch_size, shuffle=True)\n",
        "\n",
        "def was_gan_train():\n",
        "    for epoch in range(was_opt.n_epochs):\n",
        "\n",
        "        for i, peps in enumerate(w_gan_dataloader):\n",
        "\n",
        "            # Configure input\n",
        "            real_peps = Variable(peps.type(Tensor))\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            w_optimizer_D.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (peps.shape[0], was_opt.latent_dim))))\n",
        "\n",
        "            # Generate a batch of peptides\n",
        "            gen_peps = w_generator(z).detach()\n",
        "            # Adversarial loss\n",
        "            d_loss = -torch.mean(w_discriminator(real_peps)) + torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "            d_loss.backward()\n",
        "            w_optimizer_D.step()\n",
        "\n",
        "            # Clip weights of discriminator\n",
        "            for p in w_discriminator.parameters():\n",
        "                p.data.clamp_(-was_opt.clip_value, was_opt.clip_value)\n",
        "\n",
        "            # Train the generator every n_critic iterations\n",
        "            if i % was_opt.n_critic == 0:\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                w_optimizer_G.zero_grad()\n",
        "\n",
        "                # Generate a batch of images\n",
        "                gen_peps = w_generator(z)\n",
        "                # Adversarial loss\n",
        "                g_loss = -torch.mean(w_discriminator(gen_peps))\n",
        "\n",
        "                g_loss.backward()\n",
        "                w_optimizer_G.step()\n",
        "\n",
        "        unique, mean, std = evaluate_gan(w_generator)\n",
        "        print(\"epoch {}/{} d loss {:.4f}, g loss {:.4f}, unique {:.4f}%, mean: {:.4f}, std: {:.4f}\".format(epoch+1, was_opt.n_epochs, d_loss.item(), g_loss.item(), unique, mean, std))\n",
        "\n",
        "if was_opt.load_discriminator:\n",
        "  load_model(w_discriminator, was_opt.load_disc_file)\n",
        "\n",
        "if was_opt.load_generator:\n",
        "  load_model(w_generator, was_opt.load_gen_file)\n",
        "\n",
        "if was_opt.train_gan:\n",
        "  was_gan_train()\n",
        "  save_model(w_discriminator, was_opt.save_disc_file)\n",
        "  save_model(w_generator, was_opt.save_gen_file)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading New Dataset...\n",
            "epoch 1/120 d loss -0.1115, g loss 0.0153, unique 70.5000%, mean: -0.7874, std: 0.1903\n",
            "epoch 2/120 d loss -0.0964, g loss 0.0211, unique 68.9000%, mean: -0.8145, std: 0.1880\n",
            "epoch 3/120 d loss -0.1200, g loss 0.0269, unique 68.2000%, mean: -0.8380, std: 0.1656\n",
            "epoch 4/120 d loss -0.1029, g loss 0.0277, unique 64.6000%, mean: -0.8432, std: 0.1748\n",
            "epoch 5/120 d loss -0.1017, g loss 0.0244, unique 64.5000%, mean: -0.8617, std: 0.1674\n",
            "epoch 6/120 d loss -0.1134, g loss 0.0273, unique 61.4000%, mean: -0.8608, std: 0.1605\n",
            "epoch 7/120 d loss -0.1248, g loss 0.0268, unique 59.1000%, mean: -0.8790, std: 0.1510\n",
            "epoch 8/120 d loss -0.0895, g loss 0.0369, unique 59.3000%, mean: -0.8664, std: 0.1529\n",
            "epoch 9/120 d loss -0.0994, g loss 0.0188, unique 56.6000%, mean: -0.8766, std: 0.1494\n",
            "epoch 10/120 d loss -0.1215, g loss 0.0172, unique 56.1000%, mean: -0.8843, std: 0.1442\n",
            "epoch 11/120 d loss -0.1142, g loss 0.0308, unique 56.3000%, mean: -0.8806, std: 0.1449\n",
            "epoch 12/120 d loss -0.1079, g loss 0.0319, unique 55.4000%, mean: -0.8812, std: 0.1518\n",
            "epoch 13/120 d loss -0.1122, g loss 0.0380, unique 57.4000%, mean: -0.8816, std: 0.1513\n",
            "epoch 14/120 d loss -0.0972, g loss 0.0204, unique 57.1000%, mean: -0.8855, std: 0.1501\n",
            "epoch 15/120 d loss -0.1108, g loss 0.0332, unique 58.1000%, mean: -0.8914, std: 0.1423\n",
            "epoch 16/120 d loss -0.1167, g loss 0.0292, unique 55.4000%, mean: -0.8821, std: 0.1484\n",
            "epoch 17/120 d loss -0.0962, g loss 0.0197, unique 52.7000%, mean: -0.8896, std: 0.1424\n",
            "epoch 18/120 d loss -0.1241, g loss 0.0405, unique 50.4000%, mean: -0.8921, std: 0.1442\n",
            "epoch 19/120 d loss -0.1146, g loss 0.0265, unique 51.9000%, mean: -0.8946, std: 0.1460\n",
            "epoch 20/120 d loss -0.1039, g loss 0.0188, unique 53.4000%, mean: -0.8938, std: 0.1361\n",
            "epoch 21/120 d loss -0.1198, g loss 0.0284, unique 52.5000%, mean: -0.8803, std: 0.1515\n",
            "epoch 22/120 d loss -0.0991, g loss 0.0367, unique 50.7000%, mean: -0.8896, std: 0.1384\n",
            "epoch 23/120 d loss -0.1120, g loss 0.0325, unique 53.5000%, mean: -0.8881, std: 0.1418\n",
            "epoch 24/120 d loss -0.0999, g loss 0.0235, unique 52.5000%, mean: -0.8832, std: 0.1455\n",
            "epoch 25/120 d loss -0.0964, g loss 0.0353, unique 52.7000%, mean: -0.8924, std: 0.1426\n",
            "epoch 26/120 d loss -0.0964, g loss 0.0330, unique 52.3000%, mean: -0.8982, std: 0.1416\n",
            "epoch 27/120 d loss -0.1202, g loss 0.0194, unique 51.6000%, mean: -0.8893, std: 0.1460\n",
            "epoch 28/120 d loss -0.0829, g loss 0.0194, unique 50.8000%, mean: -0.8866, std: 0.1490\n",
            "epoch 29/120 d loss -0.1110, g loss 0.0287, unique 50.7000%, mean: -0.8957, std: 0.1401\n",
            "epoch 30/120 d loss -0.1171, g loss 0.0324, unique 53.5000%, mean: -0.8906, std: 0.1466\n",
            "epoch 31/120 d loss -0.0985, g loss 0.0218, unique 51.7000%, mean: -0.8840, std: 0.1502\n",
            "epoch 32/120 d loss -0.1032, g loss 0.0174, unique 50.5000%, mean: -0.8841, std: 0.1468\n",
            "epoch 33/120 d loss -0.1021, g loss 0.0273, unique 51.6000%, mean: -0.8944, std: 0.1488\n",
            "epoch 34/120 d loss -0.1034, g loss 0.0330, unique 49.8000%, mean: -0.8912, std: 0.1480\n",
            "epoch 35/120 d loss -0.1049, g loss 0.0350, unique 51.0000%, mean: -0.8981, std: 0.1373\n",
            "epoch 36/120 d loss -0.1129, g loss 0.0341, unique 49.0000%, mean: -0.8903, std: 0.1362\n",
            "epoch 37/120 d loss -0.1019, g loss 0.0340, unique 48.0000%, mean: -0.9031, std: 0.1438\n",
            "epoch 38/120 d loss -0.1150, g loss 0.0274, unique 50.2000%, mean: -0.8884, std: 0.1507\n",
            "epoch 39/120 d loss -0.1120, g loss 0.0190, unique 50.9000%, mean: -0.8983, std: 0.1485\n",
            "epoch 40/120 d loss -0.1123, g loss 0.0309, unique 49.8000%, mean: -0.8929, std: 0.1486\n",
            "epoch 41/120 d loss -0.0996, g loss 0.0266, unique 48.1000%, mean: -0.8970, std: 0.1480\n",
            "epoch 42/120 d loss -0.0858, g loss 0.0345, unique 49.8000%, mean: -0.8991, std: 0.1441\n",
            "epoch 43/120 d loss -0.0892, g loss 0.0386, unique 48.2000%, mean: -0.8961, std: 0.1516\n",
            "epoch 44/120 d loss -0.1213, g loss 0.0342, unique 51.4000%, mean: -0.8970, std: 0.1392\n",
            "epoch 45/120 d loss -0.0869, g loss 0.0294, unique 48.7000%, mean: -0.8977, std: 0.1449\n",
            "epoch 46/120 d loss -0.1194, g loss 0.0304, unique 50.9000%, mean: -0.8926, std: 0.1454\n",
            "epoch 47/120 d loss -0.1085, g loss 0.0220, unique 48.2000%, mean: -0.8967, std: 0.1507\n",
            "epoch 48/120 d loss -0.1098, g loss 0.0282, unique 50.3000%, mean: -0.8984, std: 0.1507\n",
            "epoch 49/120 d loss -0.0973, g loss 0.0331, unique 51.9000%, mean: -0.8905, std: 0.1595\n",
            "epoch 50/120 d loss -0.1123, g loss 0.0301, unique 49.0000%, mean: -0.8998, std: 0.1517\n",
            "epoch 51/120 d loss -0.0833, g loss 0.0366, unique 49.3000%, mean: -0.8960, std: 0.1567\n",
            "epoch 52/120 d loss -0.1017, g loss 0.0328, unique 47.7000%, mean: -0.9051, std: 0.1478\n",
            "epoch 53/120 d loss -0.0958, g loss 0.0248, unique 48.6000%, mean: -0.8962, std: 0.1475\n",
            "epoch 54/120 d loss -0.1040, g loss 0.0257, unique 49.4000%, mean: -0.8983, std: 0.1479\n",
            "epoch 55/120 d loss -0.0989, g loss 0.0227, unique 48.4000%, mean: -0.9057, std: 0.1454\n",
            "epoch 56/120 d loss -0.1053, g loss 0.0245, unique 50.8000%, mean: -0.8969, std: 0.1534\n",
            "epoch 57/120 d loss -0.1100, g loss 0.0277, unique 50.4000%, mean: -0.8985, std: 0.1541\n",
            "epoch 58/120 d loss -0.1147, g loss 0.0284, unique 46.9000%, mean: -0.9019, std: 0.1520\n",
            "epoch 59/120 d loss -0.1041, g loss 0.0339, unique 50.0000%, mean: -0.9086, std: 0.1453\n",
            "epoch 60/120 d loss -0.0942, g loss 0.0333, unique 49.3000%, mean: -0.9046, std: 0.1481\n",
            "epoch 61/120 d loss -0.1003, g loss 0.0275, unique 50.6000%, mean: -0.8999, std: 0.1444\n",
            "epoch 62/120 d loss -0.0998, g loss 0.0256, unique 49.2000%, mean: -0.9005, std: 0.1482\n",
            "epoch 63/120 d loss -0.1070, g loss 0.0183, unique 48.6000%, mean: -0.8923, std: 0.1491\n",
            "epoch 64/120 d loss -0.0896, g loss 0.0197, unique 51.3000%, mean: -0.9008, std: 0.1512\n",
            "epoch 65/120 d loss -0.1036, g loss 0.0269, unique 51.3000%, mean: -0.9041, std: 0.1554\n",
            "epoch 66/120 d loss -0.1041, g loss 0.0261, unique 51.3000%, mean: -0.9023, std: 0.1562\n",
            "epoch 67/120 d loss -0.1003, g loss 0.0265, unique 51.3000%, mean: -0.9088, std: 0.1542\n",
            "epoch 68/120 d loss -0.1177, g loss 0.0184, unique 52.6000%, mean: -0.9021, std: 0.1555\n",
            "epoch 69/120 d loss -0.0989, g loss 0.0282, unique 50.5000%, mean: -0.8903, std: 0.1549\n",
            "epoch 70/120 d loss -0.0764, g loss 0.0344, unique 51.4000%, mean: -0.9085, std: 0.1418\n",
            "epoch 71/120 d loss -0.0935, g loss 0.0355, unique 51.0000%, mean: -0.9051, std: 0.1521\n",
            "epoch 72/120 d loss -0.0953, g loss 0.0332, unique 51.4000%, mean: -0.9125, std: 0.1474\n",
            "epoch 73/120 d loss -0.0925, g loss 0.0407, unique 51.4000%, mean: -0.9086, std: 0.1481\n",
            "epoch 74/120 d loss -0.0901, g loss 0.0273, unique 51.9000%, mean: -0.9148, std: 0.1484\n",
            "epoch 75/120 d loss -0.0891, g loss 0.0290, unique 49.9000%, mean: -0.9217, std: 0.1376\n",
            "epoch 76/120 d loss -0.0984, g loss 0.0321, unique 52.1000%, mean: -0.9057, std: 0.1531\n",
            "epoch 77/120 d loss -0.1013, g loss 0.0327, unique 51.4000%, mean: -0.9169, std: 0.1529\n",
            "epoch 78/120 d loss -0.1135, g loss 0.0184, unique 50.7000%, mean: -0.9131, std: 0.1458\n",
            "epoch 79/120 d loss -0.1004, g loss 0.0291, unique 50.5000%, mean: -0.9083, std: 0.1479\n",
            "epoch 80/120 d loss -0.0932, g loss 0.0370, unique 51.4000%, mean: -0.9094, std: 0.1470\n",
            "epoch 81/120 d loss -0.0963, g loss 0.0188, unique 52.2000%, mean: -0.9095, std: 0.1613\n",
            "epoch 82/120 d loss -0.1114, g loss 0.0247, unique 52.0000%, mean: -0.9215, std: 0.1465\n",
            "epoch 83/120 d loss -0.0982, g loss 0.0227, unique 49.7000%, mean: -0.9086, std: 0.1525\n",
            "epoch 84/120 d loss -0.1092, g loss 0.0342, unique 48.4000%, mean: -0.9079, std: 0.1488\n",
            "epoch 85/120 d loss -0.1016, g loss 0.0234, unique 50.1000%, mean: -0.9120, std: 0.1501\n",
            "epoch 86/120 d loss -0.1136, g loss 0.0287, unique 48.9000%, mean: -0.9179, std: 0.1483\n",
            "epoch 87/120 d loss -0.1005, g loss 0.0345, unique 49.2000%, mean: -0.9104, std: 0.1488\n",
            "epoch 88/120 d loss -0.0935, g loss 0.0323, unique 48.8000%, mean: -0.9127, std: 0.1503\n",
            "epoch 89/120 d loss -0.0902, g loss 0.0246, unique 49.9000%, mean: -0.9166, std: 0.1496\n",
            "epoch 90/120 d loss -0.1108, g loss 0.0251, unique 49.5000%, mean: -0.9097, std: 0.1426\n",
            "epoch 91/120 d loss -0.1038, g loss 0.0196, unique 51.1000%, mean: -0.9073, std: 0.1449\n",
            "epoch 92/120 d loss -0.1114, g loss 0.0153, unique 50.5000%, mean: -0.9087, std: 0.1517\n",
            "epoch 93/120 d loss -0.1044, g loss 0.0289, unique 51.1000%, mean: -0.9167, std: 0.1447\n",
            "epoch 94/120 d loss -0.1210, g loss 0.0289, unique 49.1000%, mean: -0.9006, std: 0.1556\n",
            "epoch 95/120 d loss -0.1173, g loss 0.0344, unique 50.3000%, mean: -0.9098, std: 0.1439\n",
            "epoch 96/120 d loss -0.1045, g loss 0.0233, unique 50.2000%, mean: -0.8979, std: 0.1606\n",
            "epoch 97/120 d loss -0.0994, g loss 0.0264, unique 49.8000%, mean: -0.9061, std: 0.1522\n",
            "epoch 98/120 d loss -0.1166, g loss 0.0227, unique 52.9000%, mean: -0.9082, std: 0.1493\n",
            "epoch 99/120 d loss -0.0922, g loss 0.0234, unique 48.7000%, mean: -0.9038, std: 0.1516\n",
            "epoch 100/120 d loss -0.0962, g loss 0.0297, unique 47.2000%, mean: -0.9029, std: 0.1508\n",
            "epoch 101/120 d loss -0.0838, g loss 0.0390, unique 46.6000%, mean: -0.9007, std: 0.1493\n",
            "epoch 102/120 d loss -0.1037, g loss 0.0299, unique 48.5000%, mean: -0.9112, std: 0.1448\n",
            "epoch 103/120 d loss -0.1170, g loss 0.0172, unique 48.5000%, mean: -0.9091, std: 0.1471\n",
            "epoch 104/120 d loss -0.0980, g loss 0.0315, unique 50.8000%, mean: -0.9049, std: 0.1554\n",
            "epoch 105/120 d loss -0.0984, g loss 0.0320, unique 49.0000%, mean: -0.9042, std: 0.1504\n",
            "epoch 106/120 d loss -0.0893, g loss 0.0350, unique 51.8000%, mean: -0.9046, std: 0.1517\n",
            "epoch 107/120 d loss -0.1098, g loss 0.0251, unique 47.9000%, mean: -0.8951, std: 0.1489\n",
            "epoch 108/120 d loss -0.0986, g loss 0.0353, unique 49.7000%, mean: -0.8995, std: 0.1528\n",
            "epoch 109/120 d loss -0.1027, g loss 0.0264, unique 49.3000%, mean: -0.8981, std: 0.1536\n",
            "epoch 110/120 d loss -0.1072, g loss 0.0220, unique 51.5000%, mean: -0.8993, std: 0.1486\n",
            "epoch 111/120 d loss -0.0989, g loss 0.0289, unique 50.7000%, mean: -0.9042, std: 0.1516\n",
            "epoch 112/120 d loss -0.1025, g loss 0.0222, unique 49.4000%, mean: -0.9059, std: 0.1465\n",
            "epoch 113/120 d loss -0.1028, g loss 0.0258, unique 50.0000%, mean: -0.8976, std: 0.1571\n",
            "epoch 114/120 d loss -0.1032, g loss 0.0374, unique 47.4000%, mean: -0.8962, std: 0.1528\n",
            "epoch 115/120 d loss -0.1055, g loss 0.0218, unique 49.5000%, mean: -0.9003, std: 0.1573\n",
            "epoch 116/120 d loss -0.0978, g loss 0.0276, unique 49.0000%, mean: -0.9049, std: 0.1464\n",
            "epoch 117/120 d loss -0.1034, g loss 0.0205, unique 49.1000%, mean: -0.8944, std: 0.1472\n",
            "epoch 118/120 d loss -0.0725, g loss 0.0507, unique 49.5000%, mean: -0.9019, std: 0.1535\n",
            "epoch 119/120 d loss -0.0979, g loss 0.0294, unique 49.2000%, mean: -0.8999, std: 0.1532\n",
            "epoch 120/120 d loss -0.0847, g loss 0.0231, unique 49.0000%, mean: -0.9063, std: 0.1430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84wV6USBCU8c",
        "colab_type": "text"
      },
      "source": [
        "# GAN evalutation\n",
        "\n",
        "Code to evaluate gan performance on several heuristics to see which setup provides the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQOH3EFyniU",
        "colab_type": "code",
        "outputId": "f80ad349-95b3-42d0-93a3-54fb4dee5fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "# ---------------\n",
        "# GAN evaluation\n",
        "# ---------------\n",
        "\n",
        "def evaluate_gan(test_model, count=1000, draw_histogram=False):\n",
        "  z = Variable(Tensor(np.random.normal(0, 1, (count, gan_opt.latent_dim))))\n",
        "\n",
        "  gen = test_model(z)\n",
        "  peptides = decode_peptide_s(gen)\n",
        "  peptides = np.unique(peptides)\n",
        "  peptides = one_hot_s(peptides)\n",
        "\n",
        "  gen_scores = tox_predictor(peptides.cuda())\n",
        "\n",
        "  if draw_histogram:\n",
        "    print(\"Unique Peptides Generated: {}\".format(peptides.size(0)))\n",
        "    print(\"Mean Score: {:.6f}\".format(gen_scores.mean()))\n",
        "    print(\"Standard Deviation: {:.6f}\".format(gen_scores.std()))\n",
        "    # generate comparison data\n",
        "    data = generate_random_peptide_s(count)\n",
        "    data_scores = tox_predictor(data.cuda())\n",
        "    print(\"Distance Between Means: {:.6f}\".format(data_scores.mean().item() - gen_scores.mean().item()))\n",
        "\n",
        "    colors = ['red', 'blue']\n",
        "    labels = ['generated peptides', 'random peptides']\n",
        "    plt.figure(figsize=(16,9))\n",
        "    plt.hist(\n",
        "        [gen_scores.detach().cpu().numpy(), data_scores.detach().cpu().numpy()], \n",
        "        bins=10, density=True, histtype='bar', color=colors, label=labels)\n",
        "    plt.legend(prop={'size': 10})\n",
        "    plt.title('distribution of peptide data')\n",
        "\n",
        "  uniqueness = (peptides.size(0)/count)*100\n",
        "  return uniqueness, gen_scores.mean().item(), gen_scores.std().item()\n",
        "\n",
        "evaluate_gan(w_generator, count=30000, draw_histogram=True)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Peptides Generated: 2914\n",
            "Mean Score: -0.895486\n",
            "Standard Deviation: 0.166711\n",
            "Distance Between Means: 0.700684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.713333333333333, -0.8954861760139465, 0.1667114794254303)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAIYCAYAAABg/MHpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjZ38DKIB3qD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "eeb703c2-599a-4fcd-faca-a05d87a98517"
      },
      "source": [
        "# ---------------\n",
        "# Top N peptides\n",
        "# ---------------\n",
        "\n",
        "#generate and display the top N toxic peptides\n",
        "def top_n_peptides(generator, count=20):\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (count*100, gan_opt.latent_dim))))\n",
        "\n",
        "    gen = generator(z)\n",
        "    peptides = decode_peptide_s(gen)\n",
        "    peptides = np.unique(peptides)\n",
        "    peptides = one_hot_s(peptides)\n",
        "\n",
        "    gen_scores = tox_predictor(peptides.cuda())\n",
        "    score = np.array(gen_scores.squeeze().tolist())\n",
        "    peptide_data = [(pep, score[i]) for i, pep in enumerate(decode_peptide_s(peptides))]\n",
        "    peptide_data.sort(key=lambda tup: tup[1]) \n",
        "    return peptide_data[:count]\n",
        "\n",
        "top_n_peptides(w_generator)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('YYYLRIYL', -1.3079304695129395),\n",
              " ('IYYLYIRY', -1.2074322700500488),\n",
              " ('LYYYYIYL', -1.206214427947998),\n",
              " ('IYYLYIRL', -1.201981544494629),\n",
              " ('FYLYYYLY', -1.1993763446807861),\n",
              " ('LYIYIYRY', -1.1973598003387451),\n",
              " ('YLYYYIRY', -1.1969962120056152),\n",
              " ('YYIIFYRY', -1.1729531288146973),\n",
              " ('FYLYLYLL', -1.1697323322296143),\n",
              " ('IYYYYIRY', -1.1666827201843262),\n",
              " ('RYYLVIYL', -1.1643898487091064),\n",
              " ('FYLYYIRY', -1.1631367206573486),\n",
              " ('YYYLRYYL', -1.1611418724060059),\n",
              " ('LHIIFYYY', -1.1596765518188477),\n",
              " ('YYYIYYRL', -1.1584758758544922),\n",
              " ('LYILYYRY', -1.1553714275360107),\n",
              " ('YYYLYYRL', -1.1538212299346924),\n",
              " ('IYIYYIRY', -1.1495006084442139),\n",
              " ('YYYYYYLL', -1.1484143733978271),\n",
              " ('FYLYLILL', -1.1483674049377441)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMg33Z8dtOQu",
        "colab_type": "text"
      },
      "source": [
        "# Data Genreation\n",
        "\n",
        "code that takes the original dataset and modifies it to be better/ more usable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH04hDoEhtGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c579dcd9-962c-41ba-89cb-a5992ebdaa21"
      },
      "source": [
        "# -------------------\n",
        "# Generate Base Data\n",
        "# -------------------\n",
        "\n",
        "all_sequences = []\n",
        "all_scores = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "      line_count += 1\n",
        "      all_sequences.append(row[0])\n",
        "      all_scores.append(row[1])\n",
        "    \n",
        "    print(f'\\nProcessed {line_count} lines.\\n')\n",
        "  \n",
        "#This is the base dataset that everything is based on\n",
        "np.save(\"allPeptideSequences\", all_sequences)\n",
        "np.save(\"allPeptideScores\", all_scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 67729/105416 [00:00<00:00, 677279.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 105416 lines.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZclKItdvQabI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "344e370f-9001-4a36-ca58-accf8ed67c7b"
      },
      "source": [
        "# -----------------------------------\n",
        "# Top-N Non Canonical Toxic Peptides\n",
        "# -----------------------------------\n",
        "import pdb\n",
        "def toxic_non_cannon(count):\n",
        "    peptides = []\n",
        "    non_cannon_peps = []\n",
        "\n",
        "    # Read in all the peptides and scores\n",
        "    with open('all_data_filtered.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        for row in csv_reader:\n",
        "            peptides.append((row[0], row[1]))\n",
        "\n",
        "    for p in peptides:\n",
        "        sequence = p[0]\n",
        "\n",
        "        hydrophobicity = 0\n",
        "        cysteines = 0\n",
        "        for aa in sequence:\n",
        "            if aa in [\"F\",\"I\",\"V\",\"L\",\"W\"]:\n",
        "                hydrophobicity = hydrophobicity + 1\n",
        "            if aa == \"C\":\n",
        "                cysteines = cysteines + 1\n",
        "\n",
        "        if hydrophobicity < 4 and cysteines < 3:\n",
        "            non_cannon_peps.append((sequence, float(p[1])))\n",
        "\n",
        "    non_cannon_peps.sort(key=lambda tup: tup[1]) \n",
        "    ret = np.array(non_cannon_peps[:count])\n",
        "    return ret[:,0], ret[:,1]\n",
        "    \n",
        "dataset_size = [40, 30, 20, 10, 5, 2, 1] #Size is in thousands\n",
        "for s in dataset_size:\n",
        "  sk = s * 1000\n",
        "  print(\"Generating the top {}K toxic NC peptide...\".format(s))\n",
        "  peptides, scores = toxic_non_cannon(sk)\n",
        "  print(\"Saving Data...\".format(s))\n",
        "  np.save(\"{}K_NC_Toxic_Sequences\".format(s), peptides)\n",
        "  np.save(\"{}K_NC_Toxic_Scores\".format(s), scores)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating the top 40K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 30K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 20K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 10K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 5K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 2K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 1K toxic NC peptide...\n",
            "Saving Data...\n",
            "Generating the top 0.5K toxic NC peptide...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-d42e1e2e5baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0msk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating the top {}K toxic NC peptide...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mpeptides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoxic_non_cannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving Data...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}K_NC_Toxic_Sequences\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeptides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-171-d42e1e2e5baf>\u001b[0m in \u001b[0;36mtoxic_non_cannon\u001b[0;34m(count)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnon_cannon_peps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_cannon_peps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi8V7YR2WGj",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Peptide Hall Of Fame\n",
        "\n",
        "All the best most toxic peptides that get generated should get stuck here along with their score so we don't loose track of them\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NejtSWCbE8FI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0b67278-6ed1-4ad6-e6a3-daa963a47f0e"
      },
      "source": [
        "best_toxic_non_canonical_peptides = [\n",
        "  ('YYYLRIYL', -1.3079304695129395),\n",
        "  ('IYYLYIRY', -1.2074322700500488),\n",
        "  ('LYYYYIYL', -1.206214427947998),\n",
        "  ('IYYLYIRL', -1.201981544494629),\n",
        "  ('FYLYYYLY', -1.1993763446807861),\n",
        "  ('LYIYIYRY', -1.1973598003387451),\n",
        "  ('YLYYYIRY', -1.1969962120056152),\n",
        "  ('YYIIFYRY', -1.1729531288146973),\n",
        "  ('FYLYLYLL', -1.1697323322296143),\n",
        "  ('IYYYYIRY', -1.1666827201843262),\n",
        "  ('RYYLVIYL', -1.1643898487091064),\n",
        "  ('FYLYYIRY', -1.1631367206573486),\n",
        "  ('YYYLRYYL', -1.1611418724060059),\n",
        "  ('LHIIFYYY', -1.1596765518188477),\n",
        "  ('YYYIYYRL', -1.1584758758544922),\n",
        "  ('LYILYYRY', -1.1553714275360107),\n",
        "  ('YYYLYYRL', -1.1538212299346924),\n",
        "  ('IYIYYIRY', -1.1495006084442139),\n",
        "  ('YYYYYYLL', -1.1484143733978271),\n",
        "  ('FYLYLILL', -1.1483674049377441),\n",
        "  ('FRGRFFFL', -1.397936),\n",
        "  ('FYFYSFCF', -1.116865),\n",
        "  ('FYFHVFCF', -1.346310),\n",
        "  ('FYVLCCFY', -1.095583),\n",
        "  ('FFLYVCFG', -1.038978),\n",
        "  ('FRLYVVVC', -1.035730),\n",
        "  ('FFLYVCFG', -1.038978),\n",
        "  ('FFYICFCC', -1.063583),\n",
        "  ('FFFICFCF', -1.361509),\n",
        "  ('VFLFSLFI', -1.535002),\n",
        "  ('VFLFGLFI', -1.367290),\n",
        "  ('VFVFGLFI', -1.190928),\n",
        "  ('VFLFGLFI', -1.367290),\n",
        "  ('LYLVCHFL', -1.002115),\n",
        "  ('LVYFVLCF', -1.046022),\n",
        "  ('LVYFFLLF', -1.109444),\n",
        "  ('CCVIRVLI', -1.294149),\n",
        "  ('FICFFSVF', -1.055895),\n",
        "  ('YCCLFFFF', -1.043908),\n",
        "  ('GICVICYV', -1.096067),\n",
        "  ('YICVFRIV', -1.064286),\n",
        "  ('LYCCVLFF', -1.020174),\n",
        "  ('FYLVCCIV', -1.010827),\n",
        "  ('FCLCICIY', -1.142711),\n",
        "  ('FCLCICIV', -1.031946),\n",
        "  ('VFVFYGCF', -1.045995),\n",
        "  ('FFCCICIF', -1.163224),\n",
        "  ('FYCCICIF', -1.138242),\n",
        "  ('IIRLCFSY', -1.070690),\n",
        "  ('LYIICLLI', -1.017955),\n",
        "  ('GVLFVCCF', -1.033618),\n",
        "  ('FFLCICFY', -1.167259),\n",
        "  ('FYLCICFY', -1.154940),\n",
        "  ('VYLCVCFY', -1.114934),\n",
        "  ('FCVFFFRR', -1.077060),\n",
        "  ('CFFVYRVL', -1.119970),\n",
        "  ('LFCCVLIV', -1.043952),\n",
        "  ('FFRFLVVV', -1.301919),\n",
        "  ('FFRFLYVV', -1.355798),\n",
        "  ('YFFFFYGF', -1.236266),\n",
        "  ('ICLSVYFF', -1.224726 ),\n",
        "  ]\n",
        "\n",
        "# Remove any acidental duplicates\n",
        "best_toxic_non_canonical_peptides = np.unique(best_toxic_non_canonical_peptides)\n",
        "# Sort by most toxic\n",
        "best_toxic_non_canonical_peptides.sort(key=lambda tup: tup[1])\n",
        "# Display the results!\n",
        "print(np.array(best_toxic_non_canonical_peptides))"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['VFLFSLFI' '-1.535002']\n",
            " ['FRGRFFFL' '-1.397936']\n",
            " ['VFLFGLFI' '-1.36729']\n",
            " ['VFLFGLFI' '-1.36729']\n",
            " ['FFFICFCF' '-1.361509']\n",
            " ['FFRFLYVV' '-1.355798']\n",
            " ['FYFHVFCF' '-1.34631']\n",
            " ['YYYLRIYL' '-1.3079304695129395']\n",
            " ['FFRFLVVV' '-1.301919']\n",
            " ['CCVIRVLI' '-1.294149']\n",
            " ['YFFFFYGF' '-1.236266']\n",
            " ['ICLSVYFF' '-1.224726']\n",
            " ['IYYLYIRY' '-1.2074322700500488']\n",
            " ['LYYYYIYL' '-1.206214427947998']\n",
            " ['IYYLYIRL' '-1.201981544494629']\n",
            " ['FYLYYYLY' '-1.1993763446807861']\n",
            " ['LYIYIYRY' '-1.1973598003387451']\n",
            " ['YLYYYIRY' '-1.1969962120056152']\n",
            " ['VFVFGLFI' '-1.190928']\n",
            " ['YYIIFYRY' '-1.1729531288146973']\n",
            " ['FYLYLYLL' '-1.1697323322296143']\n",
            " ['FFLCICFY' '-1.167259']\n",
            " ['IYYYYIRY' '-1.1666827201843262']\n",
            " ['RYYLVIYL' '-1.1643898487091064']\n",
            " ['FFCCICIF' '-1.163224']\n",
            " ['FYLYYIRY' '-1.1631367206573486']\n",
            " ['YYYLRYYL' '-1.1611418724060059']\n",
            " ['LHIIFYYY' '-1.1596765518188477']\n",
            " ['YYYIYYRL' '-1.1584758758544922']\n",
            " ['LYILYYRY' '-1.1553714275360107']\n",
            " ['FYLCICFY' '-1.15494']\n",
            " ['YYYLYYRL' '-1.1538212299346924']\n",
            " ['IYIYYIRY' '-1.1495006084442139']\n",
            " ['YYYYYYLL' '-1.1484143733978271']\n",
            " ['FYLYLILL' '-1.1483674049377441']\n",
            " ['FCLCICIY' '-1.142711']\n",
            " ['FYCCICIF' '-1.138242']\n",
            " ['CFFVYRVL' '-1.11997']\n",
            " ['FYFYSFCF' '-1.116865']\n",
            " ['VYLCVCFY' '-1.114934']\n",
            " ['LVYFFLLF' '-1.109444']\n",
            " ['GICVICYV' '-1.096067']\n",
            " ['FYVLCCFY' '-1.095583']\n",
            " ['FCVFFFRR' '-1.07706']\n",
            " ['IIRLCFSY' '-1.07069']\n",
            " ['YICVFRIV' '-1.064286']\n",
            " ['FFYICFCC' '-1.063583']\n",
            " ['FICFFSVF' '-1.055895']\n",
            " ['LVYFVLCF' '-1.046022']\n",
            " ['VFVFYGCF' '-1.045995']\n",
            " ['LFCCVLIV' '-1.043952']\n",
            " ['YCCLFFFF' '-1.043908']\n",
            " ['FFLYVCFG' '-1.038978']\n",
            " ['FFLYVCFG' '-1.038978']\n",
            " ['FRLYVVVC' '-1.03573']\n",
            " ['GVLFVCCF' '-1.033618']\n",
            " ['FCLCICIV' '-1.031946']\n",
            " ['LYCCVLFF' '-1.020174']\n",
            " ['LYIICLLI' '-1.017955']\n",
            " ['FYLVCCIV' '-1.010827']\n",
            " ['LYLVCHFL' '-1.002115']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0Dq3r4U1oj",
        "colab_type": "text"
      },
      "source": [
        "# Tests\n",
        "\n",
        "Please ignore everything bellow this line. This is garbage code just used for testing random things.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iEK8ku8U-7O",
        "colab_type": "code",
        "outputId": "1a2dd1ed-4013-42c3-e20f-7fe8cad765e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = np.load(\"mostToxicNCSequences.npy\")\n",
        "scores = np.load(\"mostToxicNCScores.npy\")\n",
        "print(len(sequences))\n",
        "print(sequences[:5]) #Why are these peptides 7 aa's long? That seems wrong?\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "# Repair old data\n",
        "new_sequences = []\n",
        "with open('all_data_filtered.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  pbar = tqdm(total=45957)\n",
        "  for row in csv_reader:\n",
        "    line_count += 1\n",
        "    for i, sequence in enumerate(sequences):\n",
        "      if sequence in row[0] and scores[i] == row[1]:\n",
        "        pbar.update()\n",
        "        new_sequences.append(row[0])\n",
        "\n",
        "  print(f'Processed {line_count} lines.')\n",
        "\n",
        "print(len(new_sequences))\n",
        "print(new_sequences[:5])\n",
        "\n",
        "np.save(\"newMostToxicNCSequences\", new_sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/45957 [00:00<21:02, 36.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45957\n",
            "['DCHRGFV' 'YRCCIIV' 'VCVHFLC' 'CCDIYVC' 'HCFCFDI']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 45954/45957 [25:52<00:00, 28.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 105416 lines.\n",
            "45957\n",
            "['CDCHRGFV', 'IYRCCIIV', 'HVCVHFLC', 'DCCDIYVC', 'FHCFCFDI']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}